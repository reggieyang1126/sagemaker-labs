{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Deploy a model for asynchronous inference\n",
    "\n",
    "## Task 4.1: Environment setup\n",
    "\n",
    "Install packages and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install-dependencies\n",
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker_datawrangler\n",
    "import time\n",
    "from sagemaker.session import Session\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "prefix = 'sagemaker/mlasms'\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model from the training and tuning lab in the default Amazon Simple Storage Service (Amazon S3) bucket. Set up a model using **create_model** and configure **ModelDataUrl** to reference the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set-up-model\n",
    "# Upload the model to your Amazon S3 bucket\n",
    "s3_client.upload_file(Filename=\"model.tar.gz\", Bucket=bucket, Key=f\"{prefix}/models/model.tar.gz\")\n",
    "\n",
    "# Set a date to use in the model name\n",
    "create_date = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "model_name = 'income-model-{}'.format(create_date)\n",
    "\n",
    "# Retrieve the container image\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    region=boto3.Session().region_name, \n",
    "    framework='xgboost', \n",
    "    version='1.5-1'\n",
    ")\n",
    "\n",
    "# Set up the model\n",
    "income_model = sm.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': f's3://{bucket}/{prefix}/models/model.tar.gz',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the asynchronous records to the default Amazon S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload-dataset\n",
    "s3_client.upload_file(Filename=\"asynchronous_records.csv\", Bucket=bucket, Key=f\"{prefix}/asynchronous_records.csv\", ExtraArgs={\"ContentType\": \"text/csv;charset=utf-8\"})\n",
    "input_location = f\"s3://{bucket}/{prefix}/asynchronous_records.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.2: Create an endpoint from the provided synthesized, retrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Asynchronous Inference is a capability in SageMaker that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1 GB), long processing times (up to 15 minutes), and near real-time latency requirements. With Asynchronous Inference, you can reduce costs by autoscaling the instance count to zero when there are no requests to process. Therefore, you only pay when your endpoint is processing requests.\n",
    "\n",
    "There are three steps to creating an asynchronous endpoint using the SageMaker Python SDK. These are the same steps used for the real-time and serverless endpoints, but the steps have different configurations:\n",
    "1. Create a SageMaker model in SageMaker.\n",
    "2. Create an endpoint configuration for an HTTPS endpoint.\n",
    "3. Create an HTTPS endpoint.\n",
    "\n",
    "You have already created a model. You are now ready to create an endpoint configuration and an endpoint. \n",
    "\n",
    "First, set up the endpoint configuration name and the instance type that you want to use. Then, call the CreateEndpointConfig API.\n",
    "\n",
    "To create an endpoint configuration, you need to set the following options:\n",
    "- **VariantName**: The name of the production variant (one or more models in production).\n",
    "- **ModelName**: The name of the model that you want to host. This is the name that you specified when you created the model.\n",
    "- **InstanceType**: The compute instance type.\n",
    "- **S3OutputPath**: The location to upload response outputs to when no location is provided in the request.\n",
    "- **MaxConcurrentInvocationsPerInstance**: (Optional) The maximum number of concurrent requests sent by the SageMaker client to the model container.\n",
    "\n",
    "Optionally, you can also set a NotificationConfig, selecting an Amazon Simple Notification Service (Amazon SNS) topic that posts notifications when an inference request is successful or if it fails. In this lab, you do not need to set up this option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create-endpoint-configuration \n",
    "# Create an endpoint config name. Here you create one based on the date so you can search endpoints based on creation time.\n",
    "endpoint_config_name = 'income-model-asynchronous-endpoint-{}'.format(create_date)                              \n",
    "output_location = f\"s3://{bucket}/{prefix}/output\"\n",
    "\n",
    "endpoint_config_response = sm.create_endpoint_config(\n",
    "   EndpointConfigName=endpoint_config_name,\n",
    "   ProductionVariants=[\n",
    "        {\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"variant1\", # The name of the production variant.\n",
    "            \"InstanceType\": \"ml.m5.xlarge\", # Specify the compute instance type.\n",
    "            \"InitialInstanceCount\": 1 # Number of instances to launch initially.\n",
    "            \n",
    "        } \n",
    "    ],\n",
    "    AsyncInferenceConfig={\n",
    "        \"OutputConfig\": {\n",
    "            # Location to upload response outputs when no location is provided in the request.\n",
    "            \"S3OutputPath\": output_location\n",
    "        },\n",
    "        \"ClientConfig\": {\n",
    "            # (Optional) Specify the max number of inflight invocations per instance\n",
    "            # If no value is provided, Amazon SageMaker chooses an optimal value for you\n",
    "            \"MaxConcurrentInvocationsPerInstance\": 4\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created EndpointConfig: {endpoint_config_response['EndpointConfigArn']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create an endpoint. When you create an asynchronous endpoint, SageMaker launches the machine learning (ML) compute instances and deploys the model as specified in the configuration. Refer to [Asynchronous Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html) for more information about the options available to you with asynchronous endpoints.\n",
    "\n",
    "When the endpoint is in service, the helper function prints the endpoint Amazon Resource Name (ARN). Endpoint creation can take as long as 7 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create-endpoint\n",
    "# The name of the endpoint. The name must be unique within an AWS Region in your AWS account.\n",
    "endpoint_name = '{}-name'.format(endpoint_config_name)\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    EndpointConfigName=endpoint_config_name\n",
    ") \n",
    "\n",
    "def wait_for_endpoint_creation_complete(endpoint):\n",
    "    \"\"\"Helper function to wait for the completion of creating an endpoint\"\"\"\n",
    "    response = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = response.get(\"EndpointStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Endpoint Creation\")\n",
    "        time.sleep(15)\n",
    "        response = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "        status = response.get(\"EndpointStatus\")\n",
    "\n",
    "    if status != \"InService\":\n",
    "        print(f\"Failed to create endpoint, response: {response}\")\n",
    "        failureReason = response.get(\"FailureReason\", \"\")\n",
    "        raise SystemExit(\n",
    "            f\"Failed to create endpoint {create_endpoint_response['EndpointArn']}, status: {status}, reason: {failureReason}\"\n",
    "        )\n",
    "    print(f\"Endpoint {create_endpoint_response['EndpointArn']} successfully created.\")\n",
    "\n",
    "wait_for_endpoint_creation_complete(endpoint=create_endpoint_response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SageMaker Studio, you can view the endpoint details under the **Endpoints** tab.\n",
    "\n",
    "The next step opens a new tab in SageMaker Studio. To follow these directions, use one of the following options:\n",
    "- **Option 1:** View the tabs side by side. To create a split screen view from the main SageMaker Studio window, either drag the **asynchronous_inference.ipynb** tab to the side or choose (right-click) the **asynchronous_inference.ipynb** tab and choose **New View for Notebook**. You can now have the directions visible as you explore the endpoint.\n",
    "- **Option 2:** Switch between the SageMaker Studio tabs to follow these instructions. When you are finished exploring the endpoint, return to the notebook by choosing the **asynchronous_inference.ipynb** tab.\n",
    "\n",
    "1. Choose the **SageMaker Home** icon.\n",
    "2. Choose **Deployments**.\n",
    "3. Choose **Endpoints**.\n",
    "\n",
    "SageMaker Studio displays the **Endpoints** tab.\n",
    "\n",
    "4. Select the endpoint which has **income-model-asynchronous-** in the **Name** column.\n",
    "\n",
    "If the endpoint does not appear, choose the refresh icon until the endpoint appears in the list.\n",
    "\n",
    "SageMaker Studio displays the **ENDPOINT DETAILS** tab.\n",
    "\n",
    "5. Choose the **AWS settings** tab.\n",
    "\n",
    "If you opened the endpoint before it finished creating, choose the refresh icon until the **Endpoint status** changes from *Creating* to *InService*.\n",
    "\n",
    "The **Endpoint type** is listed as **Async**. The **Endpoint runtime settings** section shows the configurations that you chose earlier in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.3: Invoke an endpoint for an asynchronous inference with asynchronous customer records\n",
    "\n",
    "After you deploy your model using SageMaker hosting services, you can test your model on that endpoint by sending it test data.\n",
    "\n",
    "To test an asynchronous endpoint, you must include the Amazon S3 input location in the API call. For this lab, there is a asynchronous_records.csv file in the default SageMaker S3 bucket with 100 customer records that you can test the endpoint with. If the action is successful, the service sends back an HTTP 202 response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send-test-file\n",
    "response = sagemaker_runtime.invoke_endpoint_async(\n",
    "                            EndpointName=endpoint_name, \n",
    "                            InputLocation=input_location)\n",
    "\n",
    "print(response)\n",
    "\n",
    "output_key = response['OutputLocation'].split(\"/\", 3)[3]\n",
    "print('\\nThe output key is: {}'.format(output_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output location to see if the inference has been processed. When it has been processed, print out the prediction scores for all the customers included in the invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get-output\n",
    "def get_output():\n",
    "    while True:\n",
    "        try:\n",
    "            return sagemaker.session.Session().read_s3_file(bucket=bucket, key_prefix=output_key)\n",
    "        except ClientError as e:\n",
    "            if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "                print(\"Waiting for output...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "output = get_output()\n",
    "print(f\"Predictions for the 100 customers: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.4: Delete the endpoint\n",
    "\n",
    "Cleaning up an endpoint can be accomplished in three steps. First, delete the endpoint. Then, delete the endpoint configuration. Finally, if you no longer need the model that you deployed, delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete-resources\n",
    "# Delete endpoint\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "# Delete endpoint configuration\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "                   \n",
    "# Delete model\n",
    "sm.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Congratulations! You have used SageMaker to successfully create an asynchronous endpoint, using the SageMaker Python SDK, and to invoke the endpoint.\n",
    "\n",
    "The next task of the lab focuses on batch transform.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with **Task 5: Use batch transform to get inferences from a large dataset**."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e45558c452cedcb26631315a9b3b77e80a9c32d662ed25df58964b99bc5b9b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
