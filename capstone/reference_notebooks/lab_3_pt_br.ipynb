{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarefa 3: Explorar e consultar dados do kernel PySpark\n",
    "\n",
    "Neste notebook, você vai:\n",
    "\n",
    "- Aprender a usar um notebook do Studio para procurar visualmente um cluster do Amazon EMR, autenticar-se nele e conectar-se a ele.\n",
    "- Explorar e consultar os dados usando o Spark ML. \n",
    "- Monitorar os trabalhos do Spark com a IU do Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarefa 3.1: Informações da sessão\n",
    "\n",
    "Como você está usando o kernel do PySpark, é possível usar a mágica %%info do PySpark para exibir as informações da sessão atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarefa 3.2: Explorar e consultar os dados\n",
    "\n",
    "Quando o kernel do PySpark é usado, um SparkContext e um HiveContext são criados automaticamente após a conexão com um cluster do EMR. Você pode usar o HiveContext para consultar dados na tabela Hive e disponibilizá-los em um dataframe do Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use o HiveContext para consultar o Hive e observar os bancos de dados e as tabelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query-hive\n",
    "\n",
    "sqlContext = HiveContext(sqlContext)\n",
    "\n",
    "dbs = sqlContext.sql(\"show databases\")\n",
    "dbs.show()\n",
    "\n",
    "tables = sqlContext.sql(\"show tables\")\n",
    "tables.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulte a tabela de dados de adultos e coloque os dados em um dataframe do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load-data\n",
    "\n",
    "adult_df = sqlContext.sql(\"select * from adult_data\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use o dataframe para consultar a forma e visualizar as cinco primeiras linhas do conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-shape\n",
    "\n",
    "print((adult_df.count(), len(adult_df.columns)))\n",
    "\n",
    "#Show first 5 rows \n",
    "adult_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para gerar uma saída mais limpa, converta o dataframe do Spark em um dataframe do Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert-dataframe\n",
    "\n",
    "adult_df.limit(5).toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns algoritmos de machine learning (ML), como regressão linear, exigem recursos numéricos. O conjunto de dados de adultos usado neste laboratório inclui recursos categóricos como **workclass**, **education**, **occupation**, **marital status**, **relationship**, **race** e **sex**.\n",
    "\n",
    "O bloco de código a seguir ilustra como usar o StringIndexer e o OneHotEncoderEstimator para converter variáveis categóricas em um conjunto de variáveis numéricas que usa os valores 0 e 1.\n",
    "\n",
    "- O StringIndexer converte uma coluna de valores de string em uma coluna de índices de rótulo. \n",
    "- O OneHotEncoderEstimator mapeia uma coluna de índices de categoria para uma coluna de vetores binários, com no máximo um \"1\" em cada linha que indica o índice de categoria dessa linha.\n",
    "\n",
    "A codificação one-hot no Spark é um processo de duas etapas. Primeiro você usa o **StringIndexer**, depois o **OneHotEncoder**.\n",
    "\n",
    "Consulte [StringIndexer](https://spark.apache.org/docs/latest/ml-features.html#stringindexer) e [OneHotEncoder](https://spark.apache.org/docs/latest/ml-features.html#onehotencoder) para saber mais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert-variables\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n",
    "\n",
    "categorical_variables = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"-index\") for column in categorical_variables]\n",
    "\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "    outputCols=[\"{0}-encoded\".format(indexer.getOutputCol()) for indexer in indexers]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe VectorAssembler usa várias colunas como entrada. Ela gera uma só coluna que contém uma matriz de valores.\n",
    "\n",
    "Consulte [VectorAssembler](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler) para saber mais sobre esse assembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector-assembler\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=encoder.getOutputCols(),\n",
    "    outputCol=\"categorical-features\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um pipeline é uma lista ordenada de transformadores e estimadores. É possível definir um pipeline para automatizar e garantir a repetição das transformações a serem aplicadas a um conjunto de dados. Nesta etapa, você define o pipeline e depois o aplica ao conjunto de dados.\n",
    "\n",
    "Semelhante ao **StringIndexer**, um pipeline é um **estimator**. O método pipeline.fit() retorna um **PipelineModel**, que é um **transformador**.\n",
    "\n",
    "Consulte [Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html#pipeline) para saber mais sobre os pipelines de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark-pipelines\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define the pipeline based on the stages created in previous steps.\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler])\n",
    "\n",
    "# Define the pipeline model.\n",
    "pipelineModel = pipeline.fit(adult_df)\n",
    "\n",
    "# Apply the pipeline model to the dataset.\n",
    "adult_df = pipelineModel.transform(adult_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine todas as diferentes colunas que foram criadas na etapa anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print-schema\n",
    "\n",
    "adult_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a aplicação das transformações, uma só coluna conterá uma matriz com todas as variáveis categóricas codificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-categorical-features\n",
    "\n",
    "adult_df.select('categorical-features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, codifique o rótulo de destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode-target\n",
    "\n",
    "indexer = StringIndexer(inputCol='income', outputCol='label')\n",
    "\n",
    "adult_df = indexer.fit(adult_df).transform(adult_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarefa 3.3: Monitorar e depurar com a IU do Spark\n",
    "\n",
    "Nesta seção, você vai usar a IU do Spark para monitorar e inspecionar o desempenho dos trabalhos do Spark executados na etapa anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenha as informações da sessão atual do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontre os hiperlinks da **IU do Spark** e do **Log do driver**. Neste laboratório, o link **Log do driver** está inativo. O URL pré-assinado da **IU do Spark** é gerado no momento da conexão com o cluster do EMR. Ao clicar nesse link, você acessará a IU do Spark para inspecionar as execuções de trabalho do Spark em um navegador da web. Essas métricas ajudam no ajuste de desempenho. \n",
    "\n",
    "Veja alguns recursos importantes a serem procurados no servidor Spark:\n",
    "- A aba **Jobs** (Trabalhos) mostra o status de todos os trabalhos do Spark nessa aplicação do Spark.\n",
    "- Na seção de resumo, a seção **Event Timeline** (Linha do tempo de eventos) mostra os vários estágios da execução.\n",
    "- A seção **Completed Jobs** (Trabalhos concluídos) é mostrada em formato tabular. Na seção **Completed Jobs** (Trabalhos concluídos), você pode escolher um trabalho para examinar as informações sobre os estágios de tarefas que ele contém.\n",
    "- Usando a **DAG Visualization** (Visualização DAG), você pode explorar as tarefas que já foram executadas. Como na visualização **Event Timeline** (Linha do tempo de eventos), usando a **DAG Visualization** (Visualização DAG), você pode escolher um estágio e expandir os detalhes que ele contém."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusão\n",
    "\n",
    "Parabéns! Você se conectou a um cluster do EMR e explorou e consultou os dados usando o PySpark com sucesso.\n",
    "\n",
    "## Limpeza\n",
    "\n",
    "Você concluiu este notebook. Passe para a próxima parte do laboratório da seguinte forma:\n",
    "\n",
    "- Feche este arquivo de notebook.\n",
    "- Retorne à sessão do laboratório e continue com a **Conclusão**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
