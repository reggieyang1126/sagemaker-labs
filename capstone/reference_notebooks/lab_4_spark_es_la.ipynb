{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea 2: realizar el procesamiento de datos con SageMaker Processing\n",
    "\n",
    "En este cuaderno, configure el entorno necesario para ejecutar una aplicación básica de Apache Spark con Amazon SageMaker Processing. Al usar Apache Spark en SageMaker Processing, puede ejecutar trabajos de Spark sin tener que aprovisionar un clúster de Amazon EMR. Luego, defina y ejecute un trabajo de Spark usando la clase **PySparkProcessor** del **SDK de Python para SageMaker**. Por último, valide los resultados del procesamiento de datos guardados en Amazon Simple Storage Service (Amazon S3).\n",
    "\n",
    "El script de procesamiento realiza un procesamiento de datos básico, como la indexación de cadenas, la codificación one-hot, el ensamblaje de vectores y una división 80-20 de los datos procesados para entrenar y validar los conjuntos de datos.     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea 2.1: configuración del entorno\n",
    "\n",
    "Instale el paquete del SDK para Python de SageMaker más reciente y otras dependencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install awscli --upgrade\n",
    "%pip install boto3 --upgrade\n",
    "%pip install -U \"sagemaker>2.0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Después de actualizar el SDK, reinicie el kernel de su cuaderno. Para ello, seleccione el ícono **Restart Kernel** (Reiniciar kernel) en la barra de herramientas del cuaderno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ahora, importe las bibliotecas requeridas, obtenga el rol de ejecución para ejecutar el trabajo de SageMaker Processing y configure el bucket de Amazon S3 para almacenar las salidas del trabajo de Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install-dependencies\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "#Execution role to run the SageMaker Processing job\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"SageMaker Execution Role: \", role)\n",
    "\n",
    "#S3 bucket to read the Spark processing script and writing processing job outputs\n",
    "s3 = boto3.resource('s3')\n",
    "for buckets in s3.buckets.all():\n",
    "    if 'labdatabucket' in buckets.name:\n",
    "        bucket = buckets.name\n",
    "print(\"Bucket: \", bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se produce un error, asegúrese de reiniciar el kernel de su cuaderno. Para ello, seleccione el ícono **Restart kernel** (Reiniciar kernel) en la barra de herramientas del cuaderno. Luego, vuelva a ejecutar la celda."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea 2.2: ejecutar el trabajo de SageMaker Processing\n",
    "\n",
    "En esta tarea, importe y revise el conjunto de datos preprocesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import-data\n",
    "prefix = 'data/input'\n",
    "\n",
    "S3Downloader.download(s3_uri=f\"s3://{bucket}/{prefix}/spark_adult_data.csv\", local_path= 'data/')\n",
    "\n",
    "shape=pd.read_csv(\"data/spark_adult_data.csv\", header=None)\n",
    "shape.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, cree la clase PySparkProcessor de Sagemaker Spark para definir una aplicación de Spark como un trabajo de procesamiento. Consulte [Sagemaker Spark PySparkProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor) para obtener más información sobre esta clase.\n",
    "\n",
    "Para crear la clase PySparkProcessor, configure los siguientes parámetros:\n",
    "- **base_job_name**: prefijo para el nombre del trabajo de procesamiento\n",
    "- **framework_version**: versión de PySpark para SageMaker\n",
    "- **role**: rol de ejecución de SageMaker\n",
    "- **instance_count**: cantidad de instancias para ejecutar el trabajo de procesamiento\n",
    "- **instance_type**: tipo de instancia de Amazon Elastic Compute Cloud (Amazon EC2) usada para el trabajo de procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark-processor\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# create a PySparkProcessor\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-preprocessor\",\n",
    "    framework_version=\"3.1\", # Spark version\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, use el método de ejecución PySparkProcessor para ejecutar el script **pyspark_preprocessing.py** como un trabajo de procesamiento. Consulte [método de ejecución de PySparkProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor.run) para obtener más información sobre este método. Para este laboratorio, las transformaciones de datos, como indexación de cadenas y codificación one-hot, se realizan en las funciones categóricas.\n",
    "\n",
    "Para ejecutar el trabajo de procesamiento, configure los siguientes parámetros:\n",
    "- **submit_app**: ruta del script de preprocesamiento \n",
    "- **outputs**: ruta de salida para el script de preprocesamiento (ubicaciones de salida de Amazon S3)\n",
    "- **arguments**: argumentos de línea de comandos para el script de preprocesamiento (como las ubicaciones de entrada y salida de Amazon S3)\n",
    "\n",
    "El tiempo para completar este trabajo de procesamiento es de aproximadamente 5 minutos. Mientras se ejecuta el trabajo, puede revisar la fuente para el script del preprocesamiento (que se configuró como parte de este laboratorio) abriendo el archivo **pyspark_preprocessing.py** desde el navegador de archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing-job\n",
    "import os\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Amazon S3 path prefix\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "input_raw_data_prefix = \"data/input\"\n",
    "output_preprocessed_data_prefix = \"data/output\"\n",
    "scripts_prefix = \"scripts/smstudiofiles\"\n",
    "logs_prefix = \"logs\"\n",
    "\n",
    "# Run the processing job\n",
    "spark_processor.run(\n",
    "    submit_app='s3://' + os.path.join(bucket, scripts_prefix, \"pyspark_preprocessing.py\"),\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", \n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"train\")),\n",
    "        ProcessingOutput(output_name=\"validation_data\", \n",
    "                         source=\"/opt/ml/processing/validation\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"validation\")),\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--s3_input_bucket\", bucket,\n",
    "        \"--s3_input_key_prefix\", input_raw_data_prefix,\n",
    "        \"--s3_output_bucket\", bucket,\n",
    "        \"--s3_output_key_prefix\", output_preprocessed_data_prefix],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, logs_prefix),\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "print(\"Spark Processing Job Completed.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea 2.3: validar los resultados del procesamiento de datos\n",
    "\n",
    "Valide la salida del trabajo de procesamiento de datos que ejecutó mediante la revisión de las primeras cinco filas de los conjuntos de datos de salida de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-train-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/train/train_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-validation-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/validation/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/validation/validation_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "¡Felicitaciones! Usó SageMaker Processing para crear correctamente un trabajo de procesamiento de Spark usando el SDK para Python de SageMaker y ejecutó un trabajo de procesamiento.\n",
    "\n",
    "En la siguiente tarea del laboratorio, se enfocará en realizar el procesamiento de datos con SageMaker Processing y el contenedor scikit-learn integrado.\n",
    "\n",
    "### Limpieza\n",
    "\n",
    "Ha completado este cuaderno. Para ir a la siguiente parte del laboratorio, complete estos pasos:\n",
    "\n",
    "- Cierre este archivo de cuaderno.\n",
    "- Regrese a la sesión de laboratorio y continúe con la **Tarea 3: realizar el procesamiento de datos con SageMaker Processing y el contenedor integrado de scikit-learn**."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
