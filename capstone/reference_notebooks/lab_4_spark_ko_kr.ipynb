{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과제 2: SageMaker Processing을 사용하여 데이터 처리 수행\n",
    "\n",
    "이 노트북에서는 Amazon SageMaker Processing을 사용하여 기본적인 Apache Spark 애플리케이션을 실행하는 데 필요한 환경을 설정합니다. SageMaker Processing에서 Apache Spark를 사용하면 Amazon EMR 클러스터를 프로비저닝하지 않고도 Spark 작업을 실행할 수 있습니다. 그런 다음 **SageMaker Python SDK**의 **PySparkProcessor** 클래스를 사용하여 Spark 작업을 정의하고 실행합니다. 그리고 마지막으로 Amazon Simple Storage Service(Amazon S3)에 저장된 데이터 처리 결과를 검증합니다.\n",
    "\n",
    "처리 스크립트는 문자열 인덱싱, 원-핫 인코딩, 벡터 어셈블리, 처리된 데이터를 훈련 데이터 집합과 검증 데이터 집합으로 분할(80-20 비율) 등의 몇 가지 기본적인 데이터 처리를 수행합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 2.1: 환경 설정\n",
    "\n",
    "최신 SageMaker Python SDK 패키지와 기타 종속성을 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install awscli --upgrade\n",
    "%pip install boto3 --upgrade\n",
    "%pip install -U \"sagemaker>2.0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SDK를 업그레이드한 후 노트북 커널을 다시 시작합니다. \n",
    "\n",
    "1. 노트북 도구 모음에서 **Restart kernel** 아이콘을 선택합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "이제 필요한 라이브러리를 가져오고 SageMaker 처리 작업을 실행할 실행 역할을 가져온 다음, Spark 작업 출력을 저장할 Amazon S3 버킷을 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install-dependencies\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "#Execution role to run the SageMaker Processing job\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"SageMaker Execution Role: \", role)\n",
    "\n",
    "#S3 bucket to read the Spark processing script and writing processing job outputs\n",
    "s3 = boto3.resource('s3')\n",
    "for buckets in s3.buckets.all():\n",
    "    if 'labdatabucket' in buckets.name:\n",
    "        bucket = buckets.name\n",
    "print(\"Bucket: \", bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오류가 발생하면 노트북 도구 모음에서 **Restart kernel** 아이콘을 선택하여 노트북 커널을 다시 시작했는지 확인합니다. 그런 다음, 셀을 재실행합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 2.2: SageMaker 처리 작업 실행\n",
    "\n",
    "이 과제에서는 전처리된 데이터 집합을 가져와서 검토합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import-data\n",
    "prefix = 'data/input'\n",
    "\n",
    "S3Downloader.download(s3_uri=f\"s3://{bucket}/{prefix}/spark_adult_data.csv\", local_path= 'data/')\n",
    "\n",
    "shape=pd.read_csv(\"data/spark_adult_data.csv\", header=None)\n",
    "shape.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 SageMaker Spark PySparkProcessor 클래스를 생성하여 Spark 애플리케이션을 처리 작업으로 정의하고 실행합니다. 이 클래스에 관한 자세한 내용은 [SageMaker Spark PySparkProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor)를 참조하세요.\n",
    "\n",
    "PySparkProcessor 클래스를 생성할 때는 다음 파라미터를 구성합니다.\n",
    "- **base_job_name**: 처리 작업 이름의 접두사\n",
    "- **framework_version**: SageMaker PySpark 버전\n",
    "- **role**: SageMaker 실행 역할\n",
    "- **instance_count**: 처리 작업을 실행할 인스턴스의 수\n",
    "- **instance_type**: 처리 작업에 사용되는 Amazon Elastic Compute Cloud(Amazon EC2) 인스턴스의 유형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark-processor\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# create a PySparkProcessor\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-preprocessor\",\n",
    "    framework_version=\"3.1\", # Spark version\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로는 PySparkProcessor run 메서드를 사용하여 **pyspark_preprocessing.py** 스크립트를 처리 작업으로 실행합니다. 이 메서드에 관한 자세한 내용은 [PySparkProcessor run method](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor.run)를 참조하세요. 이 실습에서는 범주별 특성에 대해 문자열 인덱싱, 원-핫 인코딩 등의 데이터 변환을 수행합니다.\n",
    "\n",
    "처리 작업을 실행하기 위해 다음 파라미터를 구성합니다.\n",
    "- **submit_app**: 처리 스크립트의 경로 \n",
    "- **outputs**: 전처리 스크립트의 출력 경로(Amazon S3 출력 위치)\n",
    "- **arguments**: 전처리 스크립트의 명령줄 인수(예: Amazon S3 입력 및 출력 위치)\n",
    "\n",
    "처리 작업을 완료하려면 약 5분이 소요됩니다. 작업이 실행되는 동안 파일 브라우저에서 **pyspark_preprocessing.py** 파일을 열면 이 실습의 일부분으로 사전 구성된 전처리 스크립트의 소스를 검토할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing-job\n",
    "import os\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Amazon S3 path prefix\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "input_raw_data_prefix = \"data/input\"\n",
    "output_preprocessed_data_prefix = \"data/output\"\n",
    "scripts_prefix = \"scripts/smstudiofiles\"\n",
    "logs_prefix = \"logs\"\n",
    "\n",
    "# Run the processing job\n",
    "spark_processor.run(\n",
    "    submit_app='s3://' + os.path.join(bucket, scripts_prefix, \"pyspark_preprocessing.py\"),\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", \n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"train\")),\n",
    "        ProcessingOutput(output_name=\"validation_data\", \n",
    "                         source=\"/opt/ml/processing/validation\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"validation\")),\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--s3_input_bucket\", bucket,\n",
    "        \"--s3_input_key_prefix\", input_raw_data_prefix,\n",
    "        \"--s3_output_bucket\", bucket,\n",
    "        \"--s3_output_key_prefix\", output_preprocessed_data_prefix],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, logs_prefix),\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "print(\"Spark Processing Job Completed.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제 2.3: 데이터 처리 결과 검증\n",
    "\n",
    "실행한 데이터 처리 작업의 출력을 검증합니다. 이렇게 하려면 훈련 및 검증 출력 데이터 집합의 첫 5개 행을 검토합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-train-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/train/train_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-validation-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/validation/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/validation/validation_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마무리\n",
    "\n",
    "축하합니다! SageMaker Processing에서 SageMaker Python SDK를 사용하여 Spark 처리 작업을 생성했으며 처리 작업을 실행했습니다.\n",
    "\n",
    "이 실습의 다음 과제에서는 SageMaker Processing 및 기본 제공 scikit-learn 컨테이너를 사용하여 데이터 처리를 수행하는 방법을 중점적으로 살펴봅니다.\n",
    "\n",
    "### 정리\n",
    "\n",
    "이 노트북을 완료했습니다. 실습의 다음 부분으로 이동하려면 다음을 수행합니다.\n",
    "\n",
    "- 이 노트북 파일을 닫습니다.\n",
    "- 실습 세션으로 돌아가 **과제 3: SageMaker Processing 및 기본 제공 scikit-learn 컨테이너를 사용하여 데이터 처리 수행**을 계속 진행합니다."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
