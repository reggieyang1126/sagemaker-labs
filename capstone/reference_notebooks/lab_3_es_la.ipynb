{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 3: explorar y hacer consultas sobre los datos del kernel de PySpark\n",
    "\n",
    "En este cuaderno, hará lo siguiente:\n",
    "\n",
    "- Aprenderá a usar un cuaderno de Studio para explorar visualmente, autenticarse y conectarse a un clúster de Amazon EMR.\n",
    "- Explorará y hará consultas sobre los datos con Spark ML. \n",
    "- Supervisará sus trabajos de Spark con la interfaz de usuario de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 3.1: información sobre la sesión\n",
    "\n",
    "Como está usando el kernel de PySpark, puede utilizar magic %%info de PySpark para mostrar información sobre la sesión actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 3.2: explorar y hacer consultas sobre los datos\n",
    "\n",
    "Al usar el kernel de PySpark, se crean automáticamente un SparkContext y un HiveContext después de conectarse a un clúster de EMR. Puede usar HiveContext para hacer consultas sobre los datos en la tabla de Hive y hacer que estén disponibles en un DataFrame de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use HiveContext para hacer consultas sobre Hive y observar las bases de datos y tablas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query-hive\n",
    "\n",
    "sqlContext = HiveContext(sqlContext)\n",
    "\n",
    "dbs = sqlContext.sql(\"show databases\")\n",
    "dbs.show()\n",
    "\n",
    "tables = sqlContext.sql(\"show tables\")\n",
    "tables.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haga consultas sobre la tabla de datos Adult y lleve los datos a un DataFrame de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load-data\n",
    "\n",
    "adult_df = sqlContext.sql(\"select * from adult_data\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use el DataFrame para revisar la forma y visualice las primeras cinco filas del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-shape\n",
    "\n",
    "print((adult_df.count(), len(adult_df.columns)))\n",
    "\n",
    "#Show first 5 rows \n",
    "adult_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que el resultado sea más claro, convierta el DataFrame de Spark en un DataFrame de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert-dataframe\n",
    "\n",
    "adult_df.limit(5).toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos algoritmos de machine learning (ML), como la regresión lineal, requieren funciones numéricas. El conjunto de datos Adult que usa en este laboratorio incluye funciones categóricas, como **workclass**, **education**, **occupation**, **marital status**, **relationship**, **race** y **sex** (clase de trabajo, educación, ocupación, estado civil, relación, raza y sexo).\n",
    "\n",
    "El siguiente bloque de código muestra cómo usar StringIndexer y OneHotEncoderEstimator para convertir variables categóricas en un conjunto de variables numéricas que asumen valores de 0 y 1.\n",
    "\n",
    "- StringIndexer convierte una columna de valores de cadena en una columna de índices de etiquetas. \n",
    "- OneHotEncoderEstimator asigna una columna de índices de categorías a una columna de vectores binarios, con un máximo de un \"1\" en cada fila, que indica el índice de categoría de esa fila.\n",
    "\n",
    "La codificación one-hot en Spark es un proceso de dos pasos. Primero, se usa **StringIndexer** y, luego, **OneHotEncoder**.\n",
    "\n",
    "Consulte [StringIndexer](https://spark.apache.org/docs/latest/ml-features.html#stringindexer) y [OneHotEncoder](https://spark.apache.org/docs/latest/ml-features.html#onehotencoder) para obtener más información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert-variables\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n",
    "\n",
    "categorical_variables = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"-index\") for column in categorical_variables]\n",
    "\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "    outputCols=[\"{0}-encoded\".format(indexer.getOutputCol()) for indexer in indexers]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase VectorAssembler toma varias columnas como entrada. Como resultado, arroja una sola columna que contiene una matriz de valores.\n",
    "\n",
    "Consulte [VectorAssembler](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler) para obtener más información sobre este ensamblador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector-assembler\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=encoder.getOutputCols(),\n",
    "    outputCol=\"categorical-features\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una canalización es una lista ordenada de transformadores y estimadores. Puede definir una canalización para automatizar y garantizar la repetición de las transformaciones que se aplicarán a un conjunto de datos. En este paso, debe definir la canalización y, luego, aplicarla al conjunto de datos.\n",
    "\n",
    "Al igual que **StringIndexer**, una canalización es un **estimador**. El método pipeline.fit() muestra un **PipelineModel**, que es un **transformador**.\n",
    "\n",
    "Consulte [Canalizaciones](https://spark.apache.org/docs/latest/ml-pipeline.html#pipeline) para obtener más información sobre las canalizaciones de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark-pipelines\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define the pipeline based on the stages created in previous steps.\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler])\n",
    "\n",
    "# Define the pipeline model.\n",
    "pipelineModel = pipeline.fit(adult_df)\n",
    "\n",
    "# Apply the pipeline model to the dataset.\n",
    "adult_df = pipelineModel.transform(adult_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revise todas las columnas diferentes que se crearon en el paso anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print-schema\n",
    "\n",
    "adult_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de aplicar las transformaciones, una sola columna contendrá una matriz con todas las variables categóricas codificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-categorical-features\n",
    "\n",
    "adult_df.select('categorical-features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, codifique la etiqueta objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode-target\n",
    "\n",
    "indexer = StringIndexer(inputCol='income', outputCol='label')\n",
    "\n",
    "adult_df = indexer.fit(adult_df).transform(adult_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 3.3: supervisar y depurar con la interfaz de usuario de Spark\n",
    "\n",
    "En esta sección, usará la interfaz de usuario de Spark para supervisar e inspeccionar el rendimiento de los trabajos de Spark que ejecutó en los pasos anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenga información sobre la sesión actual de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede encontrar hipervínculos para la **interfaz de usuario de Spark** y para el **registro del controlador**. En este laboratorio, el vínculo al **registro del controlador** está inactivo. La URL prefirmada de la **interfaz de usuario de Spark** se genera en el momento de la conexión al clúster de EMR. Si elige este vínculo, se lo dirigirá a la interfaz de usuario de Spark para inspeccionar las ejecuciones de trabajos de Spark en un navegador web. Estas métricas son útiles para ajustar el rendimiento. \n",
    "\n",
    "A continuación, se presentan algunas de las funciones importantes que debe buscar en el servidor de Spark:\n",
    "- La pestaña **Jobs** (Trabajos) muestra el estado de todos los trabajos de Spark en esta aplicación de Spark.\n",
    "- En la sección de resumen, la sección **Event Timeline** (Cronología del evento) muestra las diversas etapas de la ejecución.\n",
    "- La sección **Completed Jobs** (Trabajos finalizados) se muestra en formato tabular. En la sección **Completed Jobs** (Trabajos finalizados), puede elegir un trabajo para revisar información sobre las etapas de las tareas incluidas en esta.\n",
    "- Con **DAG Visualization** (Visualización de DAG), puede explorar las tareas que ejecutó antes. Al igual que con la vista de **Event Timeline** (Cronología del evento), con **DAG visualization** (Visualización de DAG), puede elegir una etapa y expandir los detalles incluidos en esta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "¡Felicitaciones! Se conectó correctamente a un clúster de EMR y exploró e hizo consultas sobre los datos con PySpark.\n",
    "\n",
    "### Limpieza\n",
    "\n",
    "Ha completado este cuaderno. Para ir a la siguiente parte del laboratorio, haga lo siguiente:\n",
    "\n",
    "- Cierre este archivo de cuaderno.\n",
    "- Regrese a la sesión de laboratorio y continúe con la **Conclusión**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
