{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarefa 2: Executar o processamento de dados com o SageMaker Processing\n",
    "\n",
    "Neste notebook, você vai definir o ambiente necessário para executar uma aplicação básica do Apache Spark usando o Amazon SageMaker Processing. Usando o Apache Spark no SageMaker Processing, você pode executar os trabalhos do Spark sem precisar provisionar um cluster do Amazon EMR. Depois, você vai definir e executar um trabalho do Spark usando a classe **PySparkProcessor** do **SDK Python do SageMaker**. Por fim, você vai validar os resultados do processamento de dados salvos no Amazon Simple Storage Service (Amazon S3).\n",
    "\n",
    "O script de processamento executa um processamento básico de dados, como a indexação de string, a codificação one-hot, a montagem de vetor e uma divisão 80-20 dos dados processados para treinar e validar conjuntos de dados.     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 2.1: Configuração do ambiente\n",
    "\n",
    "Instale o pacote mais recente do SDK Python do SageMaker e outras dependências."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install awscli --upgrade\n",
    "%pip install boto3 --upgrade\n",
    "%pip install -U \"sagemaker>2.0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Depois de atualizar o SDK, reinicie o kernel do notebook selecionando o ícone **Restart kernel** (Reiniciar kernel) na barra de ferramentas do notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Agora, importe as bibliotecas necessárias, faça com que a função de execução execute o trabalho do SageMaker Processing e defina o bucket do Amazon S3 para armazenar as saídas do trabalho do Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install-dependencies\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "#Execution role to run the SageMaker Processing job\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"SageMaker Execution Role: \", role)\n",
    "\n",
    "#S3 bucket to read the Spark processing script and writing processing job outputs\n",
    "s3 = boto3.resource('s3')\n",
    "for buckets in s3.buckets.all():\n",
    "    if 'labdatabucket' in buckets.name:\n",
    "        bucket = buckets.name\n",
    "print(\"Bucket: \", bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se houver erro, reinicie o kernel do notebook selecionando o ícone **Restart kernel** (Reiniciar kernel) na barra de ferramentas do notebook. Depois, execute a célula novamente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 2.2: Executar o trabalho de processamento do SageMaker\n",
    "\n",
    "Nesta tarefa, você vai importar e examinar o conjunto de dados pré-processado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import-data\n",
    "prefix = 'data/input'\n",
    "\n",
    "S3Downloader.download(s3_uri=f\"s3://{bucket}/{prefix}/spark_adult_data.csv\", local_path= 'data/')\n",
    "\n",
    "shape=pd.read_csv(\"data/spark_adult_data.csv\", header=None)\n",
    "shape.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois, crie a classe PySparkProcessor do Spark do SageMaker para definir e executar uma aplicação do Spark como um trabalho de processamento. Consulte [PySparkProcessor do Spark do SageMaker](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor) para saber mais sobre essa classe.\n",
    "\n",
    "Para criar a classe PySparkProcessor, configure os seguintes parâmetros:\n",
    "- **base_job_name**: prefixo do nome do trabalho de processamento\n",
    "- **framework_version**: versão do PySpark do SageMaker\n",
    "- **role**: função de execução do SageMaker\n",
    "- **instance_count**: número de instâncias para executar o trabalho de processamento\n",
    "- **instance_type**: tipo de instância do Amazon Elastic Compute Cloud (Amazon EC2) usado para o trabalho de processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark-processor\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# create a PySparkProcessor\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-preprocessor\",\n",
    "    framework_version=\"3.1\", # Spark version\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois, use o método de execução PySparkProcessor para executar o script **pyspark_preprocessing.py** como um trabalho de processamento. Consulte [método de execução PySparkProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor.run) para saber mais sobre esse método. Neste laboratório, as transformações de dados, como indexação de string e codificação one-hot são executadas nos recursos categóricos.\n",
    "\n",
    "Para executar o trabalho de processamento, configure os seguintes parâmetros:\n",
    "- **submit_app**: caminho do script de pré-processamento \n",
    "- **outputs**: caminho da saída do script de pré-processamento (locais de saída do Amazon S3)\n",
    "- **arguments**: argumentos de linha de comando para o script de pré-processamento (como os locais de entrada e saída do Amazon S3)\n",
    "\n",
    "O trabalho de processamento exige cerca de cinco minutos para ser concluído. Enquanto o trabalho estiver em execução, examine o código do script de pré-processamento (que foi pré-configurado como parte deste laboratório) abrindo o arquivo **pyspark_preprocessing.py** com o navegador de arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing-job\n",
    "import os\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Amazon S3 path prefix\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "input_raw_data_prefix = \"data/input\"\n",
    "output_preprocessed_data_prefix = \"data/output\"\n",
    "scripts_prefix = \"scripts/smstudiofiles\"\n",
    "logs_prefix = \"logs\"\n",
    "\n",
    "# Run the processing job\n",
    "spark_processor.run(\n",
    "    submit_app='s3://' + os.path.join(bucket, scripts_prefix, \"pyspark_preprocessing.py\"),\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", \n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"train\")),\n",
    "        ProcessingOutput(output_name=\"validation_data\", \n",
    "                         source=\"/opt/ml/processing/validation\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"validation\")),\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--s3_input_bucket\", bucket,\n",
    "        \"--s3_input_key_prefix\", input_raw_data_prefix,\n",
    "        \"--s3_output_bucket\", bucket,\n",
    "        \"--s3_output_key_prefix\", output_preprocessed_data_prefix],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, logs_prefix),\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "print(\"Spark Processing Job Completed.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 2.3: Validar os resultados do processamento de dados\n",
    "\n",
    "Valide a saída do trabalho de processamento de dados que você executou examinando as cinco primeiras linhas dos conjuntos de dados de saída de treinamento e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-train-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/train/train_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-validation-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/validation/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/validation/validation_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusão\n",
    "\n",
    "Parabéns! Você usou o SageMaker Processing para criar um trabalho de processamento do Spark usando o SDK Python do SageMaker e executou um trabalho de processamento com sucesso.\n",
    "\n",
    "A próxima tarefa do laboratório se concentra no processamento de dados com o SageMaker Processing e o contêiner scikit-learn integrado.\n",
    "\n",
    "## Limpeza\n",
    "\n",
    "Você concluiu este notebook. Passe para a próxima parte do laboratório da seguinte forma:\n",
    "\n",
    "- Feche este arquivo de notebook.\n",
    "- Retorne à sessão do laboratório e continue na **Tarefa 3: Executar o processamento de dados com o SageMaker Processing e o contêiner scikit-learn integrado**."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
