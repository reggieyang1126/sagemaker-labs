{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Use batch transform to get inferences from a large dataset\n",
    "\n",
    "## Task 5.1: Environment setup\n",
    "\n",
    "Install packages and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install-dependencies\n",
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker_datawrangler\n",
    "import time\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "prefix = 'sagemaker/mlasms'\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model from the training and tuning lab in the default Amazon Simple Storage Service (Amazon S3) bucket. Set up a model using **create_model** and configure **ModelDataUrl** to reference the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set-up-model\n",
    "# Upload the model and dataset to your Amazon S3 bucket\n",
    "s3_client.upload_file(Filename=\"model.tar.gz\", Bucket=bucket, Key=f\"{prefix}/models/model.tar.gz\")\n",
    "\n",
    "# Set a date to use in the model name\n",
    "create_date = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "model_name = 'income-model-{}'.format(create_date)\n",
    "\n",
    "# Retrieve the container image\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    region=boto3.Session().region_name, \n",
    "    framework='xgboost', \n",
    "    version='1.5-1'\n",
    ")\n",
    "\n",
    "# Set up the model\n",
    "income_model = sm.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': f's3://{bucket}/{prefix}/models/model.tar.gz',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the batch records to the default Amazon S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload-dataset\n",
    "s3_client.upload_file(Filename=\"batch_data.csv\", Bucket=bucket, Key=f\"{prefix}/batch_data.csv\", ExtraArgs={\"ContentType\": \"text/csv;charset=utf-8\"})\n",
    "batch_path = f\"s3://{bucket}/{prefix}/batch_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.2: Create a batch transform job\n",
    "\n",
    "Batch transform automatically manages the processing of large datasets within the limits of specified parameters. When a batch transform job starts, SageMaker initializes compute instances and distributes the inference or preprocessing workload between them. Batch transform partitions the Amazon S3 objects in the input by key and maps Amazon S3 objects to instances.\n",
    "\n",
    "Use batch transform when you need to get inferences from large datasets or when you don't need a persistent endpoint.\n",
    "\n",
    "To create a batch transform job, you need to set the following options:\n",
    "- **model_name**: The name of your model.\n",
    "- **instance_type**: The type of Amazon Elastic Compute Cloud (Amazon EC2) instance to use; for example, \"ml.c4.xlarge\".\n",
    "- **instance_count**: The number of EC2 instances to use.\n",
    "- **assemble_with**: The way in which the output is assembled. Valid values are \"Line\" or \"None\".\n",
    "- **strategy**: The strategy used to decide how to batch records in a single request. Valid values are \"MultiRecord\" and \"SingleRecord\".\n",
    "- **accept**: The file type to accept.\n",
    "- **output_path**: The Amazon S3 location for saving the transform result. If not specified, results are stored to a default bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create-batch-transformer\n",
    "transformer = Transformer(\n",
    "    model_name=model_name,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    instance_count=1,\n",
    "    assemble_with=\"Line\",\n",
    "    strategy=\"MultiRecord\",\n",
    "    accept=\"text/csv\",\n",
    "    output_path=\"s3://{}/{}/batch-transform/test\".format(bucket, prefix)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the test dataset as your customer records and run the batch transform job. The job can take as long as 10 minutes to run with this set of customer records.\n",
    "\n",
    "Refer to [Use Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) for more information about batch transform jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run-batch-transform-job\n",
    "transformer.transform(batch_path, content_type=\"text/csv\", split_type=\"Line\", join_source=\"Input\")\n",
    "transformer.wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.3: View the prediction data in Amazon S3\n",
    "\n",
    "The batch transform job stores the output in the bucket and folder that you specified when you set up the transformer. You can view the prediction results in Amazon S3, either in the AWS Management Console or in the notebook.\n",
    "\n",
    "If you want to download and view the output from the console, navigate to Amazon S3, open the bucket starting with **sagemaker-**, and navigate to the object located in **/sagemaker/mlasms/batch-transform/test**. Download the **batch_data.csv.out** object and open it with a notepad editor. The file contains hundreds of predicted values for the customer records that you ran through the batch transform job.\n",
    "\n",
    "A sample of the output can also be displayed in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $transformer.output_path ./\n",
    "!head batch_data.csv.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Congratulations! You have used Amazon SageMaker to successfully run a batch transform job.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with the **Conclusion**."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e45558c452cedcb26631315a9b3b77e80a9c32d662ed25df58964b99bc5b9b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
