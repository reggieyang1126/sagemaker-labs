{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务 3：探索和查询来自 PySpark 内核的数据\n",
    "\n",
    "在此笔记本中，您将执行以下操作：\n",
    "\n",
    "- 了解如何使用 Studio 笔记本以可视化方式浏览 Amazon EMR 集群、对其进行身份验证并连接到该集群。\n",
    "- 使用 Spark ML 探索和查询数据。\n",
    "- 使用 Spark UI 监控 Spark 作业。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 3.1：会话信息\n",
    "\n",
    "因为您正在使用 PySpark 内核，所以可以使用 PySpark 魔法 %%info 来显示当前会话信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 3.2：探索和查询数据\n",
    "\n",
    "使用 PySpark 内核时，连接到 EMR 集群后会自动创建 SparkContext 和 HiveContext。您可以使用 HiveContext 来查询 Hive 表中的数据，并使其在 Spark 数据帧中可用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 HiveContext 来查询 Hive 并观察数据库和表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query-hive\n",
    "\n",
    "sqlContext = HiveContext(sqlContext)\n",
    "\n",
    "dbs = sqlContext.sql(\"show databases\")\n",
    "dbs.show()\n",
    "\n",
    "tables = sqlContext.sql(\"show tables\")\n",
    "tables.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查询成人数据表并将数据传入 Spark 数据帧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load-data\n",
    "\n",
    "adult_df = sqlContext.sql(\"select * from adult_data\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用数据帧观察形状并查看数据集的前五行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-shape\n",
    "\n",
    "print((adult_df.count(), len(adult_df.columns)))\n",
    "\n",
    "#Show first 5 rows \n",
    "adult_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了获得更清晰的输出，请将 Spark 数据帧转换为 Pandas 数据帧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert-dataframe\n",
    "\n",
    "adult_df.limit(5).toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些机器学习 (ML) 算法（例如线性回归）需要数值特征。您在本实验中使用的成人数据集包括分类特征，例如 **workclass**、**education**、**occupation**、**marital status**、**relationship**、**race** 和 **sex**。\n",
    "\n",
    "下面的代码块演示了如何使用 StringIndexer 和 OneHotEncoderEstimator 将分类变量转换为一组值为 0 和 1 的数值变量。\n",
    "\n",
    "- StringIndexer 可将字符串值列转换为标签索引列。\n",
    "- OneHotEncoderEstimator 可将分类索引列映射到二进制向量列，每行最多一个“1”，表示该行的分类索引。\n",
    "\n",
    "Spark 中的独热编码是一个分为两步的流程。首先使用 **StringIndexer**，然后使用 **OneHotEncoder**。\n",
    "\n",
    "有关更多信息，请参阅 [StringIndexer](https://spark.apache.org/docs/latest/ml-features.html#stringindexer) 和 [OneHotEncoder](https://spark.apache.org/docs/latest/ml-features.html#onehotencoder)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert-variables\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n",
    "\n",
    "categorical_variables = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"-index\") for column in categorical_variables]\n",
    "\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "    outputCols=[\"{0}-encoded\".format(indexer.getOutputCol()) for indexer in indexers]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorAssembler 类将多个列作为输入。它输出一个包含值数组的列。\n",
    "\n",
    "有关此汇编器的更多信息，请参阅 [VectorAssembler](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector-assembler\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=encoder.getOutputCols(),\n",
    "    outputCol=\"categorical-features\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "管道是转换器和估算器的有序列表。您可以定义管道来实现自动化并确保应用于数据集的转换的可重复性。在此步骤中，定义管道，然后将其应用于数据集。\n",
    "\n",
    "类似于 **StringIndexer**，管道是**估算器**。pipeline.fit() 方法会返回 **PipelineModel**，这是**转换器**。\n",
    "\n",
    "有关机器学习管道的更多信息，请参阅[管道](https://spark.apache.org/docs/latest/ml-pipeline.html#pipeline)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark-pipelines\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define the pipeline based on the stages created in previous steps.\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler])\n",
    "\n",
    "# Define the pipeline model.\n",
    "pipelineModel = pipeline.fit(adult_df)\n",
    "\n",
    "# Apply the pipeline model to the dataset.\n",
    "adult_df = pipelineModel.transform(adult_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看在上一步中创建的所有不同的列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print-schema\n",
    "\n",
    "adult_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应用转换后，单个列包含一个数组，其中有每个已编码的分类变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-categorical-features\n",
    "\n",
    "adult_df.select('categorical-features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，编码目标标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode-target\n",
    "\n",
    "indexer = StringIndexer(inputCol='income', outputCol='label')\n",
    "\n",
    "adult_df = indexer.fit(adult_df).transform(adult_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 3.3：使用 Spark UI 进行监控和调试\n",
    "\n",
    "在本部分中，您将使用 Spark UI 监控和检查在前面的步骤中运行的 Spark 任务的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取当前 Spark 会话信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以看到 **Spark UI** 和**驱动程序日志**的超链接。在本实验中，**驱动程序日志**链接无效。**Spark UI** 预签名 URL 是在连接到 EMR 集群时生成的。选择此链接会将您转至 Spark UI，从而在 Web 浏览器中检查 Spark 任务运行情况。这些指标对优化性能很有帮助。\n",
    "\n",
    "下面是 Spark 服务器中的一些重要功能：\n",
    "- **Jobs**（任务）选项卡显示该 Spark 应用程序中所有 Spark 任务的状态。\n",
    "- 在 <b>Summary</b>（摘要）部分下，**Event Timeline**（事件时间线）部分显示运行的各个阶段。\n",
    "- **Completed Jobs**（已完成的任务）部分以表格形式显示。在 **Completed Jobs**（已完成的任务）部分下，您可以选择一项任务，从而查看该任务中子任务的阶段信息。\n",
    "- 使用 **DAG Visualization**（DAG 可视化），您可以探索之前运行的子任务。与 **Event Timeline**（事件时间线）视图一样，使用 **DAG Visualization**（DAG 可视化），您可以选择一个阶段并展开该阶段中的详细信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "恭喜！ 您已成功连接到 EMR 集群，并使用 PySpark 探索和查询了数据。\n",
    "\n",
    "### 清理\n",
    "\n",
    "您已完成此笔记本。要进入本实验的下一部分，请执行以下操作：\n",
    "\n",
    "- 关闭此笔记本文件。\n",
    "- 返回至实验会话并继续**总结**部分。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}