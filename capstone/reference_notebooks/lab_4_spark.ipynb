{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Perform data processing with SageMaker Processing\n",
    "\n",
    "In this notebook, you set up the environment needed to run a basic Apache Spark application using Amazon SageMaker Processing. By using Apache Spark on SageMaker Processing, you can run Spark jobs without having to provision an Amazon EMR cluster. You then define and run a Spark job using the **PySparkProcessor** class from the **SageMaker Python SDK**. Finally, you validate the data processing results saved in Amazon Simple Storage Service (Amazon S3).\n",
    "\n",
    "The processing script does some basic data processing such as string indexing, one-hot encoding, vector assembly, and an 80-20 split of the processed data to train and validation datasets.     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: Environment setup\n",
    "\n",
    "Install the latest SageMaker python SDK package and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install awscli --upgrade\n",
    "%pip install boto3 --upgrade\n",
    "%pip install -U \"sagemaker>2.0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. After upgrading the SDK, restart your notebook kernel by selecting the **Restart kernel** icon from the notebook toolbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, import the required libraries, get the execution role to run the SageMaker processing job, and set up the Amazon S3 bucket to store the Spark job outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install-dependencies\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "#Execution role to run the SageMaker Processing job\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"SageMaker Execution Role: \", role)\n",
    "\n",
    "#S3 bucket to read the Spark processing script and writing processing job outputs\n",
    "s3 = boto3.resource('s3')\n",
    "for buckets in s3.buckets.all():\n",
    "    if 'labdatabucket' in buckets.name:\n",
    "        bucket = buckets.name\n",
    "print(\"Bucket: \", bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error, make sure you restarted your notebook kernel by selecting the **Restart kernel** icon from the notebook toolbar. Then, rerun the cell."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2: Run the SageMaker processing job\n",
    "\n",
    "In this task, you import and review the preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import-data\n",
    "prefix = 'data/input'\n",
    "\n",
    "S3Downloader.download(s3_uri=f\"s3://{bucket}/{prefix}/spark_adult_data.csv\", local_path= 'data/')\n",
    "\n",
    "shape=pd.read_csv(\"data/spark_adult_data.csv\", header=None)\n",
    "shape.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the Sagemaker Spark PySparkProcessor class to define and run a spark application as a processing job. Refer to [Sagemaker Spark PySparkProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor) for more information about this class.\n",
    "\n",
    "For creating the PySparkProcessor class, you configure the following parameters:\n",
    "- **base_job_name**: Prefix for the processing job name\n",
    "- **framework_version**: SageMaker PySpark version\n",
    "- **role**: SageMaker execution role\n",
    "- **instance_count**: Number of instances to run the processing job\n",
    "- **instance_type**: Type of Amazon Elastic Compute Cloud (Amazon EC2) instance used for the processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark-processor\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# create a PySparkProcessor\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-preprocessor\",\n",
    "    framework_version=\"3.1\", # Spark version\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you use the PySparkProcessor run method to run the **pyspark_preprocessing.py** script as a processing job. Refer to [PySparkProcessor run method](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor.run) for more information about this method. For this lab, data transformations such as string indexing and one-hot encoding are performed on the categorical features.\n",
    "\n",
    "For running the processing job, you configure the following parameters:\n",
    "- **submit_app**: Path of the preprocessing script \n",
    "- **outputs**: Path of output for the preprocessing script (Amazon S3 output locations)\n",
    "- **arguments**: Command-line arguments to the preprocessing script (such as the Amazon S3 input and output locations)\n",
    "\n",
    "The processing job takes approximately 5 minutes to complete. While the job is running, you can review the source for the preprocessing script (which has been preconfigured as part of this lab) by opening the **pyspark_preprocessing.py** file from the file browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing-job\n",
    "import os\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Amazon S3 path prefix\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "input_raw_data_prefix = \"data/input\"\n",
    "output_preprocessed_data_prefix = \"data/output\"\n",
    "scripts_prefix = \"scripts/smstudiofiles\"\n",
    "logs_prefix = \"logs\"\n",
    "\n",
    "# Run the processing job\n",
    "spark_processor.run(\n",
    "    submit_app='s3://' + os.path.join(bucket, scripts_prefix, \"pyspark_preprocessing.py\"),\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", \n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"train\")),\n",
    "        ProcessingOutput(output_name=\"validation_data\", \n",
    "                         source=\"/opt/ml/processing/validation\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"validation\")),\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--s3_input_bucket\", bucket,\n",
    "        \"--s3_input_key_prefix\", input_raw_data_prefix,\n",
    "        \"--s3_output_bucket\", bucket,\n",
    "        \"--s3_output_key_prefix\", output_preprocessed_data_prefix],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, logs_prefix),\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "print(\"Spark Processing Job Completed.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3: Validate the data processing results\n",
    "\n",
    "Validate the output of the data processing job that you ran by reviewing the first five rows of the train and validation output datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-train-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/train/train_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-validation-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/validation/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/validation/validation_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Congratulations! You have used SageMaker Processing to successfully create a Spark processing job using the SageMaker Python SDK and run a processing job.\n",
    "\n",
    "The next task of the lab focuses on data processing with SageMaker Processing and the built-in scikit-learn container.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with **Task 3: Perform data processing with SageMaker Processing and the built-in scikit-learn container**."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
