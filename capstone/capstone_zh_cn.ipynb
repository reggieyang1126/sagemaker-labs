{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结业设计：使用 SageMaker Studio 和 Amazon SageMaker Python SDK 构建端到端表格式数据机器学习 (ML) 项目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境设置\n",
    "\n",
    "这里的基本设置代码可以帮助您入门。请先查看并运行这些单元格，以安装程序包并创建变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U shap\n",
    "%pip install -U smdebug\n",
    "%pip install imbalanced-learn\n",
    "%pip install pytest-cov\n",
    "%pip install pytest-filter-subpackage\n",
    "%pip install sagemaker\n",
    "%pip install -U seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required-libraries\n",
    "\n",
    "import boto3\n",
    "import datetime as datetime\n",
    "import io\n",
    "import IPython\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt  # visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import re\n",
    "import sagemaker\n",
    "import seaborn as sns  # visualization\n",
    "import statistics\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sagemaker import clarify\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n",
    "from sagemaker.debugger import CollectionConfig, DebuggerHookConfig, FrameworkProfile, ProfilerConfig, ProfilerRule, Rule, rule_configs\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.experiments.run import Run, load_run\n",
    "from sagemaker.feature_store.feature_definition import FeatureDefinition\n",
    "from sagemaker.feature_store.feature_definition import FeatureTypeEnum\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.processing import FeatureStoreOutput\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.workflow.condition_step import ConditionStep, JsonGet\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThan\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic-variable-code-and-settings\n",
    "\n",
    "%matplotlib inline\n",
    "base_job_name = \"capstone-smdebugger-job\"\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "bucket_path = \"s3://{}\".format(bucket)\n",
    "prefix = \"sagemaker/capstone\"\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sagemaker_session = sagemaker.Session()\n",
    "save_interval = 5\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集描述\n",
    "\n",
    "Amazon Simple Storage Service (Amazon S3) 存储桶中存储了五个表：\n",
    "- **claims.csv**：一个包含原始索赔数据的表。\n",
    "- **customers.csv**：一个包含原始客户数据的表。\n",
    "- **claims_preprocessed.csv**：一个包含处理过的索赔数据的表。\n",
    "- **customers_preprocessed.csv**：一个包含处理过的客户数据的表。\n",
    "- **claims_customer.csv**：一个联接了处理过的索赔和客户数据的 **policy_id** 的表。\n",
    "\n",
    "在本实验中，从 **claims.csv** 和 **customers.csv** 表开始。您将在**挑战 1** 中使用 Amazon SageMaker Data Wrangler 对其进行处理。如果您遇到问题，或者想参考处理过的数据集是什么样的，可以查看预处理过的表。\n",
    "\n",
    "对于这个数据集，目标是 claims 表中的 **fraud** 列。\n",
    "\n",
    "claims 表包含以下字段： \n",
    "\n",
    "- **policy_id**：保单的唯一 ID。\n",
    "- **driver_relationship**：关系列表（配偶、自己、孩子、其他、不祥）。\n",
    "- **incident_type**：报告的事故类型（入室抢劫、车祸、盗窃）。\n",
    "- **collision_type**：碰撞位置（前面、后面、侧面、不详）。\n",
    "- **incident_severity**：事故严重性（轻微、严重、报废）。\n",
    "- **authorities_contacted**：首先联系的机构类型（无、警察、救护、消防）。\n",
    "- **num_vehicles_involved**：事故所涉车辆数量（1 至 6 辆）。\n",
    "- **num_injuries**：事故所涉受伤人数（1 至 4 人）。\n",
    "- **num_witnesses**：事故目击者人数（1 至 5 人）。\n",
    "- **police_report_available**：是否有警察报告（是或否）。\n",
    "- **injury_claim**：以 USD 为单位的受伤索赔金额（300 至 576,300 USD）。\n",
    "- **vehicle_claim**：以 USD 为单位的车辆损坏索赔金额（1000 至 51,051 USD）。\n",
    "- **total_claim_amount**：受伤和损坏索赔总额（2100 至 588,868 USD）。\n",
    "- **incident_month**：事故发生的月份（1 至 12 月）。\n",
    "- **incident_day**：事故发生的当天（1 至 31 日）。\n",
    "- **incident_dow**：事故发生的星期几（0 至 6，表示从星期日到星期六）。\n",
    "- **incident_hour**：事故发生的具体时间（0 至 23 时）。\n",
    "- **fraud**：保单是否是欺诈性的（0 或 1）。\n",
    "\n",
    "customers 表包含以下字段：\n",
    "\n",
    "- **policy_id**：保单的唯一 ID。\n",
    "- **customer_age**：客户的年龄（18 至 70 岁）。\n",
    "- **months_as_customer**：客户已支付保险费的月数（1 至 495 个月）。\n",
    "- **num_claims_past_year**：客户在过去一年提出的索赔数量。\n",
    "- **num_insurers_past_5_years**：客户在过去 5 年投保的保险公司数量。\n",
    "- **policy_state**：客户居住的州（亚利桑那州、加利福尼亚州、爱达荷州、内华达州、俄勒冈州、华盛顿州）。\n",
    "- **policy_deductable**：以 USD 为单位的保单免赔额（750 至 1100 USD）。\n",
    "- **policy_annual_premium**：以 USD 为单位的保单年费（2200 至 3000 USD）。\n",
    "- **policy_liability**：人身伤害的最高赔偿金额，分为单项和所有人身伤害（15/30、25/50、60/90、100/200）。\n",
    "- **customer_zip**：客户的邮政编码（83201 至 99362）。\n",
    "- **customer_gender**：客户的性别（男、女、其他、未知）。\n",
    "- **customer_education**：客户的学历（高中以下、高中、大专、本科、研究生）。\n",
    "- **auto_year**：汽车制造年份（2001 至 2020 年）。\n",
    "\n",
    "您可以通过内部联接使用 **policy_id** 列来联接这两个表。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 挑战实验导航\n",
    "\n",
    "本实验设有链接，可供在挑战任务和笔记本末尾的附录之间进行导航。如果您想查看附录中的项目，请选择相关的超链接。如果您想返回至当前正在进行的挑战，请选择附录中相应的任务超链接。\n",
    "\n",
    "实验结构如下所示：\n",
    "\n",
    "- 挑战 1：使用 SageMaker Data Wrangler 分析和准备数据集\n",
    "- 挑战 2：在 SageMaker 特征存储中创建特征组\n",
    "- 挑战 3：训练模型\n",
    "- 挑战 4：评估模型是否有偏差\n",
    "- 挑战 5：批量转换\n",
    "- 挑战 6：构建自动化管道\n",
    "- 附录"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 挑战 1：使用 SageMaker Data Wrangler 分析和准备数据集\n",
    "\n",
    "AnyCompany Consulting 收到了一项请求，要求分析汽车保险欺诈数据集并构建模型来帮助预测新的索赔是否有可能是欺诈性的。该模型有 5,000 条客户记录，其中每项索赔都被标记为欺诈性或非欺诈性。您可以使用这些数据来训练、测试和验证模型，然后再对一批新的记录集合运行推理。\n",
    "\n",
    "使用 Amazon SageMaker Data Wrangler 分析功能来直观了解重要列中的数据分布情况，并检查各列之间的关联以及目标泄漏情况。接着，构建快速基准模型。然后，使用 SageMaker Data Wrangler 数据处理功能来转换各列，使其更适合训练性能更高的模型。\n",
    "\n",
    "要完成此任务，您需要完成以下子任务：\n",
    "\n",
    "- 查看数据。\n",
    "- 在 Amazon SageMaker Studio 中完成探索性数据分析。\n",
    "- 使用 Amazon SageMaker Clarify 处理器任务来运行偏差报告。\n",
    "- 准备数据。\n",
    "\n",
    "完成本挑战大约需要 *100* 分钟。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 1.1：查看数据\n",
    "\n",
    "<a id=\"task1-1-continue\"></a>\n",
    "\n",
    "访问存储库中存储的表格式汽车保险数据集并查看该数据集的样本。存储库中包含两个未经处理的表。一个关于客户数据，名为 **customers.csv**，另一个关于索赔数据，名为 **claims.csv**。\n",
    "\n",
    "**提示 1**：未经处理的表位于 **./data/** 文件夹中。\n",
    "\n",
    "**提示 2**：**claims.csv** 和 **customers.csv** 表是未经处理的表。\n",
    "\n",
    "花点时间探索这两个表。是否有任何突出显示的字段？ 是否有任何需要仔细预处理的字段？\n",
    "\n",
    "有关如何查看数据的详细步骤，请参阅*附录*部分中的<a href=\"#task1-1\" target=\"_self\">**查看数据（任务 1.1）**</a>。\n",
    "\n",
    "在访问了汽车保险欺诈表并查看了数据集的样本之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_1_1_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 1.2：在 SageMaker Studio 中完成探索性数据分析 \n",
    "\n",
    "<a id=\"task1-2-continue\"></a>\n",
    "\n",
    "通过查看数据、识别数据集中的潜在问题以及检查是否有任何列与目标之间有很大关联来完成探索性数据分析。您可以在 SageMaker Data Wrangler 和笔记本中探索数据。\n",
    "\n",
    "具体来说就是花点时间查看以下项目：\n",
    "- **列直方图**：以可视化格式查看各列，并检查数据集中是哪些类型的值。\n",
    "- **快速模型**：查看数据集并思考预期的模型结果。\n",
    "- **特征关联**：查看是否有任何列与目标之间有很大关联。\n",
    "- **目标泄露**：查看是否有任何数据依赖于目标值。\n",
    "\n",
    "打开 SageMaker Data Wrangler 将在 SageMaker Studio 中打开一个新选项卡。要遵循这些指示，请使用以下选项之一：\n",
    "- **选项 1**：并排查看选项卡。要从 SageMaker Studio 主窗口创建分屏视图，请将 **capstone.ipynb** 选项卡拖到一边，或者选择（右键单击） **capstone.ipynb** 选项卡并选择 **New View for Notebook**（为笔记本新建视图）。现在，您可以在使用 SageMaker Data Wrangler 流时看到相应指示。\n",
    "- **选项 2**：在 SageMaker Studio 选项卡之间切换，以遵循这些指示。探索完 SageMaker Data Wrangler 步骤后，通过选择 **capstone.ipynb** 选项卡返回至笔记本。\n",
    "\n",
    "如果您在创建新的流文件时收到 **The following instance type is not available: ml.m5.4xlarge.Try selecting a different instance below.**（以下实例类型不可用：ml.m5.4xlarge。请尝试选择下面的其他实例。）错误消息，可以选择其他实例类型。接着，尝试 **ml.m5.8xlarge**。\n",
    "\n",
    "如果显示 **An error occurred loading this view**（加载此视图时出现错误），请关闭 **untitled.flow** 选项卡，然后从文件浏览器中重新打开流文件。\n",
    "\n",
    "**提示 1**：探索数据集的方法有很多。首先，打开 SageMaker Data Wrangler 流。您需要将 **claims.csv** 和 **customers.csv** 从名称中包含 **databucket-** 的 S3 存储桶导入到 SageMaker Data Wrangler 中。\n",
    "\n",
    "**提示 2**：要导入第二个表，请返回至 **Data flow**（数据流），然后选择 **Import**（导入）选项卡，以导入其他数据集。\n",
    "\n",
    "**提示 3**：**Get data insights**（获取数据见解）和 **Add analysis**（添加分析）是在 SageMaker Data Wrangler 中探索数据的两种方法。在查看了一些数据样本图表后，如果您愿意，可以使用笔记本中的其他绘图工具来分析数据。已安装 **plt** 和 **sns** 库。请随意使用您熟悉的任何分析工具来探索数据集。 \n",
    "\n",
    "**提示 4**：尝试通过**联接**使用 **policy_id** 来联接这两个表。然后，运行其他见解报告。您可以对这些表使用**内部**联接。\n",
    "\n",
    "联接后的数据集是否让您获得了更有意义的结果？\n",
    "\n",
    "有关如何在 SageMaker Studio 中探索数据集的详细步骤，请参阅*附录*部分中的<a href=\"#task1-2-1\" target=\"_self\">**在 SageMaker Studio 中探索数据集（任务 1.2）**</a>。\n",
    "\n",
    "有关如何在笔记本中探索数据集的详细步骤，请参阅*附录*部分中的<a href=\"#task1-2-2\" target=\"_self\">**在笔记本中探索数据集（任务 1.2）**</a>。\n",
    "\n",
    "在使用 SageMaker Data Wrangler 处理了数据，并探索了数据集，确定了要执行的处理步骤之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_1_2_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 1.3：使用 SageMaker Clarify 处理器任务来运行偏差报告\n",
    "\n",
    "<a id=\"task1-3-continue\"></a>\n",
    "\n",
    "使用 SageMaker Clarify 来运行训练前偏差报告，以捕获数据中的类不平衡。使用 SageMaker Data Wrangler 流在 SageMaker Studio 中运行偏差报告。\n",
    "1.首先，联接两个表。\n",
    "\n",
    "- 联接：通过**内部**联接使用 **policy_id** 将 **claims.csv** 联接到 **customers.csv**。\n",
    "\n",
    "**提示 1**：要创建训练前偏差报告，请向 SageMaker Data Wrangler 流添加一项新的分析，然后对于 **Analysis type**（分析类型），选择 **Bias Report**（偏差报告）。\n",
    "\n",
    "**提示 2**：您可以多次运行偏差报告，每次选择不同的特征进行分析。\n",
    "\n",
    "有关如何通过 SageMaker Data Wrangler 联接表的详细步骤，请参阅*附录*部分中的<a href=\"#task1-3-1\" target=\"_self\">**在 SageMaker Studio 中联接表（任务 1.3）**</a>。\n",
    "\n",
    "有关如何运行训练前偏差报告的详细步骤，请参阅*附录*部分中的<a href=\"#task1-3-2\" target=\"_self\">**运行训练前偏差报告（任务1.3）**</a>。\n",
    "\n",
    "在运行并查看了训练前偏差报告之后，即表示您完成了此任务。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 1.4：准备数据\n",
    "\n",
    "<a id=\"task1-4-continue\"></a>\n",
    "\n",
    "使用 SageMaker Data Wrangler 准备数据集。重点关注以下转换，但可以随意包括其他转换：\n",
    "\n",
    "- 编码分类（独热编码）：**authorities_contacted**、**collision_type**、**customer_gender**、**driver_relationship**、**incident_type** 和 **policy_state**。\n",
    "- 有序编码：**customer_education**、**policy_liability**、**incident_severity** 和 **police_report_available**。\n",
    "- 将列解析为类型：将 **vehicle_claim** 和 **total_claim_amount** 从 **Float** 解析为 **Long**。\n",
    "- 管理列（删除列）：**customer_zip**。\n",
    "- 管理列（移动列）：**fraud**（使用 **Move to start**（移至开头））。\n",
    "- 管理列（重命名列）：删除 **collision_type_N/A** 和 **driver_relationship_N/A** 中的 **/** 符号，将其改为 **_**。\n",
    "- 管理列（重命名列）：将 **policy_id_0** 重命名为 **policy_id**。\n",
    "\n",
    "**提示 1**：使用 SageMaker Data Wrangler 联接功能将 **claims** 表联接到 **customers** 表。\n",
    "\n",
    "**提示 2**：使用 **policy_id** 列联接两个表。\n",
    "\n",
    "**提示 3**：使用 **Add transform**（添加转换）选项添加转换。\n",
    "\n",
    "您认为哪些转换对模型训练的影响最大？\n",
    "\n",
    "有关如何使用 SageMaker Data Wrangler 准备数据的详细步骤，请参阅*附录*部分中的<a href=\"#task1-4-1\" target=\"_self\">**使用 SageMaker Data Wrangler 准备数据（任务 1.4）**</a>。\n",
    "\n",
    "如果您想导入一部分处理过的数据，请参阅*附录*部分中的<a href=\"#task1-4-2\" target=\"_self\">**导入一部分处理过的数据（任务 1.4）**</a>。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 挑战 2：在 SageMaker 特征存储中创建特征组\n",
    "\n",
    "现在您已经处理了数据集，接下来可以创建特征和特征组，以便在将来的分析中使用。使用 SageMaker 特征存储将这些特征存储在特征组中，并在训练模型时查询这些特征。\n",
    "\n",
    "要完成此任务，请完成以下所有子任务：\n",
    "\n",
    "1.将特征导出至 SageMaker 特征存储中。\n",
    "2.使用 Amazon Athena 查询离线存储中的特征组。\n",
    "\n",
    "完成本挑战大约需要 *30* 分钟。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 2.1：将特征导出至 SageMaker 特征存储中\n",
    "\n",
    "<a id=\"task2-1-continue\"></a>\n",
    "\n",
    "使用 SageMaker Data Wrangler **导出至**功能创建自定义 Jupyter 笔记本。笔记本将创建特征定义和特征组。笔记本将把记录摄取到特征组中。在笔记本中，完成以下步骤：\n",
    "\n",
    "- 设置 record 和 event_time 的值。\n",
    "- 运行笔记本单元格以创建特征组。\n",
    "- 运行笔记本单元格以确认创建的特征组。\n",
    "- 运行笔记本单元格以将记录摄取到特征组中。\n",
    "\n",
    "**提示 1**：所有这些步骤都可以在 SageMaker Studio 中完成。创建完特征组后，您可以返回至此笔记本继续执行任务 2.2。\n",
    "\n",
    "**提示 2**：添加自定义转换来创建 **event_time** 列。\n",
    "\n",
    "**提示 3**：添加自定义转换的代码如下所示：\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import lit\n",
    "date_time = datetime.date.today()\n",
    "\n",
    "df = df.withColumn(\"event_time\", lit(time.mktime(date_time.timetuple())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**提示 4**：在 SageMaker Data Wrangler 流的最后，选择 **+** 图标，选择 **Export to**（导出至）选项，然后选择 **SageMaker Feature Store (via JupyterNotebook)**（SageMaker 特征存储 (通过 Jupyter 笔记本)）。\n",
    "\n",
    "**提示 5**：您可以通过将 **enable_online_store** 值从 **True** 更改为 **False** 来关闭在线存储。\n",
    "\n",
    "如何使用 SageMaker 特征存储来存储和查询用于训练而非推理的记录？\n",
    "\n",
    "有关如何使用 **Export to**（导出至）选项创建特征组的详细步骤，请参阅*附录*部分中的<a href=\"#task2-1\" target=\"_self\">**使用 Export to（导出至）选项创建特征组（任务 2.1）**</a>。\n",
    "\n",
    "在创建了特征组并将数据摄取到了特征组之后，即表示您完成了此任务。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 2.2：使用 Athena 查询离线存储中的特征组\n",
    "\n",
    "<a id=\"task2-2-continue\"></a>\n",
    "\n",
    "使用 Athena 从离线数据存储中提取记录。在下一项挑战中，您需要把这些记录分割成训练、测试和验证数据集。\n",
    "\n",
    "使用下面提供的代码单元格进行 Amazon Athena API 调用。您可以使用 Amazon Athena 控制台进行查询，但此操作超出了本实验的范围。\n",
    "\n",
    "**提示 1**：您可以使用 **feature_group.athena_query()** 创建 Athena 查询，并使用 **query.table_name** 获取表名称。\n",
    "\n",
    "**提示 2**：您可以使用 **query.run(query_string=query_string, output_location=output_location)** 运行查询，并使用 **query.as_dataframe()** 将返回的值读取为数据帧。\n",
    "\n",
    "如何使用 **event_time** 来跟踪发生在数据集时间线上不同点的特征？\n",
    "\n",
    "有关如何使用 Athena 从离线数据存储中提取记录的详细步骤，请参阅*附录*部分中的<a href=\"#task2-2\" target=\"_self\">**使用 Athena 从离线存储中提取记录（任务 2.2）**</a>。\n",
    "\n",
    "在将返回的 Athena 查询保存为了数据帧变量之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_2_2_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 挑战 3：训练模型\n",
    "\n",
    "您的模型已做好训练准备。将数据分割成训练、测试和验证数据集，并训练模型。\n",
    "\n",
    "之前针对这些数据运行了 SageMaker Autopilot，得到 **F1** 为 **0.616**，**accuracy**（准确率）为 **0.978**，**AUC** 为 **0.918** 以及 **Recall**（查全率）为 **0.539**。要了解有关 SageMaker Autopilot 生成的指标的更多信息，请参阅“其他资源”部分中的“autopilot-metrics-validation”文档，获取更多信息。\n",
    "\n",
    "在进行训练和优化时，请努力达到或超过 SageMaker Autopilot 分数，并确认 Amazon SageMaker Debugger 没有报错。\n",
    "\n",
    "要完成此任务，您需要完成以下子任务：\n",
    "\n",
    "- 创建实验和运行。\n",
    "- 将数据分割成训练、测试和验证数据集。\n",
    "- 配置并运行训练任务。\n",
    "    - 运行基本训练任务。\n",
    "    - 在启用了 SageMaker Debugger 的情况下运行训练任务并分析报告（可选）。\n",
    "- 执行超参数优化。\n",
    "\n",
    "完成本挑战大约需要 *110* 分钟。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 3.1：为实验和运行命名\n",
    "\n",
    "<a id=\"task3-1-continue\"></a>\n",
    "\n",
    "设置变量来为实验和运行命名。\n",
    "实验需要 **experiment_name**、**run_name** 和 **description**。\n",
    "\n",
    "有关如何创建变量的详细步骤，请参阅*附录*部分中的<a href=\"#task3-1\" target=\"_self\">**为实验和运行命名（任务 3.1）**</a>。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_3_1_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 3.2：将数据分割成训练、测试和验证数据集\n",
    "\n",
    "<a id=\"task3-2-continue\"></a>\n",
    "\n",
    "使用您从 SageMaker 特征存储中查询的特征并将数据分割成训练、测试和验证数据集。\n",
    "\n",
    "**提示 1**：使用 **np.split** 将数据集分割到三个分区中。\n",
    "\n",
    "**提示 2**：使用 **to_csv** 创建 CSV 文件并使用 **S3Uploader.upload** 将文件添加到 Amazon S3。\n",
    "\n",
    "**提示 3**：分割的最终结果应该是一个 **data_inputs** 变量，其值为 **train** 和 **validation**。\n",
    "\n",
    "有关如何将数据分割成训练、测试和验证数据集的详细步骤，请参阅*附录*部分中的<a href=\"#task3-2\" target=\"_self\">**将数据分割成训练、测试和验证数据集（任务 3.2）**</a>。\n",
    "\n",
    "在将数据分割成了训练、测试和验证数据集之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_3_2_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 3.3：配置并运行训练任务\n",
    "\n",
    "<a id=\"task3-3-continue\"></a>\n",
    "\n",
    "开始您的首项训练任务，设置容器、估算器和超参数。然后，使用 **fit()** 训练模型。如果您想查看更详细的报告，请使用 **DebuggerHookConfig** 启用 SageMaker Debugger。\n",
    "\n",
    "**提示 1**：使用版本为 **1.5-1** 的 **XGBoost** 容器。\n",
    "\n",
    "**提示 2**：首先为 **eta**、**gamma**、**max_depth**、**min_child_weight**、**num_round**、**objective** 和 **subsample** 设置超参数。\n",
    "\n",
    "**提示 3**：使用您之前在挑战 1 中创建的 **data_inputs** 作为训练任务的 **inputs** 值。\n",
    "\n",
    "**提示 4**：您可以通过设置 inputs 和 experiment_config 来配置训练任务。experiment_config 应该包含 **sagemaker_session**、**run_name** 和 **experiment_name**。\n",
    "\n",
    "**提示 5**：如果您想使用 SageMaker Debugger，请配置 **DebuggerHookConfig**、**ProfilerConfig** 和 Debugger **rule** 对象。\n",
    "\n",
    "哪些超参数最有可能对模型的性能和准确率产生很大影响？ 您计划首先优化哪些超参数？\n",
    "\n",
    "有关如何配置并运行基本训练任务的详细步骤，请参阅*附录*部分中的<a href=\"#task3-3-1\" target=\"_self\">**配置并运行基本训练任务（任务 3.3）**</a>。\n",
    "\n",
    "有关如何在启用了 Debugger 的情况下配置并运行训练任务以及分析报告的详细步骤，请参阅*附录*部分中的<a href=\"#task3-3-2\" target=\"_self\">**在启用了 SageMaker Debugger 的情况下配置并运行训练任务以及分析报告（任务 3.3）**</a>。\n",
    "\n",
    "在完成了一项或多项训练任务之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_3_3_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 3.4：超参数优化\n",
    "\n",
    "<a id=\"task3-4-continue\"></a>\n",
    "\n",
    "现在您已经完成了一项训练任务并对其进行了分析，接下来可以根据您的发现优化超参数范围，并运行更多训练任务来改进模型。\n",
    "\n",
    "**提示 1**：首先为 **alpha**、**eta**、**max_depth**、**min_child_weight** 和 **num_round** 设置超参数范围。\n",
    "\n",
    "**提示 2**：在运行 **HyperparameterTuner** 时，请务必根据您的发现设置 **objective_metric_name** 和 **objective_type**。\n",
    "\n",
    "**提示 3**：要查看您是否改善了 SageMaker Autopilot 结果，请打开 **SageMaker resources**（SageMaker 资源）中的 **Experiments and runs**（实验和运行）菜单。在运行中，查看 **Metrics**（指标）。**ObjectiveMetric** 应高于 **F1** 分数 (**0.616**)，**validation:auc** 的 **Final value**（最终值）应高于 SageMaker Autopilot 分数 (**0.918**)。\n",
    "\n",
    "在您优化超参数时，哪个对模型性能的提高最大？\n",
    "\n",
    "有关如何配置训练超参数范围的详细步骤，请参阅*附录*部分中的<a href=\"#task3-4\" target=\"_self\">**配置训练超参数范围（任务 3.4）**</a>。\n",
    "\n",
    "在配置了训练超参数范围并开始了更多训练任务之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_3_4_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 挑战 4：评估模型是否有偏差\n",
    "\n",
    "现在您的模型已经训练好了，接下来可以使用 Amazon SageMaker Clarify 评估您的模型。如果您发现任何问题，可以消除检测到的不平衡并重新训练模型。\n",
    "\n",
    "要完成此任务，您需要完成以下子任务：\n",
    "\n",
    "- 从训练任务创建模型。\n",
    "- 创建 SageMaker Clarify 模型配置。\n",
    "- 创建 SageMaker Clarify 偏差配置。\n",
    "- 使用 SageMaker Clarify 处理器任务运行偏差、数据和模型报告。\n",
    "- 消除使用 SageMaker Clarify 检测到的不平衡（可选）。\n",
    "- 重新训练模型（可选）。\n",
    "\n",
    "完成本挑战大约需要 *80* 分钟。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 4.1：从训练任务创建模型\n",
    "\n",
    "<a id=\"task4-1-continue\"></a>\n",
    "\n",
    "创建 XGBoost 模型，使用您定义的 **model_name**、**role** 和 **container_def** 调用 **create_model**。\n",
    "\n",
    "**提示 1**：调用 **xgb.create_model()** 并为模型选择名称。\n",
    "\n",
    "**提示 2**：使用会话并调用 **create_model**，传入 **model_name**、**role** 和 **container_def**。\n",
    "\n",
    "有关如何创建模型的详细步骤，请参阅*附录*部分中的<a href=\"#task4-1\" target=\"_self\">**创建模型（任务 4.1）**</a>。\n",
    "\n",
    "在创建了模型之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_4_1_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 4.2：创建 SageMaker Clarify 模型配置\n",
    "\n",
    "<a id=\"task4-2-continue\"></a>\n",
    "\n",
    "使用 **SageMakerClarifyProcessor** 创建 SageMaker Clarify 模型配置。\n",
    "\n",
    "**提示 1**：设置 **instance_count** 和 **instance_type**。\n",
    "\n",
    "**提示 2**：使用在结业设计实验开始时创建的 **role** 和 **session**。\n",
    "\n",
    "有关如何创建 SageMaker Clarity 模型配置的详细步骤，请参阅*附录*部分中的<a href=\"#task4-2\" target=\"_self\">**创建 SageMaker Clarity 模型配置（任务 4.2）**</a>。\n",
    "\n",
    "在创建了 SageMaker Clarify 模型配置之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_4_2_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 4.3：创建 SageMaker Clarify 偏差配置\n",
    "\n",
    "<a id=\"task4-3-continue\"></a>\n",
    "\n",
    "创建数据配置、模型配置、标签配置和偏差配置。\n",
    "\n",
    "**提示 1**：首先，创建 **DataConfig**，设置输入路径、输出路径、标题和数据集类型。\n",
    "\n",
    "**提示 2**：然后，创建 **ModelConfig**，选择内容和接受类型、模型名称、实例类型以及实例数目。\n",
    "\n",
    "**提示 3**：接着，创建 **ModelPredictedLabelConfig**，设置概率阈值。\n",
    "\n",
    "**提示 4**：最后，创建 **BiasConfig**，设置标签值或阈值、分面名称以及分面值或阈值。\n",
    "\n",
    "在偏差报告中，您想首先探索哪些分面？ 是否有任何特征特别容易受到偏差的影响？\n",
    "\n",
    "有关如何创建 SageMaker Clarity 偏差配置的详细步骤，请参阅*附录*部分中的<a href=\"#task4-3\" target=\"_self\">**创建 SageMaker Clarity 偏差配置（任务 4.3）**</a>。\n",
    "\n",
    "在创建了 SageMaker Clarify 偏差配置之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_4_3_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 4.4：使用 SageMaker Clarify 处理器任务运行偏差、数据和模型报告\n",
    "\n",
    "<a id=\"task4-4-continue\"></a>\n",
    "\n",
    "您已为偏差、数据和模型报告选择了所有配置。现在，运行报告。\n",
    "\n",
    "**提示 1**：将 **data_config**、**bias_config**、**model_predicted_label_config** 和 **model_config** 值传递给 **run_bias**。\n",
    "\n",
    "**提示 2**：您需要设置 **pre_training_methods** 和 **post_training_methods**。\n",
    "\n",
    "有关如何使用 SageMaker Clarify 运行偏差、数据和模型报告的详细步骤，请参阅*附录*部分中的<a href=\"#task4-4\" target=\"_self\">**使用 SageMaker Clarify 运行偏差、数据和模型报告（任务 4.4）**</a>。\n",
    "\n",
    "在使用了 SageMaker Clarify 处理器任务运行偏差、数据和模型报告之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_4_4_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 4.5：消除使用 SageMaker Clarify 检测到的不平衡（可选）\n",
    "\n",
    "<a id=\"task4-5-continue\"></a>\n",
    "\n",
    "消除使用 SageMaker Clarify 检测到的不平衡有很多方法。请使用您熟悉的任何方法。本实验中提供了一个合成少数类过采样技术 (SMOTE) 示例，用于消除某列的偏差。\n",
    "\n",
    "**提示 1**：消除完不平衡后，如果您希望重新测试，请创建一个重新采样的新数据帧。在下一个任务中，您将创建并上传一个新的 CSV 文件。\n",
    "\n",
    "有关如何消除不平衡的详细步骤，请参阅*附录*部分中的<a href=\"#task4-5\" target=\"_self\">**消除不平衡（任务 4.5）**</a>。\n",
    "\n",
    "在消除了使用 SageMaker Clarify 检测到的所有不平衡之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_4_5_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 4.6：重新训练模型（可选）\n",
    "\n",
    "<a id=\"task4-6-continue\"></a>\n",
    "\n",
    "将新文件上传到 Amazon S3。然后，创建一个新的估算器并使用新数据重新训练。\n",
    "\n",
    "**提示 1**：使用 **s3_client.upload_file** 将新文件上传到您的存储桶。\n",
    "\n",
    "**提示 2**：使用 **xgboost_starter_script.py** 并调用 **XGBoost**。然后，使用新数据重新训练。\n",
    "\n",
    "重新训练后的模型是否获得了更高的 F1 分数？ 使用 SageMaker Debugger 能否解决发现的所有问题？\n",
    "\n",
    "有关如何重新训练模型的详细步骤，请参阅*附录*部分中的<a href=\"#task4-6\" target=\"_self\">**重新训练模型（任务 4.6）**</a>。\n",
    "\n",
    "在重新训练了模型之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_4_6_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 挑战 5：批量转换\n",
    "\n",
    "您的模型已经做好了部署准备。使用具有批量记录的批量转换任务，并在 Amazon S3 中查看预测和准确率数据。然后，清理一些 SageMaker 实例。\n",
    "\n",
    "要完成此任务，您需要完成以下子任务：\n",
    "\n",
    "- 为模型创建批量转换任务。\n",
    "- 在 Amazon S3 中查看预测数据。\n",
    "- 清理 SageMaker 实例（可选）。\n",
    "\n",
    "完成本挑战大约需要 *40* 分钟。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 5.1：为模型创建批量转换任务\n",
    "\n",
    "<a id=\"task5-1-continue\"></a>\n",
    "\n",
    "在模型估算器上使用**转换器**创建批量转换任务。然后，运行该批量任务。\n",
    "\n",
    "**提示 1**：使用转换器并配置 **instance_count**、**instance_type**、**strategy**、**assemble_with** 和 **output_path**。\n",
    "\n",
    "**提示 2**：将 **test_path** 中列出的测试数据发送到终端节点并等待结果。\n",
    "\n",
    "有关如何创建批量转换任务的详细步骤，请参阅*附录*部分中的<a href=\"#task5-1\" target=\"_self\">**创建批量转换任务（任务 5.1）**</a>。\n",
    "\n",
    "在创建了批量转换任务并使用一组记录运行了该任务之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_5_1_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 5.2：在 Amazon S3 中查看预测数据\n",
    "\n",
    "<a id=\"task5-2-continue\"></a>\n",
    "\n",
    "完成批量转换任务后，请从 Amazon S3 读取数据。\n",
    "\n",
    "**提示 1**：您可以使用 **%aws s3 cp --recursive $transformer.output_path ./** 从转换器输出中复制数据\n",
    "\n",
    "**提示 2**：获得数据后，您可以使用 **%head test_data_batch.csv.out** 进行查看\n",
    "\n",
    "查看预测。是否有任何出乎意料的预测？\n",
    "\n",
    "有关如何查看来自批量转换任务的预测数据的详细步骤，请参阅*附录*部分中的<a href=\"#task5-2\" target=\"_self\">**查看来自批量转换任务的预测数据（任务 5.2）**</a>。\n",
    "\n",
    "在查看了来自批量转换任务的预测数据之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_5_2_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 5.3：清理 SageMaker 实例（可选）\n",
    "\n",
    "为了降低成本，最佳实践是删除不再使用的实例。您可以使用 SageMaker Studio 快速删除实例。现在花点时间在 SageMaker Studio 中打开当前资源列表，并关闭所有剩余实例。\n",
    "\n",
    "如果您计划完成下一个管道任务，**请让笔记本实例保持运行**。\n",
    "\n",
    "**提示 1**：您可以通过在 SageMaker Studio 中选择**正在运行的终端和内核**图标来查看正在运行的实例的列表。\n",
    "\n",
    "**提示 2**：您可以使用**关闭**图标停止实例。\n",
    "\n",
    "<a id=\"task5-3-continue\"></a>\n",
    "\n",
    "有关如何在 SageMaker Studio 中清理 SageMaker 实例的详细步骤，请参阅*附录*部分中的<a href=\"#task5-3\" target=\"_self\">**在 SageMaker Studio 中清理 SageMaker 实例（任务 5.3）**</a>。\n",
    "\n",
    "当在 SageMaker Studio 中停止了所有 SageMaker 实例之后，即表示您完成了此任务。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 挑战 6：构建自动化管道（可选）\n",
    "\n",
    "现在您已经将 Amazon SageMaker Python SDK 和 Amazon SageMaker Studio 用于机器学习 (ML) 工作流，接下来可以使用 SageMaker Pipelines 来扩展您的工作流。在实验环境中演练提供的管道脚本即可完成本挑战。\n",
    "\n",
    "- 创建管道步骤。\n",
    "    - 查询 SageMaker 特征存储中处理过的数据。\n",
    "    - 训练和优化模型。\n",
    "    - 评估训练后的模型。\n",
    "    - 执行批量转换任务。\n",
    "    - 注册模型。\n",
    "    - 使用 SageMaker Clarify 评估模型训练。\n",
    "- 定义并启动管道。\n",
    "- 查看 ML 沿袭跟踪。\n",
    "\n",
    "完成本挑战大约需要 *120* 分钟。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 6.1：配置管道\n",
    "\n",
    "<a id=\"task6-1-continue\"></a>\n",
    "\n",
    "使用管道模板并配置输入和输出。配置就绪后，运行管道。您的管道可以包括广泛的步骤。下面是建议配置的步骤列表：\n",
    "- **AutoModelProcess**：在 .csv 文件中拉取数据并将其分割成训练、测试和验证数据集的**处理**步骤。\n",
    "- **AutoHyperParameterTuning**：使用一系列超参数并优化模型的**优化**步骤。\n",
    "- **AutoEvalBestModel**：创建评估报告来描述最佳模型的**处理**步骤。\n",
    "- **CheckAUCScoreAutoEvaluation**：根据评估指标评估模型的**条件**步骤。\n",
    "- **AutoCreateModel**：创建模型的**模型**步骤。\n",
    "- **RegisterAutoModel-RegisterModel**：注册模型的**注册模型**步骤。\n",
    "- **AutoModelConfigFile**：创建偏差报告的**处理**步骤。\n",
    "- **AutoTransform**：运行批量转换任务的**转换**步骤。\n",
    "- **ClarifyProcessingStep**：运行 SageMaker Clarify 任务的**处理**步骤。\n",
    "\n",
    "**提示 1**：有许多管道步骤可供选择。要了解有关管道步骤的更多信息并查看每个步骤的示例代码，请参阅“其他资源”部分中的“构建和管理管道步骤”文档，获取更多信息。\n",
    "\n",
    "**提示 2**：首先，创建**处理**步骤以拉取数据。然后，创建**优化**步骤以优化模型。接着，创建**模型**步骤以创建模型。\n",
    "\n",
    "**提示 3**：详细步骤中包含一种示例解决方案，其中包括创建评估报告和偏差报告、运行批量转换任务和 SageMaker Clarify 任务的步骤。\n",
    "\n",
    "有关如何配置管道的详细步骤，请参阅*附录*部分中的<a href=\"#task6-1\" target=\"_self\">**配置管道（任务 6.1）**</a>。\n",
    "\n",
    "在设置了管道并开始了管道任务之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_6_1_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务 6.2：监控管道\n",
    "\n",
    "<a id=\"task6-2-continue\"></a>\n",
    "\n",
    "监控正在运行的管道，查看输入和输出。\n",
    "\n",
    "**提示 1**：使用 `RunPipeline.describe()` 描述您刚刚创建的管道。\n",
    "\n",
    "**提示 2**：您可以查看在 SageMaker Studio UI 中运行的管道步骤。打开 **SageMaker resources**（SageMaker 资源）菜单，选择 **Pipelines**（管道），然后选择您创建的管道。\n",
    "\n",
    "有关如何监控管道的详细步骤，请参阅*附录*部分中的<a href=\"#task6-2\" target=\"_self\">**监控管道**</a>。\n",
    "\n",
    "在监控完管道之后，即表示您完成了此任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_6_2_code_here\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜！ 您使用了一个汽车保险数据集来检测可能存在欺诈的索赔。您探索了一种技术解决方案，使用 SageMaker Studio 和 Amazon SageMaker Python SDK 预测给定汽车保险索赔存在欺诈的可能性。\n",
    "\n",
    "### 清理\n",
    "\n",
    "您已完成此笔记本。要进入本实验的下一部分，请执行以下操作：\n",
    "\n",
    "- 关闭此笔记本文件。\n",
    "- 返回至实验会话并继续**总结**部分。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他资源\n",
    "\n",
    "- [Autopilot 指标](https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html)\n",
    "- [处理步骤](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附录"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1-1\" id=\"task1-1\"></a>\n",
    "\n",
    "### 附录：查看数据（任务 1.1）\n",
    "\n",
    "要查看数据，请使用 Pandas 指定路径并加载数据。花点时间查看这两个表的样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read-csv-files\n",
    "claims_data = pd.read_csv(\"./data/claims_preprocessed.csv\", index_col=0)\n",
    "customers_data = pd.read_csv(\"./data/customers_preprocessed.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#claims-data-sample\n",
    "claims_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers-data-sample\n",
    "customers_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task1-1-continue\" target=\"_self\">任务 1.1</a>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1-2-1\" id=\"task1-2-1\"></a>\n",
    "\n",
    "### 附录：在 SageMaker Studio 中探索数据集（任务 1.2）\n",
    "\n",
    "在 SageMaker Data Wrangler 中开始进行数据探索。从 S3 存储桶导入文件并分析数据。\n",
    "\n",
    "下一个步骤将在 SageMaker Studio 中打开一个新选项卡。要遵循这些指示，请使用以下选项之一：\n",
    "- **选项 1**：并排查看选项卡。要从 SageMaker Studio 主窗口创建分屏视图，请将 **capstone.ipynb** 选项卡拖到一边，或者选择 **capstone.ipynb** 选项卡，然后从工具栏中选择 **File**（文件）和 **New View for Notebook**（为笔记本新建视图）。现在，您可以在浏览特征组时看到相应指示。\n",
    "- **选项 2**：在 SageMaker Studio 选项卡之间切换，以遵循这些指示。\n",
    "\n",
    "1.在 SageMaker Studio 的左侧，选择**主页**图标。\n",
    "1.展开 **Data**（数据）部分，然后选择 **Data Wrangler**。\n",
    "\n",
    "此时 SageMaker Studio 将打开 **Data Wrangler** 选项卡。\n",
    "\n",
    "1.选择 **+** **Create Data Wrangler flow**（+ 创建 Data Wrangler 流）。\n",
    "\n",
    "此时 SageMaker Studio 将打开 **untitled.flow** 选项卡。\n",
    "\n",
    "1.等待 **untitled.flow** 选项卡完成加载，您可以通过进度条查看加载情况。这可能需要 2–3 分钟。\n",
    "\n",
    "加载完成后，SageMaker Studio 将在 *Data Wrangler* 选项卡中打开 **Create connection**（创建连接）页面。\n",
    "\n",
    "1.打开 **untitled.flow** 文件选项卡上的上下文（右键单击）菜单，然后选择 **Rename Data Wrangler Flow...**（重命名 Data Wrangler 流...）以更改文件名。\n",
    "\n",
    "此时 SageMaker Studio 将打开 **Rename File**（重命名文件）消息窗口。\n",
    "\n",
    "1.对于 **New Name**（新名称），输入 `CapstoneDataWrangler.flow`。\n",
    "1.选择 <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">Rename</span>（重命名）。\n",
    "\n",
    "此时 **Rename File**（重命名文件）消息窗口将关闭。\n",
    "\n",
    "1.在 *CapstoneDataWrangler.flow* 选项卡中的 **Data sources**（数据源）部分，选择 **Amazon S3**。\n",
    "\n",
    "此时 SageMaker Studio 将在 *DataWrangler.flow* 选项卡中打开 **Import a dataset from S3**（从 S3 导入数据集）页面。\n",
    "\n",
    "1.在存储桶列表中，打开名称中包含 **databucket** 的存储桶。\n",
    "1.选择第一个数据集，即名为 **claims.csv** 的文件。\n",
    "\n",
    "1.选择 <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">Import</span>（导入）。\n",
    "\n",
    "1.返回至 **Data flow**（数据流）视图，选择 *CapstoneDataWrangler.flow* 选项卡左上角的 **&lt; 数据流**图标。\n",
    "1.选择 *CapstoneDataWrangler.flow* 选项卡左上角的 **Import**（导入）选项卡。\n",
    "\n",
    "此时 SageMaker Studio 将打开 **Create connection**（创建连接）页面。\n",
    "\n",
    "1.在 *CapstoneDataWrangler.flow* 选项卡中的 **Data sources**（数据源）部分，选择 **Amazon S3**。\n",
    "\n",
    "此时 SageMaker Studio 将在 *DataWrangler.flow* 选项卡中打开 **Import a dataset from S3**（从 S3 导入数据集）页面。\n",
    "\n",
    "1.在存储桶列表中，打开名称中包含 **databucket** 的存储桶。\n",
    "1.选择第二个数据集，即名为 **customers.csv** 的文件。\n",
    "1.选择 <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">Import</span>（导入）。\n",
    "\n",
    "1.返回至 **Data flow**（数据流）视图，选择 *CapstoneDataWrangler.flow* 选项卡左上角的 **&lt; 数据流**图标。\n",
    "\n",
    "1.在 <b>Data flow</b>（数据流）选项卡中，选择**数据类型**图标旁边的 **+** 符号，然后选择 **Get data insights**（获取数据见解）。\n",
    "\n",
    "1.创建报告并探索见解。\n",
    "\n",
    "1.返回至 **Data flow**（数据流）选项卡，选择**数据类型**图标旁边的 **+** 符号，然后选择 **Add analysis**（添加分析）。\n",
    "\n",
    "1.创建分析并探索结果。\n",
    "\n",
    "使用任何可以帮助您全面探索数据集的报告类型。完成后，您可以继续执行下一个任务。\n",
    "\n",
    "要继续本实验，请返回至<a href=\"#task1-2-continue\" target=\"_self\">任务 1.2</a>。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1-2-2\" id=\"task1-2-2\"></a>\n",
    "\n",
    "### 附录：在笔记本中探索数据集（任务 1.2）\n",
    "\n",
    "探索数据集的方法有很多。下面是您可以采取的一些数据探索步骤的示例。使用这些示例作为参考来开始探索数据集的各个方面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gender-graph\n",
    "import matplotlib.pyplot as plt\n",
    "customers_data.customer_gender_female.value_counts(normalize=True).plot.bar()\n",
    "plt.xticks([0, 1], [\"Male\", \"Female\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fraud-graph\n",
    "claims_data.fraud.value_counts(normalize=True).plot.bar()\n",
    "plt.xticks([0, 1], [\"Not Fraud\", \"Fraud\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#education-category-graphs\n",
    "educ = customers_data.customer_education.value_counts(normalize=True, sort=False)\n",
    "plt.bar(educ.index, educ.values)\n",
    "plt.xlabel(\"Customer Education Level\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#claim-amount-graph\n",
    "plt.hist(claims_data.total_claim_amount, bins=30)\n",
    "plt.xlabel(\"Total Claim Amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#claims-filed-graph\n",
    "customers_data.num_claims_past_year.hist(density=True)\n",
    "plt.suptitle(\"Number of Claims in the Past Year\")\n",
    "plt.xlabel(\"Number of claims per year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paid-plot-graphs\n",
    "sns.pairplot(\n",
    "    data=customers_data, vars=[\"num_insurers_past_5_years\", \"months_as_customer\", \"customer_age\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fraud-insurers-graph\n",
    "combined_data = customers_data.join(claims_data)\n",
    "sns.lineplot(x=\"num_insurers_past_5_years\", y=\"fraud\", data=combined_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#months-as-customer-graph\n",
    "sns.boxplot(x=customers_data[\"months_as_customer\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customer-age-graph\n",
    "sns.boxplot(x=customers_data[\"customer_age\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fraud-gender-graph\n",
    "combined_data.groupby(\"customer_gender_female\").mean()[\"fraud\"].plot.bar()\n",
    "plt.xticks([0, 1], [\"Male\", \"Female\"])\n",
    "plt.suptitle(\"Fraud by Gender\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation-matrix-graph\n",
    "cols = [\n",
    "    \"fraud\",\n",
    "    \"customer_gender_male\",\n",
    "    \"customer_gender_female\",\n",
    "    \"months_as_customer\",\n",
    "    \"num_insurers_past_5_years\",\n",
    "]\n",
    "corr = combined_data[cols].corr()\n",
    "\n",
    "# plot the correlation matrix\n",
    "sns.heatmap(corr, annot=True, cmap=\"Reds\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load-combined-data\n",
    "combined_data = pd.read_csv(\"./data/claims_customer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove-unnecessary-columns\n",
    "combined_data = combined_data.loc[:, ~combined_data.columns.str.contains(\"^Unnamed: 0\")]\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe-combined-data\n",
    "combined_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate-statistics\n",
    "combined_stats = []\n",
    "\n",
    "for col in combined_data.columns:\n",
    "    combined_stats.append(\n",
    "        (\n",
    "            col,\n",
    "            combined_data[col].nunique(),\n",
    "            combined_data[col].isnull().sum() * 100 / combined_data.shape[0],\n",
    "            combined_data[col].value_counts(normalize=True, dropna=False).values[0] * 100,\n",
    "            combined_data[col].dtype,\n",
    "        )\n",
    "    )\n",
    "\n",
    "stats_df = pd.DataFrame(\n",
    "    combined_stats,\n",
    "    columns=[\"feature\", \"unique_values\", \"percent_missing\", \"percent_largest_category\", \"datatype\"],\n",
    ")\n",
    "stats_df.sort_values(\"percent_largest_category\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap-graph\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "corr_list = [\n",
    "    \"customer_age\",\n",
    "    \"months_as_customer\",\n",
    "    \"total_claim_amount\",\n",
    "    \"injury_claim\",\n",
    "    \"vehicle_claim\",\n",
    "    \"incident_severity\",\n",
    "    \"fraud\",\n",
    "]\n",
    "\n",
    "corr_df = combined_data[corr_list]\n",
    "corr = round(corr_df.corr(), 2)\n",
    "\n",
    "fix, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "ax = sns.heatmap(corr, mask=mask, ax=ax, annot=True, cmap=\"OrRd\")\n",
    "\n",
    "ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=10, ha=\"right\", rotation=45)\n",
    "ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=10, va=\"center\", rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task1-2-continue\" target=\"_self\">任务 1.2</a>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1-3-1\" id=\"task1-3-1\"></a>\n",
    "\n",
    "### 附录：在 SageMaker Studio 中联接表（任务1.3）\n",
    "\n",
    "1.返回至 **Data flow**（数据流）视图，选择 *CapstoneDataWrangler.flow* 选项卡左上角的 **&lt; 数据流**图标。\n",
    "1.选择 **claims.CSV 数据类型**图标旁边的 **+** 符号，然后从上下文菜单中选择 **Join**（联接）。\n",
    "\n",
    "此时 SageMaker Data Wrangler 将显示 **Join**（联接）页面。\n",
    "\n",
    "1.选择 **customers.csv 数据类型**图标。\n",
    "1.选择 <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">Configure</span>（配置）。\n",
    "1.对于 **Join Type**（联接类型），选择 **Inner**（内部）。\n",
    "1.在 **Columns**（列）部分：\n",
    "\n",
    "    - 对于 **Left**（左侧），选择 <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">policy_id</span>。\n",
    "    \n",
    "    - 对于 **Right**（右侧），选择 <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">policy_id</span>。\n",
    "\n",
    "1.选择 <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">Preview</span>（预览）。\n",
    "\n",
    "1.选择 <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">Add</span>（添加）。\n",
    "\n",
    "要继续本实验，请返回至<a href=\"#task1-3-continue\" target=\"_self\">任务 1.3</a>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1-3-2\" id=\"task1-3-2\"></a>\n",
    "\n",
    "### 附录：运行训练前偏差报告（任务 1.3）\n",
    "\n",
    "使用 SageMaker Data Wrangler 流创建 SageMaker Clarify 偏差报告。\n",
    "\n",
    "1.选择 **CapstoneDataWrangler.flow** 选项卡。\n",
    "1.前往 **Data flow**（数据流）视图。如有必要，请选择 *DataWranglerLab.flow* 选项卡左上角的 **&lt; 数据流**图标。\n",
    "1.选择**联接**图标旁边的 **+** 符号，然后从上下文菜单中选择 **Add analysis**（添加分析）。\n",
    "1.在 **Create analysis**（创建分析）部分：\n",
    "\n",
    "- 对于 Analysis type（分析类型），选择 **Bias Report**（偏差报告）。\n",
    "- 对于 **Analysis name**（分析名称），输入 `fraud bias by age`。\n",
    "–对于 **Select the column your model predicts (target)**（选择您的模型预测的列 (目标)），选择 **fraud**（欺诈）。\n",
    "- 对于 **Is your predicted column a value or threshold?**（您的预测列是一个值还是阈值?），选择 **value**（值）选项。\n",
    "- 对于 **Predicted value(s)**（预测值），输入 **1**。\n",
    "–对于 **Select the column to analyze for bias**（选择要分析偏差的列），选择 **customer_age**。\n",
    "\n",
    "1.选择 <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">Check for bias</span>（检查偏差）。\n",
    "\n",
    "在任务完成后，查看返回的指标。注意是否有偏差，并计划您希望对所分析的列采取的任何处理步骤。\n",
    "\n",
    "1.对于任何您想要保存的分析，请选择 <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">Save</span>（保存）。\n",
    "\n",
    "您可以对想要分析偏差的任何列重复这些步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task1-3-continue\" target=\"_self\">任务 1.3</a>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1-4-1\" id=\"task1-4-1\"></a>\n",
    "\n",
    "### 附录：使用 SageMaker Data Wrangler 准备数据（任务 1.4）\n",
    "\n",
    "合并数据集，在 SageMaker Data Wrangler 中使用 **policy_id** 进行联接。\n",
    "\n",
    "使用 SageMaker Data Wrangler，您可以在流中的任何位置联接数据。您可以在联接之前完成各个文件的数据准备，也可以在联接之后转换特征。SageMaker Data Wrangler 流是灵活的。\n",
    "\n",
    "如果您还没有在任务 1.3 中联接表，下面的步骤将指导您完成这个过程。\n",
    "\n",
    "1.返回至 **Data flow**（数据流）视图，选择 *CapstoneDataWrangler.flow* 选项卡左上角的 **&lt; 数据流**图标。\n",
    "1.选择**数据类型**图标旁边的 **+** 符号，然后从上下文菜单中选择 **Join**（联接）。\n",
    "\n",
    "此时 SageMaker Data Wrangler 将显示 **Join**（联接）页面。\n",
    "\n",
    "1.选择第二个**数据类型**图标。\n",
    "1.选择 <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">Configure</span>（配置）。\n",
    "1.对于 **Join Type**（联接类型），选择 **Inner**（内部）。\n",
    "1.在 **Columns**（列）部分：\n",
    "\n",
    "    - 对于 **Left**（左侧），选择 <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">policy_id</span>。\n",
    "    \n",
    "    - 对于 **Right**（右侧），选择 <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">policy_id</span>。\n",
    "\n",
    "1.选择 <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">Preview</span>（预览）。\n",
    "\n",
    "1.选择 <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">Add</span>（添加）。\n",
    "\n",
    "联接数据表后，转换合并后的数据。\n",
    "\n",
    "1.返回至 **Data flow**（数据流）视图，选择 *CapstoneDataWrangler.flow* 选项卡左上角的 **&lt; 数据流**图标。\n",
    "1.选择**联接**图标旁边的 **+** 符号，然后从上下文菜单中选择 **Add transform**（添加转换）。\n",
    "\n",
    "可以使用此菜单向数据集添加多项转换。数据集的预览将显示在转换菜单的左侧。\n",
    "\n",
    "将以下转换步骤添加到 SageMaker Data Wrangler 流中：\n",
    "- 编码分类（独热编码）：**authorities_contacted**、**collision_type**、**customer_gender**、**driver_relationship**、**incident_type** 和 **policy_state**，使用 **Skip**（跳过）无效处理策略，选择输出样式为 **Columns**（列）。\n",
    "- 编码分类（有序编码）：**customer_education**、**incident_severity**、**police_report_available** 和 **policy_liability**，使用 **Skip**（跳过）无效处理策略。\n",
    "- 将列解析为类型：将 **vehicle_claim** 和 **total_claim_amount** 从 **Float** 解析为 **Long**。\n",
    "- 管理列（删除列）：**customer_zip** 和 **policy_id_1**。\n",
    "- 管理列（移动列）：**fraud**（使用 **Move to start**（移至开头））。\n",
    "- 管理列（重命名列）：将 **collision_type_N/A** 和 **driver_relationship_N/A** 中的 **/** 符号替换为 **_**。\n",
    "- 管理列（重命名列）：将 **policy_id_0** 重命名为 **policy_id**。\n",
    "\n",
    "如果任何列名中有 **/** 字符，则重命名该列，将 **/** 替换为 **_**。如果任何列名中有空白格字符，则重命名该列，将空白格替换为 **_**。例如，对于使用独热编码创建的任何列，如果值为 **N/A**，则需要重命名该列。SageMaker 特征存储不接受包含 **/** 或空白格字符的列。\n",
    "\n",
    "如果您已转换数据并做好了开始训练模型的准备，可以继续执行下一个任务。您可以随时返回至此流，并根据您在训练和优化期间的发现进行更改。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task1-4-continue\" target=\"_self\">任务 1.4</a>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1-4-2\" id=\"task1-4-2\"></a>\n",
    "\n",
    "### 附录：导入一部分处理过的数据（任务 1.4）\n",
    "\n",
    "如果您在预处理过程中遇到问题，或者希望加载一组已经为您处理过的数据，请访问存储在 S3 存储桶的数据文件夹中的处理过的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed-data-import\n",
    "s3_client.upload_file(Filename=\"data/claims_customer.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/claims_customer.csv\")\n",
    "df_processed = pd.read_csv(\"./data/claims_customer.csv\", index_col=None)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task1-4-continue\" target=\"_self\">任务 1.4</a>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task2-1\" id=\"task2-1\"></a>\n",
    "\n",
    "### 附录：使用 <b>Export to</b>（导出至）选项创建特征组（任务 2.1）\n",
    "\n",
    "SageMaker Data Wrangler 可以将数据导出至 SageMaker 特征存储。它会创建一个笔记本，其中包含配置特征组所需的所有代码，并会将转换后的数据摄取到特征组中。\n",
    "\n",
    "1.选择 **Add step**（添加步骤）。\n",
    "\n",
    "1.选择 **Custom transform**（自定义转换）。\n",
    "\n",
    "1.对于 **Name**（名称），输入 `event_time`。\n",
    "\n",
    "1.选择 **Python (PySpark)**（如果未选择）。\n",
    "\n",
    "1.对于 **Your custom transform**（您的自定义转换），输入以下内容：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "date_time = datetime.date.today()\n",
    "\n",
    "df = df.withColumn(\"event_time\", lit(time.mktime(date_time.timetuple())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.选择 <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">Preview</span>（预览）。\n",
    "\n",
    "1.选择 <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">Add</span>（添加）。\n",
    "\n",
    "这会将 **event_time** 作为列添加到您的数据集中。SageMaker 特征存储需要 **event_time** 和唯一 **record** ID。使用 **policy_id** 作为 **record** ID。\n",
    "\n",
    "1.要返回至数据流，请选择 **&lt; 数据流**图标。\n",
    "\n",
    "1.在 SageMaker Data Wrangler 中选择转换旁边的 **+** 图标。\n",
    "\n",
    "1.选择 **Export to**（导出至）。\n",
    "\n",
    "1.选择 **SageMaker Feature Store (via Jupyter Notebook)**（SageMaker 特征存储 (通过 Jupyter 笔记本)）。\n",
    "\n",
    "此时将打开一个新的笔记本。\n",
    "\n",
    "1.在第一个单元格中，更改以下变量：\n",
    "- 对于 **record_identifier_feature_name**，将 **None** 替换为 `\"policy_id\"`。如果您联接了 customers 表和 claims 表，并且没有删除第二个 **policy_id** 列，则可能需要将 **None** 替换为 `\"policy_id_0\"`。请勿更改 **if** 语句后面的 **None** 值。\n",
    "- 对于 **event_time_feature_name**，将 **None** 替换为 `\"event_time\"`。请勿更改 **if** 语句后面的 **None** 值。\n",
    "\n",
    "**预计输出**：当您完成单元格的编辑之后，它应该与以下示例类似：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifier_feature_name = \"policy_id\"\n",
    "if record_identifier_feature_name is None:\n",
    "   raise SystemExit(\"Select a column name as the feature group record identifier.\")\n",
    "\n",
    "event_time_feature_name = \"event_time\"\n",
    "if event_time_feature_name is None:\n",
    "   raise SystemExit(\"Select a column name as the event time feature name.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2.运行所有单元格以创建特征定义和特征组，并使用处理任务将转换后的数据摄取到特征组中。\n",
    "\n",
    "当单元格完成后，您即可使用特征存储。\n",
    "\n",
    "要继续本实验，请返回至<a href=\"#task2-1-continue\" target=\"_self\">任务 2.1</a>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task2-2\" id=\"task2-2\"></a>\n",
    "\n",
    "### 附录：使用 Athena 从离线存储中提取记录（任务 2.2）\n",
    "\n",
    "使用 **athena_query** 设置 Athena 查询。然后，设置 **query_string**。最后，运行查询并查看结果示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure-and-run-athena-query\n",
    "try:\n",
    "    # If there is a feature group, get the name\n",
    "    feature_group_name = sagemaker_session.boto_session.client(\"sagemaker\", region_name=region).list_feature_groups()['FeatureGroupSummaries'][0]['FeatureGroupName']\n",
    "    feature_group = FeatureGroup(name=feature_group_name, sagemaker_session=sagemaker_session)\n",
    "\n",
    "    # Confirm the Athena settings are configured\n",
    "    try:\n",
    "        boto3.client('athena').update_work_group(\n",
    "            WorkGroup='primary',\n",
    "            ConfigurationUpdates={\n",
    "                'EnforceWorkGroupConfiguration':False\n",
    "            }\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Configure the query\n",
    "    query = feature_group.athena_query()\n",
    "    table = query.table_name\n",
    "    query_string = f'SELECT * FROM \"{table}\" '\n",
    "    output_location = f\"s3://{sagemaker_session.default_bucket()}/query_results/\"\n",
    "    print(f\"Athena query output location: \\n{output_location}\")\n",
    "\n",
    "    # Run the query\n",
    "    query.run(query_string=query_string, output_location=output_location)\n",
    "    query.wait()\n",
    "    df_feature_store = query.as_dataframe()\n",
    "    \n",
    "    # Wait for data to appear in the feature group\n",
    "    attempts = 0\n",
    "    while len(df_feature_store.index) == 0 and attempts < 30:\n",
    "        print(\"Waiting for feature group to populate...\")\n",
    "        time.sleep(60)\n",
    "        # Rerun the query\n",
    "        query.run(query_string=query_string, output_location=output_location)\n",
    "        query.wait()\n",
    "        df_feature_store = query.as_dataframe()\n",
    "        # Increment the attempts\n",
    "        attempts += 1\n",
    "    if len(df_feature_store.index) != 0:\n",
    "        print(\"The feature group is populated.\")\n",
    "except IndexError as e:\n",
    "    # If there is no feature group, thrown an error\n",
    "    print(\"No feature groups were found. Please create a feature group.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行之前的代码块后，Amazon Athena 查询记录就保存在名称以 *sagemaker* 开头的 Amazon S3 存储桶中。保存的查询对象位于名为 **query_results** 的目录中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task2-2-continue\" target=\"_self\">任务 2.2</a>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task3-1\" id=\"task3-1\"></a>\n",
    "\n",
    "### 附录：为实验和运行命名（任务 3.1）\n",
    "\n",
    "要创建实验，请使用库 **sagemaker.experiments.run**。设置 **experiment_name**、**run_name** 和 **description**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "#create experiment and run-names\n",
    "create_date = strftime(\"%m%d%H%M\")\n",
    "capstone_experiment_name=\"capstone-experiment-{}\".format(create_date)\n",
    "capstone_run_name = \"lab-capstone-run-{}\".format(create_date)\n",
    "\n",
    "# define a run_tag\n",
    "run_tags = [{'Key': 'lab-capstone', 'Value': 'lab-capstone-run'}]\n",
    "\n",
    "# provide a description\n",
    "description=\"Using SM Experiments with the Auto dataset.\"\n",
    "\n",
    "print(f\"Experiment name - {capstone_experiment_name},  run name - {capstone_run_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task3-1-continue\" target=\"_self\">任务 3.1</a>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task3-2\" id=\"task3-2\"></a>\n",
    "\n",
    "### 附录：将数据分割成训练、测试和验证数据集（任务 3.2）\n",
    "\n",
    "要分割数据，请使用 **np.split** 并指定要如何分割数据。然后，创建 CSV 文件并将其上传到 S3 存储桶。接着，设置训练输入。最后，创建 **data_inputs** 变量。在整个挑战中使用 data_inputs，以在训练模型时指定训练和验证数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-validation-test-split\n",
    "try:\n",
    "    # If there is a feature group, use it\n",
    "    df_feature_store = df_feature_store.iloc[: , :-4]\n",
    "    df_processed_pre_split = df_feature_store\n",
    "    print(\"Using the records from the feature group\")\n",
    "except NameError:\n",
    "    # If there is no feature group, use the processed dataset\n",
    "    df_processed = pd.read_csv(\"./data/claims_customer.csv\", index_col=None)\n",
    "    df_processed_pre_split = df_processed\n",
    "    print(\"Using the processed records from Amazon S3\")\n",
    "\n",
    "# Split the data into train, validation, and test datasets\n",
    "train_data, validation_data, test_data = np.split(\n",
    "    df_processed_pre_split.sample(frac=1, random_state=1729),\n",
    "    [int(0.7 * len(df_processed_pre_split)), int(0.9 * len(df_processed_pre_split))],\n",
    ")\n",
    "\n",
    "# Create the CSV files and upload them to your default bucket\n",
    "train_data.to_csv(\"train_data.csv\", index=False, header=False)\n",
    "validation_data.to_csv(\"validation_data.csv\", index=False, header=False)\n",
    "test_data.to_csv(\"test_data.csv\", index=False, header=False)\n",
    "\n",
    "train_path = S3Uploader.upload(\"train_data.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "validation_path = S3Uploader.upload(\"validation_data.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "test_path = S3Uploader.upload(\"test_data.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "\n",
    "# Set the training inputs\n",
    "train_input = TrainingInput(train_path, content_type=\"text/csv\")\n",
    "validation_input = TrainingInput(validation_path, content_type=\"text/csv\")\n",
    "test_input = TrainingInput(test_path, content_type=\"text/csv\")\n",
    "\n",
    "data_inputs = {\n",
    "    \"train\": train_input,\n",
    "    \"validation\": validation_input\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task3-2-continue\" target=\"_self\">任务 3.2</a>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task3-3-1\" id=\"task3-3-1\"></a>\n",
    "\n",
    "### 附录：配置并运行基本训练任务（任务 3.3）\n",
    "\n",
    "如果您想从基本训练任务开始，请使用基本 **XGBoost** 容器。然后，配置估算器，记下您想要使用的容器和角色。设置了这些配置后，即可选择超参数。您可以使用下面的代码中提供的默认值，也可以根据您在数据准备期间的发现对其进行编辑。\n",
    "\n",
    "要运行训练任务，请调用 **fit()**，将输入设置为 **data_inputs** 变量，并为 **run_name** 和 **experiment_name** 设置配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker import image_uris\n",
    "#train-model\n",
    "# Retrieve the container image\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    region=boto3.Session().region_name, \n",
    "    framework=\"xgboost\", \n",
    "    version=\"1.5-1\"\n",
    ")\n",
    "\n",
    "# Set the hyperparameters\n",
    "eta=0.2\n",
    "gamma=4\n",
    "max_depth=5\n",
    "min_child_weight=6\n",
    "num_round=800\n",
    "objective='binary:logistic'\n",
    "subsample=0.8\n",
    "\n",
    "hyperparameters = {\n",
    "    \"eta\":eta,\n",
    "    \"gamma\":gamma,\n",
    "    \"max_depth\":max_depth,\n",
    "    \"min_child_weight\":min_child_weight,\n",
    "    \"num_round\":num_round,\n",
    "    \"objective\":objective,\n",
    "    \"subsample\":subsample\n",
    "}\n",
    "\n",
    "# Set up the estimator\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,    \n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    max_run=1800,\n",
    "    hyperparameters=hyperparameters,\n",
    "    tags = run_tags\n",
    ")\n",
    "\n",
    "with Run(\n",
    "    experiment_name=capstone_experiment_name,\n",
    "    run_name=capstone_run_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ") as run:\n",
    "    run.log_parameter(\"eta\", eta)\n",
    "    run.log_parameter(\"gamma\", gamma)\n",
    "    run.log_parameter(\"max_depth\", max_depth)\n",
    "    run.log_parameter(\"min_child_weight\", min_child_weight)\n",
    "    run.log_parameter(\"objective\", objective)\n",
    "    run.log_parameter(\"subsample\", subsample)\n",
    "    run.log_parameter(\"num_round\", num_round)\n",
    "\n",
    "# Train the model associating the training run with the current \"experiment\"\n",
    "    xgb.fit(\n",
    "        inputs = data_inputs\n",
    "    )        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task3-3-continue\" target=\"_self\">任务 3.3</a>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task3-3-2\" id=\"task3-3-2\"></a>\n",
    "\n",
    "### 附录：在启用了 SageMaker Debugger 的情况下配置并运行训练任务以及分析报告（任务 3.3）\n",
    "\n",
    "SageMaker Debugger 可以帮助您查看能够快速告知超参数优化情况的其他报告，从而在您开始运行更多具有超参数范围的训练任务时节省时间。要启用 Debugger，请配置 **DebuggerHookConfig** 和 **rules**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable-debugger\n",
    "# Retrieve the container image\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    region=boto3.Session().region_name, \n",
    "    framework=\"xgboost\", \n",
    "    version=\"1.5-1\"\n",
    ")\n",
    "\n",
    "# Set the hyperparameters\n",
    "eta=0.2\n",
    "gamma=4\n",
    "max_depth=5\n",
    "min_child_weight=6\n",
    "num_round=300\n",
    "objective='binary:logistic'\n",
    "subsample=0.7\n",
    "        \n",
    "hyperparameters = {\n",
    "        \"eta\":eta,\n",
    "        \"gamma\":gamma,\n",
    "        \"max_depth\":max_depth,\n",
    "        \"min_child_weight\":min_child_weight,\n",
    "        \"num_round\":num_round,\n",
    "        \"objective\":objective,\n",
    "        \"subsample\":subsample\n",
    "}\n",
    "\n",
    "# Set up the estimator\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role, \n",
    "    base_job_name=base_job_name,\n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    max_run=1800,\n",
    "    hyperparameters=hyperparameters,\n",
    "    tags = run_tags,\n",
    "\n",
    "    #Set the Debugger Hook Config\n",
    "    debugger_hook_config=DebuggerHookConfig(\n",
    "        s3_output_path=bucket_path,  # Required\n",
    "        collection_configs=[\n",
    "            CollectionConfig(name=\"metrics\", parameters={\"save_interval\": str(save_interval)}),\n",
    "            CollectionConfig(name=\"feature_importance\", parameters={\"save_interval\": str(save_interval)},),\n",
    "            CollectionConfig(name=\"full_shap\", parameters={\"save_interval\": str(save_interval)}),\n",
    "            CollectionConfig(name=\"average_shap\", parameters={\"save_interval\": str(save_interval)}),\n",
    "        ],\n",
    "        ),\n",
    "        #Set the Debugger Profiler Configuration\n",
    "        profiler_config = ProfilerConfig(\n",
    "            system_monitor_interval_millis=500,\n",
    "            framework_profile_params=FrameworkProfile()\n",
    "    ),\n",
    "        #Configure the Debugger Rule Object\n",
    "        rules = [\n",
    "            ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "            Rule.sagemaker(rule_configs.create_xgboost_report()),  \n",
    "            Rule.sagemaker(rule_configs.overfit()),\n",
    "            Rule.sagemaker(rule_configs.overtraining()),\n",
    "            Rule.sagemaker(rule_configs.loss_not_decreasing(),\n",
    "                rule_parameters={\n",
    "                    \"collection_names\": \"metrics\",\n",
    "                    \"num_steps\": str(save_interval * 2),\n",
    "                }\n",
    "            )\n",
    "    ]\n",
    ")\n",
    "with Run(\n",
    "    experiment_name=capstone_experiment_name,\n",
    "    run_name=capstone_run_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ") as run:\n",
    "    run.log_parameter(\"eta\", eta)\n",
    "    run.log_parameter(\"gamma\", gamma)\n",
    "    run.log_parameter(\"max_depth\", max_depth)\n",
    "    run.log_parameter(\"min_child_weight\", min_child_weight)\n",
    "    run.log_parameter(\"objective\", objective)\n",
    "    run.log_parameter(\"subsample\", subsample)\n",
    "    run.log_parameter(\"num_round\", num_round)\n",
    "# Train the model\n",
    "xgb.fit(\n",
    "    inputs = data_inputs\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task3-3-continue\" target=\"_self\">任务 3.3</a>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task3-4\" id=\"task3-4\"></a>\n",
    "\n",
    "### 附录：配置训练超参数范围（任务 3.4）\n",
    "\n",
    "现在您已经训练了至少一个模型，接下来可以使用从数据处理和 SageMaker Debugger 中了解到的信息来确定您为超参数选择的范围。编辑以下超参数范围并运行优化任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune-model\n",
    "# Setup the hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    \"alpha\": ContinuousParameter(0, 2),\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "    \"min_child_weight\": ContinuousParameter(1, 10),\n",
    "    \"num_round\": IntegerParameter(100, 1000)\n",
    "}\n",
    "\n",
    "# Define the target metric and the objective type (max/min)\n",
    "objective_metric_name = \"validation:auc\"\n",
    "objective_type=\"Maximize\"\n",
    "\n",
    "# Define the HyperparameterTuner\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator = xgb,\n",
    "    objective_metric_name = objective_metric_name,\n",
    "    hyperparameter_ranges = hyperparameter_ranges,\n",
    "    objective_type = objective_type,\n",
    "    max_jobs=12,\n",
    "    max_parallel_jobs=4,\n",
    "    early_stopping_type=\"Auto\"\n",
    ")\n",
    "\n",
    "# Tune the model\n",
    "tuner.fit(\n",
    "    inputs = data_inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task3-4-continue\" target=\"_self\">任务 3.4</a>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task4-1\" id=\"task4-1\"></a>\n",
    "\n",
    "### 附录：创建模型（任务 4.1）\n",
    "\n",
    "创建 XGBoost 模型，使用您定义的 **model_name**、**role** 和 **container_def** 调用 **create_model**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model-configurations\n",
    "model_name = \"capstone-clarify-model\"\n",
    "model = xgb.create_model(name=model_name)\n",
    "container_def = model.prepare_container_def()\n",
    "sagemaker_session.create_model(model_name, role, container_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task4-1-continue\" target=\"_self\">任务 4.1</a>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task4-2\" id=\"task4-2\"></a>\n",
    "\n",
    "### 附录：创建 SageMaker Clarify 模型配置（任务 4.2）\n",
    "\n",
    "使用 **SageMakerClarifyProcessor** 创建 SageMaker Clarify 模型配置。设置 **instance_count** 和 **instance_type**。使用在结业设计实验开始时创建的 **role** 和 **session**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-clarify-processor\n",
    "clarify_processor = clarify.SageMakerClarifyProcessor(\n",
    "    role=role, \n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.m5.xlarge\", \n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task4-2-continue\" target=\"_self\">任务 4.2</a>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task4-3\" id=\"task4-3\"></a>\n",
    "\n",
    "### 附录：创建 SageMaker Clarify 偏差配置（任务 4.3）\n",
    "\n",
    "要创建 SageMaker Clarify 偏差配置，请选择数据的输出路径，设置训练任务的输入路径以及 **label**、**headers** 和 **dataset_type**。\n",
    "\n",
    "然后，创建 **ModelConfig** 和 **ModelPredictedLabelConfig**。\n",
    "\n",
    "最后，配置 **BiasConfig**，其中包含您希望 SageMaker Clarify 观察的字段。您可以根据最初的发现添加或删除您想要探索的任何字段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-data-config\n",
    "bias_report_output_path = \"s3://{}/{}/clarify-bias\".format(bucket, prefix)\n",
    "bias_data_config = clarify.DataConfig(\n",
    "    s3_data_input_path=train_path,\n",
    "    s3_output_path=bias_report_output_path,\n",
    "    label=\"fraud\",\n",
    "    headers=train_data.columns.to_list(),\n",
    "    dataset_type=\"text/csv\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-model-config\n",
    "model_config = clarify.ModelConfig(\n",
    "    model_name=model_name,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    accept_type=\"text/csv\",\n",
    "    content_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-label-config\n",
    "predictions_config = clarify.ModelPredictedLabelConfig(probability_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-bias-config\n",
    "bias_config = clarify.BiasConfig(\n",
    "    label_values_or_threshold=[1], facet_name=\"customer_gender_female\", facet_values_or_threshold=[0], group_name=\"customer_age\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task4-3-continue\" target=\"_self\">任务 4.3</a>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task4-4\" id=\"task4-4\"></a>\n",
    "\n",
    "### 附录：使用 SageMaker Clarify 运行偏差、数据和模型报告（任务 4.4）\n",
    "\n",
    "现在您已经配置了 SageMaker Clarify 任务，接下来可以通过调用 **run_bias** 来运行任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run-bias-report\n",
    "clarify_processor.run_bias(\n",
    "    data_config=bias_data_config,\n",
    "    bias_config=bias_config,\n",
    "    model_config=model_config,\n",
    "    model_predicted_label_config=predictions_config,\n",
    "    pre_training_methods=\"all\",\n",
    "    post_training_methods=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task4-4-continue\" target=\"_self\">任务 4.4</a>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task4-5\" id=\"task4-5\"></a>\n",
    "\n",
    "### 附录：消除不平衡（任务 4.5）\n",
    "\n",
    "在本示例中，您要对 **customer_gender_female** 进行上采样，以减少数据集中的偏差。如果您发现其他包含偏差的特征，也可以消除这些特征中的不平衡。**random_state** 已设置为 `42`，但您可以更改此值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display-summary\n",
    "gender = train_data[\"customer_gender_female\"]\n",
    "gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove-imbalance\n",
    "sm = SMOTE(random_state=42)\n",
    "train_data_upsampled, gender_res = sm.fit_resample(train_data, gender)\n",
    "train_data_upsampled[\"customer_gender_female\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task4-5-continue\" target=\"_self\">任务 4.5</a>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task4-6\" id=\"task4-6\"></a>\n",
    "\n",
    "### 附录：重新训练模型（任务 4.6）\n",
    "\n",
    "您发现了不平衡，且有一个新的训练数据集。使用此数据集并重新训练文件。为此，请上传新文件并创建新的估算器。然后，使用 **fit()** 重新训练数据。下面的代码中包含了几个超参数作为样本。在重新训练期间，您可以根据需要添加、删除或调整这些超参数，从而找到最佳模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload-upsampled-csv\n",
    "train_data_upsampled.to_csv(\"data/upsampled_train.csv\", index=False, header=False)\n",
    "retrain_path = S3Uploader.upload(\"data/upsampled_train.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "retrain_input = TrainingInput(retrain_path, content_type=\"text/csv\")\n",
    "\n",
    "retrain_data_inputs = {\n",
    "    \"train\": retrain_input,\n",
    "    \"validation\": validation_input\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create-estimator\n",
    "hyperparameters= {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"300\",\n",
    "}\n",
    "\n",
    "xgb_retrained = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role, \n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain-upsampled-data\n",
    "xgb_retrained.fit(\n",
    "    inputs = retrain_data_inputs\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task4-6-continue\" target=\"_self\">任务 4.6</a>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task5-1\" id=\"task5-1\"></a>\n",
    "\n",
    "### 附录：创建批量转换任务（任务 5.1）\n",
    "\n",
    "使用模型估算器通过**转换器**创建批量转换任务。将策略设置为 **MultiRecord** 以提高处理效率。然后，传入 **test_path** 并等待推理运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create-batch-transformer\n",
    "# Use the retrained model if it exists, otherwise, use the original model\n",
    "try:\n",
    "    model = xgb_retrained\n",
    "except NameError:\n",
    "    model = xgb\n",
    "\n",
    "# Create the transformer\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    strategy=\"MultiRecord\",\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",\n",
    "    output_path=\"s3://{}/{}/batch-transform/\".format(bucket, prefix)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run-batch-transform-job\n",
    "test_data_batch = test_data.drop(\"fraud\", axis=1)\n",
    "test_data_batch.to_csv(\"test_data_batch.csv\", index=False, header=False)\n",
    "test_path_batch = S3Uploader.upload(\"test_data_batch.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "\n",
    "transformer.transform(test_path_batch, content_type=\"text/csv\", split_type=\"Line\", join_source=\"Input\")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task5-1-continue\" target=\"_self\">任务 5.1</a>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task5-2\" id=\"task5-2\"></a>\n",
    "\n",
    "### 附录：查看来自批量转换任务的预测和准确率数据（任务 5.2）\n",
    "\n",
    "完成批量转换任务后，查看存储在 Amazon S3 中的预测数据。您可以引用在**转换器**中设置的输出路径，并对数据进行采样。\n",
    "\n",
    "在此输出中，每条记录的末尾都附加了 **fraud** 预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $transformer.output_path ./\n",
    "test_data = pd.read_csv(\"test_data_batch.csv.out\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task5-2-continue\" target=\"_self\">任务 5.2</a>。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task5-3\" id=\"task5-3\"></a>\n",
    "\n",
    "### 附录：在 SageMaker Studio 中清理 SageMaker 实例（任务 5.3）\n",
    "\n",
    "在 SageMaker Studio 中开发模型时，请进行定期检查以了解是否有任何需要清理的实例。如果有，您可以在 SageMaker Studio 中关闭实例。\n",
    "\n",
    "1.在左侧菜单栏中，选择**正在运行的终端和内核**图标（中间有一个正方形的圆圈）。\n",
    "\n",
    "1.如果有任何实例仍然处于打开状态，在每个实例类型的右侧，选择**关闭**图标。\n",
    "\n",
    "您可以查看每个实例上正在运行的应用程序，以确认要关闭哪些应用程序。\n",
    "\n",
    "1.如果出现弹出窗口，请选择 **Shut down all**（关闭全部）。\n",
    "\n",
    "1.时不时选择**刷新列表**图标，直到列表中已不再有相应实例。关闭实例可能需要 2–5 分钟。\n",
    "\n",
    "您不需要关闭 **capstone.ipynb** 笔记本正在使用的实例。\n",
    "\n",
    "要继续本实验，请返回至<a href=\"#task5-3-continue\" target=\"_self\">任务 5.3</a>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task6-1\" id=\"task6-1\"></a>\n",
    "\n",
    "### 附录：配置管道（任务 6.1）\n",
    "\n",
    "要创建管道，请定义管道流程的每个步骤，然后运行该管道。\n",
    "\n",
    "在本示例中，您将创建以下步骤：\n",
    "- **AutoModelProcess**：在 .csv 文件中拉取数据并将其分割成训练、测试和验证数据集的**处理**步骤。\n",
    "- **AutoHyperParameterTuning**：使用一系列超参数并优化模型的**优化**步骤。\n",
    "- **AutoEvalBestModel**：创建评估报告来描述最佳模型的**处理**步骤。\n",
    "- **CheckAUCScoreAutoEvaluation**：根据评估指标评估模型的**条件**步骤。\n",
    "- **AutoCreateModel**：创建模型的**模型**步骤。\n",
    "- **RegisterAutoModel-RegisterModel**：注册模型的**注册模型**步骤。\n",
    "- **AutoModelConfigFile**：创建偏差报告的**处理**步骤。\n",
    "- **AutoTransform**：运行批量转换任务的**转换**步骤。\n",
    "- **ClarifyProcessingStep**：运行 SageMaker Clarify 任务的**处理**步骤。\n",
    "\n",
    "如果您在构建管道时遇到问题，可以自定义以下代码，或者在构建自己的管道时将其作为指导。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run-pipeline\n",
    "# Set the variables\n",
    "model_name = \"Auto-model\"\n",
    "sklearn_processor_version=\"0.23-1\"\n",
    "model_package_group_name=\"AutoModelPackageGroup\"\n",
    "pipeline_name= \"AutoModelSMPipeline\"\n",
    "clarify_image = sagemaker.image_uris.retrieve(framework='sklearn',version=sklearn_processor_version,region=region)\n",
    "\n",
    "# Upload files to the default S3 bucket\n",
    "s3_client.put_object(Bucket=bucket,Key='data/')\n",
    "s3_client.put_object(Bucket=bucket,Key='input/code/')\n",
    "s3_client.upload_file(Filename=\"data/batch_data.csv\", Bucket=bucket, Key=\"data/batch_data.csv\")  #If you edit this, make sure to also edit the headers listed in generate_config to match your column names.\n",
    "s3_client.upload_file(Filename=\"data/claims_customer.csv\", Bucket=bucket, Key=\"data/claims_customer.csv\")  #If you edit this, make sure to also edit the headers listed in generate_config to match your column names.\n",
    "s3_client.upload_file(Filename=\"pipelines/evaluate.py\", Bucket=bucket, Key=\"input/code/evaluate.py\")\n",
    "s3_client.upload_file(Filename=\"pipelines/generate_config.py\", Bucket=bucket, Key=\"input/code/generate_config.py\")\n",
    "s3_client.upload_file(Filename=\"pipelines/preprocess.py\", Bucket=bucket, Key=\"input/code/preprocess.py\")\n",
    "\n",
    "# Configure important settings. Change the input_data if you want to\n",
    "# use a file other than the claims_customer.csv and batch_data.csv files.\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "processing_instance_type = ParameterString(\n",
    "        name=\"ProcessingInstanceType\",\n",
    "        default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "training_instance_type = ParameterString(\n",
    "        name=\"TrainingInstanceType\",\n",
    "        default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "input_data = ParameterString(\n",
    "        name=\"InputData\",\n",
    "        default_value=\"s3://{}/data/claims_customer.csv\".format(bucket), \n",
    ")\n",
    "batch_data = ParameterString(\n",
    "        name=\"BatchData\",\n",
    "        default_value=\"s3://{}/data/batch_data.csv\".format(bucket),\n",
    ")\n",
    "\n",
    "# Run a scikit-learn script to do data processing on SageMaker using \n",
    "# using the SKLearnProcessor class\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "        framework_version=sklearn_processor_version,\n",
    "        instance_type=processing_instance_type.default_value, \n",
    "        instance_count=processing_instance_count,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    ")\n",
    "\n",
    "# Configure the processing step to pull in the input_data\n",
    "step_process = ProcessingStep(\n",
    "        name=\"AutoModelProcess\",\n",
    "        processor=sklearn_processor,\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\",\\\n",
    "                             destination=f\"s3://{bucket}/output/train\" ),\n",
    "            ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\",\\\n",
    "                            destination=f\"s3://{bucket}/output/validation\"),\n",
    "            ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\",\\\n",
    "                            destination=f\"s3://{bucket}/output/test\"),\n",
    "            ProcessingOutput(output_name=\"batch\", source=\"/opt/ml/processing/batch\",\\\n",
    "                            destination=f\"s3://{bucket}/data/batch\"),\n",
    "            ProcessingOutput(output_name=\"baseline\", source=\"/opt/ml/processing/baseline\",\\\n",
    "                            destination=f\"s3://{bucket}/input/baseline\")\n",
    "        ],\n",
    "        code=f\"s3://{bucket}/input/code/preprocess.py\",\n",
    "        job_arguments=[\"--input-data\", input_data],\n",
    ")\n",
    "\n",
    "# Set up the model path, image uri, and hyperparameters for the estimator\n",
    "model_path = f\"s3://{bucket}/output\"\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.5-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=training_instance_type.default_value,\n",
    ")\n",
    "\n",
    "fixed_hyperparameters = {\n",
    "    \"eval_metric\":\"auc\",\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"num_round\":\"100\",\n",
    "    \"rate_drop\":\"0.3\",\n",
    "    \"tweedie_variance_power\":\"1.4\"\n",
    "}\n",
    "\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    hyperparameters=fixed_hyperparameters,\n",
    "    output_path=model_path,\n",
    "    base_job_name=f\"auto-train\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "# Set the hyperparameter ranges for the tuning step and configure the tuning step\n",
    "hyperparameter_ranges = {\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "    \"min_child_weight\": ContinuousParameter(1, 10),\n",
    "    \"alpha\": ContinuousParameter(0, 2),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "}\n",
    "objective_metric_name = \"validation:auc\"\n",
    "\n",
    "step_tuning = TuningStep(\n",
    "    name = \"AutoHyperParameterTuning\",\n",
    "    tuner = HyperparameterTuner(xgb_train, objective_metric_name, hyperparameter_ranges, max_jobs=2, max_parallel_jobs=2),\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Configure the processing step for evaluation\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-auto-eval\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"AutoEvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"AutoEvalBestModel\",\n",
    "    processor=script_eval,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_tuning.get_top_model_s3_uri(top_k=0,s3_bucket=bucket,prefix=\"output\"),\n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\",\\\n",
    "                            destination=f\"s3://{bucket}/output/evaluation\"),\n",
    "    ],\n",
    "    code=f\"s3://{bucket}/input/code/evaluate.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")\n",
    "\n",
    "# Configure model creation\n",
    "model = Model(\n",
    "    image_uri=image_uri,        \n",
    "    model_data=step_tuning.get_top_model_s3_uri(top_k=0,s3_bucket=bucket,prefix=\"output\"),\n",
    "    name=model_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    accelerator_type=\"ml.inf1.xlarge\",\n",
    ")\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"AutoCreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    ")\n",
    "\n",
    "script_processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=clarify_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=processing_instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "bias_report_output_path = f\"s3://{bucket}/clarify-output/bias\"\n",
    "clarify_instance_type = 'ml.m5.xlarge'\n",
    "step_config_file = ProcessingStep(\n",
    "    name=\"AutoModelConfigFile\",\n",
    "    processor=script_processor,\n",
    "    code=f\"s3://{bucket}/input/code/generate_config.py\",\n",
    "    job_arguments=[\"--modelname\",step_create_model.properties.ModelName,\"--bias-report-output-path\",bias_report_output_path,\"--clarify-instance-type\",clarify_instance_type,\\\n",
    "                  \"--default-bucket\",bucket,\"--num-baseline-samples\",\"50\",\"--instance-count\",\"1\"],\n",
    "    depends_on= [step_create_model.name]\n",
    ")\n",
    "\n",
    "# Configure the step to perform a batch transform job\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",    \n",
    "    output_path=f\"s3://{bucket}/AutoTransform\"\n",
    ")\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"AutoTransform\",\n",
    "    transformer=transformer,\n",
    "    inputs=TransformInput(data=batch_data,content_type=\"text/csv\",join_source=\"Input\",split_type=\"Line\")\n",
    ")\n",
    "\n",
    "# Configure the SageMaker Clarify processing step\n",
    "analysis_config_path = f\"s3://{bucket}/clarify-output/bias/analysis_config.json\"\n",
    "\n",
    "data_config = sagemaker.clarify.DataConfig(\n",
    "    s3_data_input_path=f's3://{bucket}/output/train/train.csv', \n",
    "    s3_output_path=bias_report_output_path,\n",
    "    label=0,\n",
    "    headers=list(pd.read_csv(\"./data/claims_customer.csv\", index_col=None).columns), #If you edit this, make sure to also edit the headers listed in generate_config to match your column names.\n",
    "    dataset_type=\"text/csv\",\n",
    ")\n",
    "\n",
    "clarify_processor = sagemaker.clarify.SageMakerClarifyProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=clarify_instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "config_input = ProcessingInput(\n",
    "    input_name=\"analysis_config\",\n",
    "    source=analysis_config_path,\n",
    "    destination=\"/opt/ml/processing/input/analysis_config\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_compression_type=\"None\",\n",
    ")\n",
    "\n",
    "data_input = ProcessingInput(\n",
    "    input_name=\"dataset\",\n",
    "    source=data_config.s3_data_input_path,\n",
    "    destination=\"/opt/ml/processing/input/data\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=data_config.s3_data_distribution_type,\n",
    "    s3_compression_type=data_config.s3_compression_type,\n",
    ")\n",
    "\n",
    "result_output = ProcessingOutput(\n",
    "    source=\"/opt/ml/processing/output\",\n",
    "    destination=data_config.s3_output_path,\n",
    "    output_name=\"analysis_result\",\n",
    "    s3_upload_mode=\"EndOfJob\",\n",
    ")\n",
    "\n",
    "step_clarify = ProcessingStep(\n",
    "    name=\"ClarifyProcessingStep\",\n",
    "    processor=clarify_processor,\n",
    "    inputs= [data_input, config_input],\n",
    "    outputs=[result_output],\n",
    "    depends_on = [step_config_file.name]\n",
    ")\n",
    "\n",
    "# Configure the model registration step\n",
    "model_statistics = MetricsSource(\n",
    "    s3_uri=\"s3://{}/output/evaluation/evaluation.json\".format(bucket),\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "explainability = MetricsSource(\n",
    "    s3_uri=\"s3://{}/clarify-output/bias/analysis.json\".format(bucket),\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "\n",
    "bias = MetricsSource(\n",
    "    s3_uri=\"s3://{}/clarify-output/bias/analysis.json\".format(bucket),\n",
    "    content_type=\"application/json\"\n",
    ") \n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=model_statistics,\n",
    "    explainability=explainability,\n",
    "    bias=bias\n",
    ")\n",
    "\n",
    "step_register = RegisterModel(\n",
    "    name=\"RegisterAutoModel\",\n",
    "    estimator=xgb_train,\n",
    "    model_data=step_tuning.get_top_model_s3_uri(top_k=0,s3_bucket=bucket,prefix=\"output\"),\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "\n",
    "# Create the model evaluation step\n",
    "cond_lte = ConditionGreaterThan(\n",
    "    left=JsonGet(\n",
    "        step=step_eval,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"binary_classification_metrics.auc.value\"\n",
    "    ),\n",
    "    right=0.75,\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"CheckAUCScoreAutoEvaluation\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_create_model,step_config_file,step_transform,step_clarify,step_register],\n",
    "    else_steps=[],\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "def get_pipeline(\n",
    "    region,\n",
    "    role=None,\n",
    "    default_bucket=None,\n",
    "    model_package_group_name=\"AutoModelPackageGroup\",\n",
    "    pipeline_name=\"AutoModelPipeline\",\n",
    "    base_prefix = None,\n",
    "    custom_image_uri = None,\n",
    "    sklearn_processor_version=None\n",
    "    ):\n",
    "    \"\"\"Gets a SageMaker ML Pipeline instance working with auto data.\n",
    "    Args:\n",
    "        region: AWS region to create and run the pipeline.\n",
    "        role: IAM role to create and run steps and pipeline.\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        an instance of a pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    # pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[\n",
    "            processing_instance_type,\n",
    "            processing_instance_count,\n",
    "            training_instance_type,\n",
    "            input_data,\n",
    "            batch_data,\n",
    "        ],\n",
    "        steps=[step_process,step_tuning,step_eval,step_cond],\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = get_pipeline(\n",
    "    region = region,\n",
    "    role=role,\n",
    "    default_bucket=bucket,\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    pipeline_name=pipeline_name,\n",
    "    custom_image_uri=clarify_image,\n",
    "    sklearn_processor_version=sklearn_processor_version\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n",
    "\n",
    "# Run the pipeline\n",
    "RunPipeline = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task6-1-continue\" target=\"_self\">任务 6.1</a>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task6-2\" id=\"task6-2\"></a>\n",
    "\n",
    "### 附录：监控管道（任务 6.2）\n",
    "\n",
    "现在您已经创建并运行了管道，接下来可以监控管道。您可以在 SageMaker Studio 中查看管道状态。\n",
    "\n",
    "如果您要删除管道，可以使用 **delete_pipeline** 将其删除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe-pipeline\n",
    "RunPipeline.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list-pipeline-steps\n",
    "RunPipeline.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove-pipeline\n",
    "response = sagemaker_session.boto_session.client(\"sagemaker\", region_name=region).delete_pipeline(PipelineName='AutoModelSMPipeline')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要继续本实验，请返回至<a href=\"#task6-2-continue\" target=\"_self\">任务 6.2</a>。"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
