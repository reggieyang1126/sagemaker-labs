{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Perform data processing with SageMaker Processing and the built-in scikit-learn container\n",
    "\n",
    "In this notebook, you set up the environment needed to run a scikit-learn script using a Docker image provided and maintained by SageMaker Processing. \n",
    "\n",
    "You then use the **SKLearnProcessor** class from the SageMaker Python SDK to define and run a scikit-learn processing job.\n",
    "\n",
    "Finally, you validate the data processing results saved in Amazon Simple Storage Service (Amazon S3).\n",
    "\n",
    "**Note:** The processing script does some basic data processing, such as removing duplicates, transforming the target column into a column that contains two labels, one-hot encoding, and an 80-20 split to produce training and test datasets. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1: Environment setup\n",
    "\n",
    "In this task, you install the needed packages and dependencies. \n",
    "\n",
    "You set up an Amazon S3 bucket to store the outputs from the processing job and get the execution role to run the SageMaker Processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install-dependencies\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "#Execution role to run the SageMaker Processing job\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"SageMaker Execution Role: \", role)\n",
    "\n",
    "#S3 bucket to read the SKLearn processing script and writing processing job outputs\n",
    "s3 = boto3.resource('s3')\n",
    "for buckets in s3.buckets.all():\n",
    "    if 'labdatabucket' in buckets.name:\n",
    "        bucket = buckets.name\n",
    "print(\"Bucket: \", bucket)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2: Run the SageMaker processing job\n",
    "\n",
    "In this task, you import and review the preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import-data\n",
    "prefix = 'data/input'\n",
    "\n",
    "S3Downloader.download(s3_uri=f\"s3://{bucket}/{prefix}/adult_data.csv\", local_path= 'data/')\n",
    "\n",
    "shape=pd.read_csv(\"data/adult_data.csv\", header=None)\n",
    "shape.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, data transformations such as removing duplicates, transforming the target column into a column that contains two labels, and one-hot encoding of the categorical features are be performed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You then create a SKLearnProcessor class to define and run a scikit-learn processing script as a processing job. Refer to [Sagemaker scikit-learn SKLearnProcessor](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html#sagemaker.sklearn.processing.SKLearnProcessor) for more information about this class.\n",
    "\n",
    "For creating the SKLearnProcessor class, you configure the following parameters:\n",
    "- **base_job_name**: Prefix for the processing job name\n",
    "- **framework_version**: scikit-learn version\n",
    "- **role**: SageMaker execution role\n",
    "- **instance_count**: Number of instances to run the processing job\n",
    "- **instance_type**: Type of Amazon Elastic Compute Cloud (Amazon EC2) instance used for the processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scikit-learn-processor\n",
    "import os\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "# create a SKLearnProcessor\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    base_job_name=\"sklearn-preprocessor\",\n",
    "    framework_version=\"0.20.0\", \n",
    "    role=role, \n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, you use the **SKLearnProcessor.run()** method to run a **sklearn_preprocessing.py** script as a processing job. \n",
    "\n",
    "For running the processing job, you configure the following parameters:\n",
    "- **code**: Path of the preprocessing script \n",
    "- **inputs and outputs**: Path of input and output for the preprocessing script (Amazon S3 input and output locations)\n",
    "- **arguments**: Command-line arguments to the preprocessing script (such as a train and test split ratio)\n",
    "\n",
    "The processing job takes approximately 4â€“5 minutes to complete. While the job is running, you can review the source for the preprocessing script (which has been preconfigured as part of this lab) by opening the **sklearn_preprocessing.py** file from the file browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing-job\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Amazon S3 path prefix\n",
    "input_raw_data_prefix = \"data/input\"\n",
    "output_preprocessed_data_prefix = \"data/output\"\n",
    "scripts_prefix = \"scripts/smstudiofiles\"\n",
    "logs_prefix = \"logs\"\n",
    "\n",
    "# Run the processing job\n",
    "sklearn_processor.run(\n",
    "    code=\"s3://\" + os.path.join(bucket, scripts_prefix, \"sklearn_preprocessing.py\"),\n",
    "    inputs=[ProcessingInput(source=\"s3://\" + os.path.join(bucket, input_raw_data_prefix, \"adult_data.csv\"),\n",
    "                            destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", \n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"train\")),\n",
    "        ProcessingOutput(output_name=\"test_data\", \n",
    "                         source=\"/opt/ml/processing/test\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"test\")),\n",
    "    ],\n",
    "    arguments=[\"--train-test-split-ratio\", \"0.2\"],\n",
    ")\n",
    "\n",
    "print(\"SKLearn Processing Job Completed.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3: Validate the data processing results\n",
    "\n",
    "In this task, you validate the output of the processing job that you ran by reviewing the first five rows of the train and test output datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-train-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/train/train_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-validation-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/validation/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/test/test_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Congratulations! You have used SageMaker Processing to successfully create a scikit-learn processing job using the SageMaker Python SDK and run the processing job.\n",
    "\n",
    "The next task of the lab focuses on data processing using SageMaker Processing with your own processing container.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with **Task 4: Perform data processing with your own container**."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
