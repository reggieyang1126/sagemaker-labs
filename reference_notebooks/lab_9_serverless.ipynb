{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Deploy a model for serverless inference\n",
    "\n",
    "## Task 3.1: Environment setup\n",
    "\n",
    "Install packages and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install-dependencies\n",
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker_datawrangler\n",
    "import sys\n",
    "import time\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "prefix = 'sagemaker/mlasms'\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model from the training and tuning lab in the default Amazon Simple Storage Service (Amazon S3) bucket. Set up a model using **create_model** and configure **ModelDataUrl** to reference the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set-up-model\n",
    "# Upload the model to your Amazon S3 bucket\n",
    "s3_client.upload_file(\n",
    "    Filename=\"model.tar.gz\", Bucket=bucket, Key=f\"{prefix}/models/model.tar.gz\"\n",
    ")\n",
    "\n",
    "# Set a date to use in the model name\n",
    "create_date = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "model_name = 'income-model-{}'.format(create_date)\n",
    "\n",
    "# Retrieve the container image\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    region=boto3.Session().region_name, \n",
    "    framework='xgboost', \n",
    "    version='1.5-1'\n",
    ")\n",
    "\n",
    "# Set up the model\n",
    "income_model = sm.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': f's3://{bucket}/{prefix}/models/model.tar.gz',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2: Create an endpoint from the provided synthesized, retrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Serverless Inference is a purpose-built inference option that helps you to deploy and scale machine learning (ML) models. Serverless Inference is ideal for workloads that have idle periods between traffic spurts and that can tolerate cold starts. Serverless endpoints automatically launch compute resources and scale them in and out depending on traffic. So, you do not need to choose instance types or manage scaling policies. This takes away the undifferentiated heavy lifting of selecting and managing servers. Serverless Inference integrates with AWS Lambda to offer you high availability, built-in fault tolerance, and automatic scaling.\n",
    "\n",
    "There are three steps to creating a serverless endpoint using the Amazon SageMaker Python SDK. These are the same steps that are used for the real-time endpoint, but the steps have different configurations:\n",
    "1. Create a SageMaker model in SageMaker.\n",
    "2. Create an endpoint configuration for an HTTPS endpoint.\n",
    "3. Create an HTTPS endpoint.\n",
    "\n",
    "You have already created a model. You are now ready to create an endpoint configuration and an endpoint. \n",
    "\n",
    "First, set up the endpoint configuration name and the memory size that you want to use. Then, call the CreateEndpointConfig API.\n",
    "\n",
    "To create an endpoint configuration, you need to set the following options:\n",
    "- **VariantName**: The name of the production variant (one or more models in production).\n",
    "- **ModelName**: The name of the model that you want to host. This is the name that you specified when you created the model.\n",
    "- **ServerlessConfig**: This is where the endpoint is set as serverless. Configure the values for **MemorySizeInMB** and **MaxConcurrency**.\n",
    "    - **MemorySizeInMB**: The allocated memory size (1024, 2048, 3072, 4096, 5120, or 6144 MB).\n",
    "    - **MaxConcurrency**: The number of concurrent invocations (1 to 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create-endpoint-configuration \n",
    "# Create an endpoint config name. Here you create one based on the date so you can search endpoints based on creation time.\n",
    "endpoint_config_name = 'income-model-serverless-endpoint-{}'.format(create_date)                              \n",
    "\n",
    "endpoint_config_response = sm.create_endpoint_config(\n",
    "   EndpointConfigName=endpoint_config_name,\n",
    "   ProductionVariants=[\n",
    "        {\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"variant1\", # The name of the production variant\n",
    "            \"ServerlessConfig\": {\n",
    "                \"MemorySizeInMB\": 2048, # The memory size\n",
    "                \"MaxConcurrency\": 20 # Number of concurrent invocations\n",
    "            }\n",
    "        } \n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Created EndpointConfig: {endpoint_config_response['EndpointConfigArn']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create an endpoint. When you create a serverless endpoint, SageMaker provisions and manages the compute resources for you. Then, you can make inference requests to the endpoint and receive model predictions in response. SageMaker scales the compute resources up and down as needed to handle your request traffic, and you only pay for what you use.\n",
    "\n",
    "You can choose either a container provided by SageMaker or bring your own. A serverless endpoint has a minimum RAM size of 1024 MB and a maximum RAM size of 6144 MB. Serverless Inference auto-assigns compute resources proportional to the memory that you select.\n",
    "\n",
    "When the endpoint is in service, the helper function prints the endpoint Amazon Resource Name (ARN). Endpoint creation can take as long as 7 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create-endpoint\n",
    "# The name of the endpoint. The name must be unique within an AWS Region in your AWS account.\n",
    "endpoint_name = '{}-name'.format(endpoint_config_name)\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    EndpointConfigName=endpoint_config_name\n",
    ") \n",
    "\n",
    "def wait_for_endpoint_creation_complete(endpoint):\n",
    "    \"\"\"Helper function to wait for the completion of creating an endpoint\"\"\"\n",
    "    response = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = response.get(\"EndpointStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Endpoint Creation\")\n",
    "        time.sleep(15)\n",
    "        response = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "        status = response.get(\"EndpointStatus\")\n",
    "\n",
    "    if status != \"InService\":\n",
    "        print(f\"Failed to create endpoint, response: {response}\")\n",
    "        failureReason = response.get(\"FailureReason\", \"\")\n",
    "        raise SystemExit(\n",
    "            f\"Failed to create endpoint {create_endpoint_response['EndpointArn']}, status: {status}, reason: {failureReason}\"\n",
    "        )\n",
    "    print(f\"Endpoint {create_endpoint_response['EndpointArn']} successfully created.\")\n",
    "\n",
    "wait_for_endpoint_creation_complete(endpoint=create_endpoint_response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SageMaker Studio, you can view the endpoint details under the **Endpoints** tab.\n",
    "\n",
    "The next step opens a new tab in SageMaker Studio. To follow these directions, use one of the following options:\n",
    "- **Option 1:** View the tabs side by side. To create a split screen view from the main SageMaker Studio window, either drag the **serverless_inference.ipynb** tab to the side or choose (right-click) the **serverless_inference.ipynb** tab and choose **New View for Notebook**. You can now have the directions visible as you explore the endpoint.\n",
    "- **Option 2:** Switch between the SageMaker Studio tabs to follow these instructions. When you are finished exploring the endpoint, return to the notebook by choosing the **serverless_inference.ipynb** tab.\n",
    "\n",
    "1. Choose the **SageMaker Home** icon.\n",
    "2. Choose **Deployments**.\n",
    "3. Choose **Endpoints**.\n",
    "\n",
    "SageMaker Studio displays the **Endpoints** tab.\n",
    "\n",
    "4. Select the endpoint which has **income-model-serverless-** in the **Name** column.\n",
    "\n",
    "If the endpoint does not appear, choose the refresh icon until the endpoint appears.\n",
    "\n",
    "SageMaker Studio displays the **ENDPOINT DETAILS** tab.\n",
    "\n",
    "5. Choose the **AWS settings** tab.\n",
    "\n",
    "If you opened the endpoint before it finished creating, choose the refresh icon until the **Endpoint status** changes from *Creating* to *InService*.\n",
    "\n",
    "The **Endpoint type** is listed as **Serverless**. The **Endpoint runtime settings** section shows the configurations that you chose earlier in the notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3: Invoke an endpoint for a serverless inference with customer records\n",
    "\n",
    "After you deploy your model using SageMaker hosting services, you can test your model on that endpoint by sending it test data.\n",
    "\n",
    "If your endpoint does not receive traffic for a while and then your endpoint suddenly receives new requests, it can take some time for your endpoint to spin up the compute resources to process the requests. This is called a cold start. Because serverless endpoints provision compute resources on demand, your endpoint might experience cold starts. A cold start can also occur if your concurrent requests exceed the current concurrent request usage. The cold start time depends on your model size, how long it takes to download your model, and the start-up time of your container.\n",
    "\n",
    "Refer to [Serverless Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html) for more information about how serverless inference and cold starts work.\n",
    "\n",
    "You received several more customer records. Confirm that the endpoint is working by invoking it with a set of records with an income value of 1 and records with an income value of 0. A list of the prediction scores for each record is output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoke-endpoint-serverless-records\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name=region)\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    ContentType='text/csv',\n",
    "    EndpointName=endpoint_name, \n",
    "    Body=bytes('47,0,4,9,0,3,4,0,1,0,1902,60\\n' +\n",
    "                '53,0,0,0,0,2,4,0,1,0,0,40\\n' +\n",
    "                '44,0,0,0,2,0,1,0,1,14344,0,40\\n', 'utf-8')\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "print('\\nTesting with records that have an income value of 1:')\n",
    "print('The returned scores are: {}'.format(response['Body'].read().decode('utf-8')))\n",
    "\n",
    "start_time = time.time()\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    ContentType='text/csv',\n",
    "    EndpointName=endpoint_name, \n",
    "    Body=bytes('19,0,1,1,1,1,2,1,0,0,0,35\\n' +\n",
    "                '56,2,1,1,0,1,0,0,0,0,0,50\\n' +\n",
    "                '61,2,0,0,0,0,0,0,0,0,0,40\\n', 'utf-8')\n",
    ")\n",
    "\n",
    "print('\\nTesting with records that have an income value of 0:')\n",
    "print('The returned scores are: {}'.format(response['Body'].read().decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.4: Delete the endpoint\n",
    "\n",
    "Cleaning up an endpoint can be accomplished in three steps. First, delete the endpoint. Then, delete the endpoint configuration. Finally, if you no longer need the model that you deployed, delete the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete-resources\n",
    "# Delete endpoint\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "# Delete endpoint configuration\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "                   \n",
    "# Delete model\n",
    "sm.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Congratulations! You have used SageMaker to successfully create a serverless endpoint, using the SageMaker Python SDK, and to invoke the endpoint.\n",
    "\n",
    "The next task of the lab focuses on deploying a model for inference using asynchronous inference.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with **Task 4: Deploy a model for asynchronous inference**."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e45558c452cedcb26631315a9b3b77e80a9c32d662ed25df58964b99bc5b9b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
