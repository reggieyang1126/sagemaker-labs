{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Use SageMaker Clarify\n",
    "\t\n",
    "In this lab, you use Amazon SageMaker Clarify to detect bias in pre-training data and post-training models and access explainability reports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: Environment setup\n",
    "\n",
    "Install packages and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install-dependencies\n",
    "\n",
    "import boto3\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sagemaker\n",
    "import sagemaker_datawrangler\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "from sagemaker import clarify\n",
    "from sagemaker import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import gmtime, strftime\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "session = Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'sagemaker/lab_8'\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import, split, and upload the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare-dataset\n",
    "\n",
    "lab_test_data = pd.read_csv('adult_data_processed.csv')\n",
    "\n",
    "# Split the dataset\n",
    "train_data, validation_data, test_data = np.split(\n",
    "    lab_test_data.sample(frac=1, random_state=1729),\n",
    "    [int(0.7 * len(lab_test_data)), int(0.9 * len(lab_test_data))],\n",
    ")\n",
    "\n",
    "train_data.to_csv('train_data.csv', index=False, header=False)\n",
    "validation_data.to_csv('validation_data.csv', index=False, header=False)\n",
    "test_data.to_csv('test_data.csv', index=False, header=False)\n",
    "\n",
    "# Upload the Dataset to S3\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "train_path = S3Uploader.upload('train_data.csv', 's3://{}/{}'.format(bucket, prefix))\n",
    "validation_path = S3Uploader.upload('validation_data.csv', 's3://{}/{}'.format(bucket, prefix))\n",
    "test_path = S3Uploader.upload('test_data.csv', 's3://{}/{}'.format(bucket, prefix))\n",
    "\n",
    "train_input = TrainingInput(train_path, content_type='text/csv')\n",
    "validation_input = TrainingInput(validation_path, content_type='text/csv')\n",
    "test_input = TrainingInput(validation_path, content_type='text/csv')\n",
    "\n",
    "data_inputs = {\n",
    "    'train': train_input,\n",
    "    'validation': validation_input\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train the XGBoost model. You use this trained model for the SageMaker Clarify ModelConfig. The training takes approximately 3-4 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-model\n",
    "\n",
    "# Retrieve the container image\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    region=boto3.Session().region_name, \n",
    "    framework='xgboost', \n",
    "    version='1.5-1'\n",
    ")\n",
    "# Set up the estimator\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "# Set the hyperparameters\n",
    "xgb.set_hyperparameters(\n",
    "    max_depth=5, \n",
    "    eta=0.2, \n",
    "    gamma=4, \n",
    "    min_child_weight=6,\n",
    "    subsample=0.8, \n",
    "    verbosity=1, \n",
    "    objective='binary:logistic', \n",
    "    num_round=800\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb.fit(\n",
    "    inputs = data_inputs\n",
    ") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2: Set the SageMaker Clarify job and access the bias report\n",
    "\n",
    "To use [SageMaker Clarify](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html), you first create a model from your training job. Then you set up the necessary configurations and run the SageMaker Clarify job on your trained model. In this task, you complete the following:\n",
    "\n",
    "- Create the model.\n",
    "- Enable SageMaker Clarify.\n",
    "- Run the bias report.\n",
    "- Access the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.1: Create the model\n",
    "\n",
    "Create a model from the training job to use with SageMaker Clarify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create-clarify-model\n",
    "\n",
    "model_name = \"lab-8-clarify-model\"\n",
    "model = xgb.create_model(name=model_name)\n",
    "container_def = model.prepare_container_def()\n",
    "session.create_model(model_name, role, container_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.2 Enable SageMaker Clarify\n",
    "\n",
    "Now, to begin configuration, enable SageMaker Clarify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable-clarify\n",
    "\n",
    "clarify_processor = clarify.SageMakerClarifyProcessor(\n",
    "    role=role, \n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.m5.xlarge\", \n",
    "    sagemaker_session=session\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SageMaker Clarify, you can detect potential pre-training bias in your data and post-training bias in your models through various metrics. Refer to [metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-processing-job-configure-analysis.html) for more information about configuring the clarify analysis.\n",
    "\n",
    "Configure the **DataConfig** object to communicate data I/O information to SageMaker Clarify. Here, you specify the location of the input dataset, the location for the output, the income (**label**) column, the header names, and the dataset type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-data-config\n",
    "\n",
    "bias_report_output_path = \"s3://{}/{}/clarify-bias\".format(bucket, prefix)\n",
    "bias_data_config = clarify.DataConfig(\n",
    "    s3_data_input_path=train_path,\n",
    "    s3_output_path=bias_report_output_path,\n",
    "    label=\"income\",\n",
    "    headers=train_data.columns.to_list(),\n",
    "    dataset_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the **ModelConfig** object to communicate information about your trained model. Set the model name and a temporary dedicated endpoint to avoid additional traffic to your production model (**instance_type** and **instance_count**). Also set an **accept_type** to denote the endpoint response payload format, and a **content_type** to denote the payload format of request to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-model-config\n",
    "\n",
    "model_config = clarify.ModelConfig(\n",
    "    model_name=model_name,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    accept_type=\"text/csv\",\n",
    "    content_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the **probability_threshold** of the **ModelPredictedLabelConfig** to convert the probability of samples to binary labels for bias analysis. Prediction above the threshold is interpreted as label value **1** and below or equal as label value **0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-model-predicted-label-config\n",
    "\n",
    "predictions_config = clarify.ModelPredictedLabelConfig(probability_threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the **BiasConfig** to denote the sensitive columns (**facets**), potential sensitive features (**facet_values_or_threshold**), and favorable outcomes (**label_values_or_threshold**).\n",
    "\n",
    "You can denote both categorical and continuous data for **facet_values_or_threshold** and **label_values_or_threshold**. The **sex** and **age** features are categorical.\n",
    "\n",
    "The goal of this model is to determine if an individual makes 50,000 USD or more, with the positive outcome being favorable. Use **BiasConfig** to provide information on which columns contain the facets with the sensitive group of **Sex**, what the sensitive features might be using **facet_values_or_threshold**, and what the desirable outcomes are using **label_values_or_threshold**. Group by **age** to see if there are any differences depending on the individual's age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-bias-config\n",
    "\n",
    "bias_config = clarify.BiasConfig(\n",
    "    label_values_or_threshold=[1], facet_name=\"sex\", facet_values_or_threshold=[0], group_name=\"age\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.3: Run the bias report\n",
    "\n",
    "Create the bias report using your configurations for pre-training and post-training analysis. This step takes approximately 15–20 minutes. While the bias report is being generated, you can discover how SageMaker Clarify computes [Fairness Measures for Machine Learning in Finance](./Fairness.Measures.for.Machine.Learning.in.Finance.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run-bias-report\n",
    "\n",
    "clarify_processor.run_bias(\n",
    "    data_config=bias_data_config,\n",
    "    bias_config=bias_config,\n",
    "    model_config=model_config,\n",
    "    model_predicted_label_config=predictions_config,\n",
    "    pre_training_methods=\"all\",\n",
    "    post_training_methods=\"all\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2.4: Access the bias report\n",
    "\n",
    "In SageMaker Studio, you can view the results under the experiments tab.\n",
    "\n",
    "The next step opens a new tab in SageMaker Studio. To follow these directions, use one of the following options:\n",
    "- **Option 1:** View the tabs side by side. To create a split screen view from the main SageMaker Studio window, either drag the **lab_8.ipynb** tab to the side or choose (right-click) the **lab_8.ipynb** tab and choose **New View for Notebook**. You can now have the directions visible as you explore the bias report.\n",
    "- **Option 2:** Switch between the SageMaker Studio tabs to follow these instructions. When you are finished exploring the bias report, return to the notebook by choosing the **lab_8.ipynb** tab.\n",
    "\n",
    "1. In SageMaker Studio, choose the **SageMaker Home** icon.\n",
    "\n",
    "2. Choose **Experiments**.\n",
    "\n",
    "SageMaker studio opens the **Experiments** tab.\n",
    "\n",
    "3. In the **Experiments** tab, select **Unassigned runs** from the left.\n",
    "\n",
    "4. In the **Unassigned runs** list, select the training job name that contains **clarify-bias-** in the title. \n",
    "\n",
    "5. In the **Experiments** tab, select **Bias Reports** from the left.\n",
    "\n",
    "A bias report of the metrics is available when SageMaker Clarify finishes running the bias job.\n",
    "\n",
    "6. Wait for SageMaker Clarify to finish running the bias job.\n",
    "\n",
    "Each bias metric has detailed explanations with examples that you can explore by selecting each value. \n",
    "\n",
    "7. Examine the **Class Imbalance** and **Disparate (Adverse) Impact (DI)** detailed explanations by choosing the arrows next to the bias metrics, and expanding the field. \n",
    "\n",
    "When you are finished exploring the bias metrics, continue with the next task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3: Access explainability reports\n",
    "\n",
    "In addition to using a bias report, you can use SageMaker Clarify to analyze the local explanations of individual predictions. Create a report to review explainability results for the predictions produced by SageMaker Clarify and analyze key metrics from the report. In this task, you complete the following:\n",
    "\n",
    "1. Define a Shapley Values (SHAP) configuration.\n",
    "2. Run the explainability report.\n",
    "3. Access the explainability report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3.1: Define a SHAP configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run an explainability report to get an explanation of why a model made a specific prediction. SageMaker Clarify uses SHAP to generate a report on the model's reasoning for its decisions. Refer to [SHAP Baselines for Explainability](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-feature-attribute-shap-baselines.html) for more information about SHAP baselines.\n",
    "\n",
    "The SHAP metrics that are configured are as follows:\n",
    "- **baseline**: A list of rows, or an Amazon Simple Storage Service (Amazon S3) object URI, to be used as the baseline dataset in the Kernel SHAP algorithm.\n",
    "- **num_samples**: Number of samples to be used in the Kernel SHAP algorithm. This number determines the size of the generated synthetic dataset to compute the SHAP values.\n",
    "- **agg_method: mean_abs**: Mean of absolute SHAP values for all instances.\n",
    "- **save_local_shap_values**: Boolean value to indicate if local SHAP values are to be saved in the output location.\n",
    "\n",
    "Refer to [SHAP metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-processing-job-configure-analysis.html) for more information about the metrics used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure-shap\n",
    "\n",
    "testing_data, clarify_data = train_test_split(test_data, test_size =0.005)\n",
    "clarify_data = clarify_data.drop(columns=[\"income\"])\n",
    "clarify_data.to_csv('clarify_data.csv', index=False, header=False)\n",
    "clarify_path = S3Uploader.upload('clarify_data.csv', 's3://{}/{}'.format(bucket, prefix))\n",
    "\n",
    "shap_config = clarify.SHAPConfig(\n",
    "    baseline=clarify_path,\n",
    "    num_samples=15,\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=True,\n",
    ")\n",
    "\n",
    "explainability_output_path = \"s3://{}/{}/clarify-explainability\".format(bucket, prefix)\n",
    "explainability_data_config = clarify.DataConfig(\n",
    "    s3_data_input_path=clarify_path,\n",
    "    s3_output_path=explainability_output_path,\n",
    "    headers=clarify_data.columns.to_list(),\n",
    "    dataset_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3.2: Run the explainability report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the explainability report using your configurations. This step takes approximately 10–15 minutes. While the explainability report is being generated, you can follow along with the job status in SageMaker Studio by continuing with the next task.\n",
    "\n",
    "Refer to [Amazon AI Fairness and Explainability Whitepaper](./Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf) for more information about the SageMaker Clarify explainability process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run-explainability-report\n",
    "\n",
    "clarify_processor.run_explainability(\n",
    "    data_config=explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3.3: Access the explainability report\n",
    "\n",
    "As with the bias report, you can review the explainability report in Studio as part of the experiments.\n",
    "\n",
    "1. Select the **Experiments** tab.\n",
    "\n",
    "2. In the **Experiments** tab, select **Unassigned runs** from the left.\n",
    "\n",
    "3. In the **Unassigned runs** list, select the training job name that contains **clarify-bias-** in the title. \n",
    "\n",
    "4. In the **Experiments** tab, select **Explainability** from the left.\n",
    "\n",
    "5. When the explainability job is finished, to view the **Feature Importance** chart, choose <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">View sample notebook</span>.\n",
    "\n",
    "SageMaker Studio opens **Fairness and Explainability with SageMaker Clarify** notebook in a tab.\n",
    "\n",
    "What features have the highest and lowest importance? Are there any results that are surprising?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Congratulations! You have used SageMaker Clarify to create bias and explainability reports that you can use to develop more trustworthy and compliant models. In the next lab, you deploy your models and run inference. You continue working with your model in the next lab.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with the **Conclusion**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e45558c452cedcb26631315a9b3b77e80a9c32d662ed25df58964b99bc5b9b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
