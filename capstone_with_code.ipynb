{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: Build an End-to-End Tabular Data ML Project Using SageMaker Studio and the Amazon SageMaker Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "This basic setup code has been included to help you get started. Read and run these cells first to get packages installed and variables created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U shap\n",
    "%pip install -U smdebug\n",
    "%pip install imbalanced-learn\n",
    "%pip install pytest-cov\n",
    "%pip install pytest-filter-subpackage\n",
    "%pip install sagemaker\n",
    "%pip install -U seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "#required-libraries\n",
    "\n",
    "import boto3\n",
    "import datetime as datetime\n",
    "import io\n",
    "import IPython\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt  # visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import re\n",
    "import sagemaker\n",
    "import seaborn as sns  # visualization\n",
    "import statistics\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sagemaker import clarify\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n",
    "from sagemaker.debugger import CollectionConfig, DebuggerHookConfig, FrameworkProfile, ProfilerConfig, ProfilerRule, Rule, rule_configs\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.experiments.run import Run, load_run\n",
    "from sagemaker.feature_store.feature_definition import FeatureDefinition\n",
    "from sagemaker.feature_store.feature_definition import FeatureTypeEnum\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.processing import FeatureStoreOutput\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.workflow.condition_step import ConditionStep, JsonGet\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThan\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-743682382463\n"
     ]
    }
   ],
   "source": [
    "#basic-variable-code-and-settings\n",
    "\n",
    "%matplotlib inline\n",
    "base_job_name = \"capstone-smdebugger-job\"\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "bucket_path = \"s3://{}\".format(bucket)\n",
    "prefix = \"sagemaker/capstone\"\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sagemaker_session = sagemaker.Session()\n",
    "save_interval = 5\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "\n",
    "Five tables are stored in an Amazon Simple Storage Service (Amazon S3) bucket:\n",
    "- **claims.csv**: A table with raw claims data.\n",
    "- **customers.csv**: A table with raw customer data.\n",
    "- **claims_preprocessed.csv**: A table with processed claims data.\n",
    "- **customers_preprocessed.csv**: A table with processed customer data.\n",
    "- **claims_customer.csv**: A table joined on the **policy_id** of the processed claims and customer data.\n",
    "\n",
    "For this lab, start with the **claims.csv** and **customers.csv** tables. You process them in **Challenge 1** using Amazon SageMaker Data Wrangler. If you get stuck, or want a reference of what the processed dataset should be, you can review the pre-processed tables.\n",
    "\n",
    "For this dataset, the target is **fraud**, a column in the claims table.\n",
    "\n",
    "The claims table includes the following fields: \n",
    "\n",
    "- **policy_id**: The unique ID for the policy.\n",
    "- **driver_relationship**: A list of relationships (Spouse, Self, Child, Other, N/A).\n",
    "- **incident_type**: The incident type reported (Break-In, Collision, Theft).\n",
    "- **collision_type**: The location of the collision (Front, Rear, Side, N/A).\n",
    "- **incident_severity**: The severity of the incident (Minor, Major, Totaled).\n",
    "- **authorities_contacted**: The type of authorities first contacted (None, Police, Ambulance, Fire).\n",
    "- **num_vehicles_involved**: The number of vehicles involved in the incident (a range from 1 to 6).\n",
    "- **num_injuries**: The number of injuries involved in the incident (a range from 1 to 4).\n",
    "- **num_witnesses**: The number of witnesses to the incident (a range from 1 to 5).\n",
    "- **police_report_available**: Whether or not a police report is available (yes or no).\n",
    "- **injury_claim**: The value claimed for injuries in US dollars (300 to 576,300 USD).\n",
    "- **vehicle_claim**: The value claimed for vehicle damage in US dollars (1000 to 51,051 USD).\n",
    "- **total_claim_amount**: The total value claimed for injuries and damages (2100 to 588,868 USD).\n",
    "- **incident_month**: The month of the incident (a range from 1 to 12).\n",
    "- **incident_day**: The day of the incident (a range from 1 to 31).\n",
    "- **incident_dow**: The day of the week of the incident (a range from 0 to 6 representing Sunday to Saturday).\n",
    "- **incident_hour**: The hour of the incident (a range from 0 to 23)\n",
    "- **fraud**: Whether or not the policy was fraudulent (0 or 1).\n",
    "\n",
    "The customers table includes the following fields:\n",
    "\n",
    "- **policy_id**: The unique ID for the policy.\n",
    "- **customer_age**: The age of the customer (a range from 18 to 70).\n",
    "- **months_as_customer**: The number of months for which this customer has paid insurance (a range from 1 to 495).\n",
    "- **num_claims_past_year**: The number of claims made by the customer in the past year.\n",
    "- **num_insurers_past_5_years**: The number of insurers that the customer had in the past 5 years.\n",
    "- **policy_state**: The state that the customer lives in (AZ, CA, ID, NV, OR, WA).\n",
    "- **policy_deductable**: The deductable value of the policy in US dollars (a range from 750 to 1100 USD).\n",
    "- **policy_annual_premium**: The annual premium of the policy in US dollars (a range from 2200 to 3000 USD).\n",
    "- **policy_liability**: The liability maximums for bodily injury, split into single and all bodily injuries (15/30, 25/50, 60/90, 100/200).\n",
    "- **customer_zip**: The zip code of the customer (a range from 83201 to 99362).\n",
    "- **customer_gender**: The gender of the customer (Male, Female, Other, Unknown).\n",
    "- **customer_education**: The education level of the customer (Below High School, High School, Associate, Bachelor, Advanced Degree).\n",
    "- **auto_year**: The year in which the automobile was made (a range from 2001 to 2020).\n",
    "\n",
    "You can join these tables with an inner join on the **policy_id** column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge lab navigation\n",
    "\n",
    "This lab is set up with links that navigate between the challenge tasks and the appendix at the end of the notebook. If you want to review an item in the appendix, choose the associated hyperlink. When you want to return to the challenge that you are currently working on, choose the corresponding task hyperlink in the appendix.\n",
    "\n",
    "The lab is organized as follows:\n",
    "\n",
    "- Challenge 1: Analyze and prepare the dataset with SageMaker Data Wrangler\n",
    "- Challenge 2: Create feature groups in SageMaker Feature Store\n",
    "- Challenge 3: Train the model\n",
    "- Challenge 4: Evaluate the model for bias\n",
    "- Challenge 5: Batch transform\n",
    "- Challenge 6: Build an automated pipeline\n",
    "- Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Analyze and prepare the dataset with SageMaker Data Wrangler\n",
    "\n",
    "AnyCompany Consulting has received a request to analyze auto insurance fraud datasets and build a model to help predict if new claims are likely to be fraudulent or not. It has 5,000 customer records where it has labeled each claim as fraudulent or not. You can use this data to train, test, and validate your model before running inference on a new batch collection of records.\n",
    "\n",
    "Use analysis features of Amazon SageMaker Data Wrangler to visualize the distributions of data in important columns, check for correlation between columns, and check for target leakage. Next, build a quick baseline model. Then, use data processing features of SageMaker Data Wrangler to transform columns so that they are better suited for training a more performant model. \n",
    "\n",
    "To complete this task, you complete the following subtasks:\n",
    "\n",
    "- Review your data.\n",
    "- Complete an exploratory data analysis in Amazon SageMaker Studio.\n",
    "- Use an Amazon SageMaker Clarify processor job to run a bias report.\n",
    "- Prepare your data.\n",
    "\n",
    "This challenge takes approximately *100* minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Review your data\n",
    "\n",
    "<a id=\"task1-1-continue\"></a>\n",
    "\n",
    "Access the tabular auto insurance dataset stored in your repository and review a sample of the dataset. The repository contains two unprocessed tables. One for customer data named **customers.csv** and one for claims data named **claims.csv**.\n",
    "\n",
    "**Hint 1**: The unprocessed tables are located in the **./data/** folder.\n",
    "\n",
    "**Hint 2**: The **claims.csv** and **customers.csv** tables are the unprocessed tables.\n",
    "\n",
    "Take a moment to explore the tables. Are there any fields that stand out? Are there any fields that require careful preprocessing?\n",
    "\n",
    "For detailed steps how to review your data, refer to <a href=\"#task1-1\" target=\"_self\">**Review your data (Task 1.1)**</a> in the *Appendix* section.\n",
    "\n",
    "After you access the auto insurance fraud table and review a sample of the dataset, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_1_1_code_here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Complete an exploratory data analysis in SageMaker Studio \n",
    "\n",
    "<a id=\"task1-2-continue\"></a>\n",
    "\n",
    "Complete an exploratory data analysis by reviewing the data, identifying potential issues in the dataset, and checking for strong correlations between any columns and the target. You can explore the data in SageMaker Data Wrangler and in the notebook.\n",
    "\n",
    "Specifically, spend time reviewing the following items:\n",
    "- **Column histograms**: Review the columns in a visual format and check what kinds of values are in the dataset.\n",
    "- **Quick model**: Review the dataset and think about an expected model outcome.\n",
    "- **Feature correlation**: Check if there is strong correlation between any columns and the target.\n",
    "- **Target leakage**: Check if there is any data that is dependent on the target value.\n",
    "\n",
    "Opening SageMaker Data Wrangler opens new tab in SageMaker Studio. To follow these directions, use one of the following options:\n",
    "- **Option 1:** View the tabs side by side. To create a split screen view from the main SageMaker Studio window, either drag the **capstone.ipynb** tab to the side or choose (right-click) the **capstone.ipynb** tab and choose **New View for Notebook**. You can now have the directions visible as you work with the SageMaker Data Wrangler flow.\n",
    "- **Option 2:** Switch between the SageMaker Studio tabs to follow these instructions. When you are finished exploring the SageMaker Data Wrangler steps, return to the notebook by choosing the **capstone.ipynb** tab.\n",
    "\n",
    "If you get a **\"The following instance type is not available: ml.m5.4xlarge. Try selecting a different instance below.\"** error when creating a new flow file, you can choose another instance type. Try the **ml.m5.8xlarge** next.\n",
    "\n",
    "If the message **An error occurred loading this view** displays, close the **untitled.flow** tab and reopen the flow file from the file browser.\n",
    "\n",
    "**Hint 1**: There are many ways to explore the dataset. Open a SageMaker Data Wrangler flow to get started. You need both the **claims.csv** and **customers.csv** imported into SageMaker Data Wrangler from the S3 bucket that contains **databucket-** in its name.\n",
    "\n",
    "**Hint 2**: To import a second table, return to your **Data flow**, then choose the **Import** tab to import another dataset.\n",
    "\n",
    "**Hint 3**: **Get data insights** and **Add analysis** are two ways to explore data in SageMaker Data Wrangler. After you review some sample charts from your data, you can use other plotting tools in the notebook to analyze the data if you want to. The **plt** and **sns** libraries have been installed. Feel free to use any analysis tools that you are familiar with to explore the dataset.  \n",
    "\n",
    "**Hint 4**: Try joining the two tables using a **Join** on **policy_id**. Then, run another insights report. You can use an **Inner** join for these tables.\n",
    "\n",
    "Did you get more meaningful results with a joined dataset?\n",
    "\n",
    "For detailed steps how to explore a dataset in SageMaker Studio, refer to <a href=\"#task1-2-1\" target=\"_self\">**Explore a dataset in SageMaker Studio (Task 1.2)**</a> in the *Appendix* section.\n",
    "\n",
    "For detailed steps how to explore a dataset in the notebook, refer to <a href=\"#task1-2-2\" target=\"_self\">**Explore a dataset in the notebook (Task 1.2)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have processed the data using SageMaker Data Wrangler, explored the dataset and identified the processing steps that you want to conduct, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_1_2_code_here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Use a SageMaker Clarify processor job to run a bias report\n",
    "\n",
    "<a id=\"task1-3-continue\"></a>\n",
    "\n",
    "Use SageMaker Clarify to run a pre-training bias report to catch class imbalance in the data. Use a SageMaker Data Wrangler flow to run the bias report in SageMaker Studio.\n",
    "1. Start with joining the two tables.\n",
    "\n",
    "- Join: **Inner** join for **claims.csv** to **customers.csv** on **policy_id**.\n",
    "\n",
    "**Hint 1**: To create a pre-training bias report, add a new analysis to your SageMaker Data Wrangler flow and choose **Bias Report** for the **Analysis type**.\n",
    "\n",
    "**Hint 2**: You can run the bias report several times, choosing different features to analyze each time.\n",
    "\n",
    "For detailed steps how to join tables with SageMaker Data Wrangler, refer to <a href=\"#task1-3-1\" target=\"_self\">**Joining tables in SageMaker Studio (Task 1.3)**</a> in the *Appendix* section.\n",
    "\n",
    "For detailed steps how to run a pre-training bias report, refer to <a href=\"#task1-3-2\" target=\"_self\">**Run a pre-training bias report (Task 1.3)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have run the pre-training bias report and viewed the report, you have completed this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4: Prepare your data\n",
    "\n",
    "<a id=\"task1-4-continue\"></a>\n",
    "\n",
    "Prepare your dataset using SageMaker Data Wrangler. Focus on the following transformations, but feel free to include other transformations:\n",
    "\n",
    "- Encode categorical (One-hot encoding): **authorities_contacted**, **collision_type**, **customer_gender**, **driver_relationship**, **incident_type**, and **policy_state**.\n",
    "- Ordinal encode: **customer_education**, **policy_liability**, **incident_severity**, and **police_report_available**.\n",
    "- Parse column as type: **vehicle_claim** and **total_claim_amount** from **Float** to **Long**.\n",
    "- Manage columns (Drop column): **customer_zip**.\n",
    "- Manage columns (Move column): **fraud** (using **Move to start**).\n",
    "- Manage columns (Rename column): Remove the **/** symbol from **collision_type_N/A**, and **driver_relationship_N/A** with a **_**.\n",
    "- Manage columns (Rename column): Rename **policy_id_0** to **policy_id**.\n",
    "\n",
    "**Hint 1**: Join the **claims** table to the **customers** tables using a join in SageMaker Data Wrangler. \n",
    "\n",
    "**Hint 2**: Join the two tables on the **policy_id** column.\n",
    "\n",
    "**Hint 3**: Add transformations using the **Add transform** option.\n",
    "\n",
    "Which transformations do you think affects the model training the most?\n",
    "\n",
    "For detailed steps how to prepare data using SageMaker Data Wrangler, refer to <a href=\"#task1-4-1\" target=\"_self\">**Prepare data using SageMaker Data Wrangler (Task 1.4)**</a> in the *Appendix* section.\n",
    "\n",
    "If you want to import an example set of processed data, refer to <a href=\"#task1-4-2\" target=\"_self\">**Import an example set of processed data (Task 1.4)**</a> in the *Appendix* section. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: Create feature groups in SageMaker Feature Store\n",
    "\n",
    "Now that you have processed the dataset, create features and feature groups to be used in future analysis. Use SageMaker Feature Store to store these in a feature group and query them when training your model.\n",
    "\n",
    "To complete this task, complete all of the following subtasks:\n",
    "\n",
    "1. Export features to SageMaker Feature Store.\n",
    "2. Query the feature group in an offline store with Amazon Athena.\n",
    "\n",
    "This challenge takes approximately *30* minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Export features to SageMaker Feature Store\n",
    "\n",
    "<a id=\"task2-1-continue\"></a>\n",
    "\n",
    "Use the SageMaker Data Wrangler **Export to** feature to create a custom Jupyter Notebook. The notebook creates a feature definition and a feature group. The notebook ingests the records into the feature group. In the notebook, complete the following steps:\n",
    "\n",
    "- Set the values for record and event_time.\n",
    "- Run the notebook cells to create the feature group.\n",
    "- Run the notebook cells to confirm the created feature group.\n",
    "- Run the notebook cells to ingest records into the feature group.\n",
    "\n",
    "**Hint 1**: All these steps can be completed in SageMaker Studio. When you have finished creating your feature group, you can return to this notebook to continue with Task 2.2.\n",
    "\n",
    "**Hint 2**: Add a custom transformation to create the **event_time** column. \n",
    "\n",
    "**Hint 3**: The code to add the custom transformation is as follows:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import lit\n",
    "date_time = datetime.date.today()\n",
    "\n",
    "df = df.withColumn(\"event_time\", lit(time.mktime(date_time.timetuple())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "sess = boto3.Session(region_name=region)\n",
    "sagemaker_client = sess.client(service_name='sagemaker', region_name=region)\n",
    "featurestore_runtime = sess.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "feature_store_session = Session(\n",
    "    boto_session=sess,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")\n",
    "feature_group_name = 'FG-capstone-flow-86eb6ede'\n",
    "#initialize-feature-group\n",
    "\n",
    "feature_group = FeatureGroup(\n",
    "    name=feature_group_name, sagemaker_session=feature_store_session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Hint 4**: At the end of your SageMaker Data Wrangler flow, choose the **+** icon, choose the **Export to** option, and choose **SageMaker Feature Store (via JupyterNotebook)**.\n",
    "\n",
    "**Hint 5**: You can turn off the online store by changing the **enable_online_store** value from **True** to **False**.\n",
    "\n",
    "How would you use SageMaker Feature Store to store and query your records for training, as opposed to inference?\n",
    "\n",
    "For detailed steps how to create a feature group using the **Export to** option, refer to <a href=\"#task2-1\" target=\"_self\">**Create a feature group using the Export to option (Task 2.1)**</a> in the *Appendix* section. \n",
    "\n",
    "After you have created a feature group and ingested data into the feature group, you have completed this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Query the feature group in an offline store with Athena\n",
    "\n",
    "<a id=\"task2-2-continue\"></a>\n",
    "\n",
    "Use Athena to extract records from an offline data store. You split these records into train, test, and validation sets in the next challenge.\n",
    "\n",
    "Use the provided code cell below to make the Amazon Athena API calls. You could use the Amazon Athena console to make the query, but that is beyond the scope of this lab.\n",
    "\n",
    "**Hint 1**: You can create an Athena query with **feature_group.athena_query()** and get the table name with **query.table_name**.\n",
    "\n",
    "**Hint 2**: You can run a query with **query.run(query_string=query_string, output_location=output_location)** and read the returned value as a dataframe with **query.as_dataframe()**.\n",
    "\n",
    "How could you use **event_time** to keep track of features that occur at different points in your dataset's timeline?\n",
    "\n",
    "For detailed steps how to extract records from an offline data store with Athena, refer to <a href=\"#task2-2\" target=\"_self\">**Extract records from an offline store with Athena (Task 2.2)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have saved the returned Athena query as a dataframe variable, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_2_2_code_here\n",
    "\n",
    "output_location = f's3://{bucket}/query_results/'\n",
    "\n",
    "query = feature_group.athena_query()\n",
    "query_string = f'SELECT * FROM \"sagemaker_featurestore\".\"fg_capstone_flow_86eb6ede_1727637810\" '\n",
    "query.run(query_string=query_string, output_location=output_location)\n",
    "query.wait()\n",
    "df = query.as_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: Train the model\n",
    "\n",
    "Your model is ready to train. Split the data into train, test, and validation datasets, and train the model. \n",
    "\n",
    "SageMaker Autopilot was run on this data earlier and got a **F1** of **0.616**, an **accuracy** of **0.978**, an **AUC** of **0.918**, and a **Recall** of **0.539**. To learn more about the metrics that SageMaker Autopilot produces, refer to the *autopilot-metrics-validation* document in the *Additional resources* section for more information.\n",
    "\n",
    "While you are training and tuning, work on reaching or exceeding the SageMaker Autopilot scores and confirm that Amazon SageMaker Debugger is reporting no errors.\n",
    "\n",
    "To complete this task, you complete the following subtasks:\n",
    "\n",
    "- Create an experiment and a run.\n",
    "- Split the data into train, test, and validation datasets.\n",
    "- Configure and run a training job.\n",
    "    - Run a basic training job.\n",
    "    - Run a training job with SageMaker Debugger enabled and analyze the reports (optional).\n",
    "- Perform hyperparameter tuning.\n",
    "\n",
    "This challenge takes approximately *110* minutes to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Name an experiment and a run\n",
    "\n",
    "<a id=\"task3-1-continue\"></a>\n",
    "\n",
    "Set up variables to name both the experiment and the runs. \n",
    "An experiment requires an **experiment_name**, **run_name** and a **description**. \n",
    "\n",
    "For detailed steps how to create the variables, refer to <a href=\"#task3-1\" target=\"_self\">**Name an experiment and a run (Task 3.1)**</a> in the *Appendix* section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_3_1_code_here\n",
    "from time import gmtime, strftime\n",
    "\n",
    "#create experiment and run-names\n",
    "create_date = strftime(\"%m%d%H%M\")\n",
    "capstone_experiment_name=\"capstone-experiment-{}\".format(create_date)\n",
    "capstone_run_name = \"lab-capstone-run-{}\".format(create_date)\n",
    "\n",
    "# define a run_tag\n",
    "run_tags = [{'Key': 'lab-capstone', 'Value': 'lab-capstone-run'}]\n",
    "\n",
    "# provide a description\n",
    "description=\"Using SM Experiments with the Auto dataset.\"\n",
    "\n",
    "print(f\"Experiment name - {capstone_experiment_name},  run name - {capstone_run_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Split the data into train, test, and validation datasets\n",
    "\n",
    "<a id=\"task3-2-continue\"></a>\n",
    "\n",
    "Use the features that you queried from SageMaker Feature Store and split the data into train, test, and validation datasets.\n",
    "\n",
    "**Hint 1**: Use **np.split** to split the dataset into three partitions.\n",
    "\n",
    "**Hint 2**: Use the **to_csv** to create CSV files and use **S3Uploader.upload** to add the files to Amazon S3.\n",
    "\n",
    "**Hint 3**: The final product of your split should be a **data_inputs** variable with values for **train** and **validation**.\n",
    "\n",
    "For detailed steps how to split data into train, test, and validation datasets, refer to <a href=\"#task3-2\" target=\"_self\">**Split data into train, test, and validation datasets (Task 3.2)**</a> in the *Appendix* section. \n",
    "\n",
    "After you have split the data into train, test, and validation datasets, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#train-validation-test-split\n",
    "df_processed_pre_split = df.copy()\n",
    "\n",
    "# Split the data into train, validation, and test datasets\n",
    "train_data, validation_data, test_data = np.split(\n",
    "    df_processed_pre_split.sample(frac=1, random_state=1729),\n",
    "    [int(0.7 * len(df_processed_pre_split)), int(0.9 * len(df_processed_pre_split))],\n",
    ")\n",
    "\n",
    "# Create the CSV files and upload them to your default bucket\n",
    "train_data.to_csv(\"train_data.csv\", index=False, header=False)\n",
    "validation_data.to_csv(\"validation_data.csv\", index=False, header=False)\n",
    "test_data.to_csv(\"test_data.csv\", index=False, header=False)\n",
    "\n",
    "train_path = S3Uploader.upload(\"train_data.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "validation_path = S3Uploader.upload(\"validation_data.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "test_path = S3Uploader.upload(\"test_data.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "\n",
    "# Set the training inputs\n",
    "train_input = TrainingInput(train_path, content_type=\"text/csv\")\n",
    "validation_input = TrainingInput(validation_path, content_type=\"text/csv\")\n",
    "test_input = TrainingInput(test_path, content_type=\"text/csv\")\n",
    "\n",
    "data_inputs = {\n",
    "    \"train\": train_input,\n",
    "    \"validation\": validation_input\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Configure and run a training job\n",
    "\n",
    "<a id=\"task3-3-continue\"></a>\n",
    "\n",
    "Start your first training job, setting up the container, an estimator, and the hyperparameters. Then, train the model with **fit()**. If you want to examine more detailed reports, enable SageMaker Debugger with **DebuggerHookConfig**.\n",
    "\n",
    "**Hint 1**: Use the **XGBoost** container with version **1.5-1**.\n",
    "\n",
    "**Hint 2**: To get started, set the hyperparameters for **eta**, **gamma**, **max_depth**, **min_child_weight**, **num_round**, **objective**, and **subsample**.\n",
    "\n",
    "**Hint 3**: Use the **data_inputs** that you created earlier in Challenge 1 as the **inputs** value for your training job.\n",
    "\n",
    "**Hint 4**: You can configure a training job by setting the inputs and experiment_config. The experiment_config should contain a **sagemaker_session**, a **run_name**, and a **experiment_name**.\n",
    "\n",
    "**Hint 5**: If you want to use SageMaker Debugger, configure the **DebuggerHookConfig**, the **ProfilerConfig**, and the Debugger **rule** object.\n",
    "\n",
    "Which hyperparameters are most likely to have a large impact on your model's performance and accuracy? Which hyperparameters do you plan on tuning first?\n",
    "\n",
    "For detailed steps how to configure and run a basic training job, refer to <a href=\"#task3-3-1\" target=\"_self\">**Configure and run a basic training job (Task 3.3)**</a> in the *Appendix* section.\n",
    "\n",
    "For detailed steps how to configure and run a training job with Debugger enabled and analyze reports, refer to <a href=\"#task3-3-2\" target=\"_self\">**Configure and run a training job with SageMaker Debugger enabled and analyze reports (Task 3.3)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have finished one or more training jobs, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-09-29-20-31-36-045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-29 20:31:36 Starting - Starting the training job...\n",
      "2024-09-29 20:31:52 Starting - Preparing the instances for training...\n",
      "2024-09-29 20:32:34 Downloading - Downloading the training image......\n",
      "2024-09-29 20:33:10 Training - Training image download completed. Training in progress...\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2024-09-29 20:33:40.628 ip-10-2-75-165.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-09-29 20:33:40.649 ip-10-2-75-165.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:40:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:40:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:40:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:40:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:40:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:40:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:41:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:41:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:41:INFO] Train matrix has 14000 rows and 50 columns\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:41:INFO] Validation matrix has 4000 rows\u001b[0m\n",
      "\u001b[34m[2024-09-29 20:33:41.064 ip-10-2-75-165.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-09-29 20:33:41.064 ip-10-2-75-165.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-09-29 20:33:41.065 ip-10-2-75-165.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-09-29 20:33:41.065 ip-10-2-75-165.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-09-29:20:33:41:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[20:33:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\u001b[0m\n",
      "\u001b[34m[2024-09-29 20:33:41.088 ip-10-2-75-165.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2024-09-29 20:33:41.091 ip-10-2-75-165.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[0]#011train-logloss:0.53441#011validation-logloss:0.53419\u001b[0m\n",
      "\u001b[34m[1]#011train-logloss:0.42822#011validation-logloss:0.42803\u001b[0m\n",
      "\u001b[34m[2]#011train-logloss:0.35315#011validation-logloss:0.35289\u001b[0m\n",
      "\u001b[34m[3]#011train-logloss:0.29837#011validation-logloss:0.29803\u001b[0m\n",
      "\u001b[34m[4]#011train-logloss:0.25737#011validation-logloss:0.25701\u001b[0m\n",
      "\u001b[34m[5]#011train-logloss:0.22616#011validation-logloss:0.22573\u001b[0m\n",
      "\u001b[34m[6]#011train-logloss:0.20236#011validation-logloss:0.20205\u001b[0m\n",
      "\u001b[34m[7]#011train-logloss:0.18420#011validation-logloss:0.18391\u001b[0m\n",
      "\u001b[34m[8]#011train-logloss:0.16992#011validation-logloss:0.17009\u001b[0m\n",
      "\u001b[34m[9]#011train-logloss:0.15881#011validation-logloss:0.15934\u001b[0m\n",
      "\u001b[34m[10]#011train-logloss:0.15019#011validation-logloss:0.15098\u001b[0m\n",
      "\u001b[34m[11]#011train-logloss:0.14360#011validation-logloss:0.14460\u001b[0m\n",
      "\u001b[34m[12]#011train-logloss:0.13834#011validation-logloss:0.13970\u001b[0m\n",
      "\u001b[34m[13]#011train-logloss:0.13425#011validation-logloss:0.13622\u001b[0m\n",
      "\u001b[34m[14]#011train-logloss:0.13117#011validation-logloss:0.13341\u001b[0m\n",
      "\u001b[34m[15]#011train-logloss:0.12868#011validation-logloss:0.13118\u001b[0m\n",
      "\u001b[34m[16]#011train-logloss:0.12601#011validation-logloss:0.12962\u001b[0m\n",
      "\u001b[34m[17]#011train-logloss:0.12419#011validation-logloss:0.12837\u001b[0m\n",
      "\u001b[34m[18]#011train-logloss:0.12267#011validation-logloss:0.12756\u001b[0m\n",
      "\u001b[34m[19]#011train-logloss:0.12146#011validation-logloss:0.12697\u001b[0m\n",
      "\u001b[34m[20]#011train-logloss:0.12029#011validation-logloss:0.12623\u001b[0m\n",
      "\u001b[34m[21]#011train-logloss:0.11901#011validation-logloss:0.12580\u001b[0m\n",
      "\u001b[34m[22]#011train-logloss:0.11759#011validation-logloss:0.12492\u001b[0m\n",
      "\u001b[34m[23]#011train-logloss:0.11668#011validation-logloss:0.12424\u001b[0m\n",
      "\u001b[34m[24]#011train-logloss:0.11561#011validation-logloss:0.12397\u001b[0m\n",
      "\u001b[34m[25]#011train-logloss:0.11393#011validation-logloss:0.12389\u001b[0m\n",
      "\u001b[34m[26]#011train-logloss:0.11335#011validation-logloss:0.12345\u001b[0m\n",
      "\u001b[34m[27]#011train-logloss:0.11288#011validation-logloss:0.12332\u001b[0m\n",
      "\u001b[34m[28]#011train-logloss:0.11250#011validation-logloss:0.12286\u001b[0m\n",
      "\u001b[34m[29]#011train-logloss:0.11139#011validation-logloss:0.12229\u001b[0m\n",
      "\u001b[34m[30]#011train-logloss:0.11035#011validation-logloss:0.12172\u001b[0m\n",
      "\u001b[34m[31]#011train-logloss:0.10930#011validation-logloss:0.12139\u001b[0m\n",
      "\u001b[34m[32]#011train-logloss:0.10888#011validation-logloss:0.12135\u001b[0m\n",
      "\u001b[34m[33]#011train-logloss:0.10844#011validation-logloss:0.12144\u001b[0m\n",
      "\u001b[34m[34]#011train-logloss:0.10751#011validation-logloss:0.12104\u001b[0m\n",
      "\u001b[34m[35]#011train-logloss:0.10643#011validation-logloss:0.12087\u001b[0m\n",
      "\u001b[34m[36]#011train-logloss:0.10600#011validation-logloss:0.12097\u001b[0m\n",
      "\u001b[34m[37]#011train-logloss:0.10532#011validation-logloss:0.12125\u001b[0m\n",
      "\u001b[34m[38]#011train-logloss:0.10471#011validation-logloss:0.12095\u001b[0m\n",
      "\u001b[34m[39]#011train-logloss:0.10379#011validation-logloss:0.12069\u001b[0m\n",
      "\u001b[34m[40]#011train-logloss:0.10329#011validation-logloss:0.12043\u001b[0m\n",
      "\u001b[34m[41]#011train-logloss:0.10254#011validation-logloss:0.12046\u001b[0m\n",
      "\u001b[34m[42]#011train-logloss:0.10187#011validation-logloss:0.12035\u001b[0m\n",
      "\u001b[34m[43]#011train-logloss:0.10123#011validation-logloss:0.12019\u001b[0m\n",
      "\u001b[34m[44]#011train-logloss:0.10093#011validation-logloss:0.11999\u001b[0m\n",
      "\u001b[34m[45]#011train-logloss:0.10063#011validation-logloss:0.12009\u001b[0m\n",
      "\u001b[34m[46]#011train-logloss:0.10004#011validation-logloss:0.12003\u001b[0m\n",
      "\u001b[34m[47]#011train-logloss:0.09970#011validation-logloss:0.11977\u001b[0m\n",
      "\u001b[34m[48]#011train-logloss:0.09943#011validation-logloss:0.12000\u001b[0m\n",
      "\u001b[34m[49]#011train-logloss:0.09920#011validation-logloss:0.11994\u001b[0m\n",
      "\u001b[34m[50]#011train-logloss:0.09907#011validation-logloss:0.11994\u001b[0m\n",
      "\u001b[34m[51]#011train-logloss:0.09886#011validation-logloss:0.11990\u001b[0m\n",
      "\u001b[34m[52]#011train-logloss:0.09859#011validation-logloss:0.11979\u001b[0m\n",
      "\u001b[34m[53]#011train-logloss:0.09807#011validation-logloss:0.11951\u001b[0m\n",
      "\u001b[34m[54]#011train-logloss:0.09756#011validation-logloss:0.11937\u001b[0m\n",
      "\u001b[34m[55]#011train-logloss:0.09704#011validation-logloss:0.11927\u001b[0m\n",
      "\u001b[34m[56]#011train-logloss:0.09662#011validation-logloss:0.11912\u001b[0m\n",
      "\u001b[34m[57]#011train-logloss:0.09642#011validation-logloss:0.11896\u001b[0m\n",
      "\u001b[34m[58]#011train-logloss:0.09574#011validation-logloss:0.11866\u001b[0m\n",
      "\u001b[34m[59]#011train-logloss:0.09519#011validation-logloss:0.11887\u001b[0m\n",
      "\u001b[34m[60]#011train-logloss:0.09460#011validation-logloss:0.11850\u001b[0m\n",
      "\u001b[34m[61]#011train-logloss:0.09387#011validation-logloss:0.11837\u001b[0m\n",
      "\u001b[34m[62]#011train-logloss:0.09387#011validation-logloss:0.11837\u001b[0m\n",
      "\u001b[34m[63]#011train-logloss:0.09348#011validation-logloss:0.11821\u001b[0m\n",
      "\u001b[34m[64]#011train-logloss:0.09310#011validation-logloss:0.11828\u001b[0m\n",
      "\u001b[34m[65]#011train-logloss:0.09238#011validation-logloss:0.11793\u001b[0m\n",
      "\u001b[34m[66]#011train-logloss:0.09217#011validation-logloss:0.11790\u001b[0m\n",
      "\u001b[34m[67]#011train-logloss:0.09160#011validation-logloss:0.11780\u001b[0m\n",
      "\u001b[34m[68]#011train-logloss:0.09095#011validation-logloss:0.11757\u001b[0m\n",
      "\u001b[34m[69]#011train-logloss:0.09080#011validation-logloss:0.11758\u001b[0m\n",
      "\u001b[34m[70]#011train-logloss:0.08969#011validation-logloss:0.11699\u001b[0m\n",
      "\u001b[34m[71]#011train-logloss:0.08953#011validation-logloss:0.11711\u001b[0m\n",
      "\u001b[34m[72]#011train-logloss:0.08909#011validation-logloss:0.11683\u001b[0m\n",
      "\u001b[34m[73]#011train-logloss:0.08868#011validation-logloss:0.11674\u001b[0m\n",
      "\u001b[34m[74]#011train-logloss:0.08796#011validation-logloss:0.11614\u001b[0m\n",
      "\u001b[34m[75]#011train-logloss:0.08771#011validation-logloss:0.11614\u001b[0m\n",
      "\u001b[34m[76]#011train-logloss:0.08719#011validation-logloss:0.11629\u001b[0m\n",
      "\u001b[34m[77]#011train-logloss:0.08670#011validation-logloss:0.11653\u001b[0m\n",
      "\u001b[34m[78]#011train-logloss:0.08603#011validation-logloss:0.11642\u001b[0m\n",
      "\u001b[34m[79]#011train-logloss:0.08586#011validation-logloss:0.11651\u001b[0m\n",
      "\u001b[34m[80]#011train-logloss:0.08586#011validation-logloss:0.11651\u001b[0m\n",
      "\u001b[34m[81]#011train-logloss:0.08515#011validation-logloss:0.11643\u001b[0m\n",
      "\u001b[34m[82]#011train-logloss:0.08493#011validation-logloss:0.11625\u001b[0m\n",
      "\u001b[34m[83]#011train-logloss:0.08462#011validation-logloss:0.11607\u001b[0m\n",
      "\u001b[34m[84]#011train-logloss:0.08454#011validation-logloss:0.11622\u001b[0m\n",
      "\u001b[34m[85]#011train-logloss:0.08394#011validation-logloss:0.11607\u001b[0m\n",
      "\u001b[34m[86]#011train-logloss:0.08380#011validation-logloss:0.11593\u001b[0m\n",
      "\u001b[34m[87]#011train-logloss:0.08341#011validation-logloss:0.11582\u001b[0m\n",
      "\u001b[34m[88]#011train-logloss:0.08316#011validation-logloss:0.11567\u001b[0m\n",
      "\u001b[34m[89]#011train-logloss:0.08282#011validation-logloss:0.11572\u001b[0m\n",
      "\u001b[34m[90]#011train-logloss:0.08223#011validation-logloss:0.11520\u001b[0m\n",
      "\u001b[34m[91]#011train-logloss:0.08181#011validation-logloss:0.11508\u001b[0m\n",
      "\u001b[34m[92]#011train-logloss:0.08149#011validation-logloss:0.11502\u001b[0m\n",
      "\u001b[34m[93]#011train-logloss:0.08129#011validation-logloss:0.11501\u001b[0m\n",
      "\u001b[34m[94]#011train-logloss:0.08105#011validation-logloss:0.11514\u001b[0m\n",
      "\u001b[34m[95]#011train-logloss:0.08072#011validation-logloss:0.11494\u001b[0m\n",
      "\u001b[34m[96]#011train-logloss:0.08018#011validation-logloss:0.11479\u001b[0m\n",
      "\u001b[34m[97]#011train-logloss:0.07978#011validation-logloss:0.11472\u001b[0m\n",
      "\u001b[34m[98]#011train-logloss:0.07928#011validation-logloss:0.11422\u001b[0m\n",
      "\u001b[34m[99]#011train-logloss:0.07899#011validation-logloss:0.11398\u001b[0m\n",
      "\u001b[34m[100]#011train-logloss:0.07860#011validation-logloss:0.11388\u001b[0m\n",
      "\u001b[34m[101]#011train-logloss:0.07792#011validation-logloss:0.11373\u001b[0m\n",
      "\u001b[34m[102]#011train-logloss:0.07769#011validation-logloss:0.11350\u001b[0m\n",
      "\u001b[34m[103]#011train-logloss:0.07769#011validation-logloss:0.11351\u001b[0m\n",
      "\u001b[34m[104]#011train-logloss:0.07763#011validation-logloss:0.11351\u001b[0m\n",
      "\u001b[34m[105]#011train-logloss:0.07696#011validation-logloss:0.11330\u001b[0m\n",
      "\u001b[34m[106]#011train-logloss:0.07646#011validation-logloss:0.11340\u001b[0m\n",
      "\u001b[34m[107]#011train-logloss:0.07603#011validation-logloss:0.11334\u001b[0m\n",
      "\u001b[34m[108]#011train-logloss:0.07587#011validation-logloss:0.11328\u001b[0m\n",
      "\u001b[34m[109]#011train-logloss:0.07545#011validation-logloss:0.11349\u001b[0m\n",
      "\u001b[34m[110]#011train-logloss:0.07533#011validation-logloss:0.11357\u001b[0m\n",
      "\u001b[34m[111]#011train-logloss:0.07479#011validation-logloss:0.11322\u001b[0m\n",
      "\u001b[34m[112]#011train-logloss:0.07442#011validation-logloss:0.11335\u001b[0m\n",
      "\u001b[34m[113]#011train-logloss:0.07380#011validation-logloss:0.11320\u001b[0m\n",
      "\u001b[34m[114]#011train-logloss:0.07348#011validation-logloss:0.11258\u001b[0m\n",
      "\u001b[34m[115]#011train-logloss:0.07276#011validation-logloss:0.11238\u001b[0m\n",
      "\u001b[34m[116]#011train-logloss:0.07225#011validation-logloss:0.11231\u001b[0m\n",
      "\u001b[34m[117]#011train-logloss:0.07190#011validation-logloss:0.11201\u001b[0m\n",
      "\u001b[34m[118]#011train-logloss:0.07155#011validation-logloss:0.11209\u001b[0m\n",
      "\u001b[34m[119]#011train-logloss:0.07154#011validation-logloss:0.11209\u001b[0m\n",
      "\u001b[34m[120]#011train-logloss:0.07144#011validation-logloss:0.11206\u001b[0m\n",
      "\u001b[34m[121]#011train-logloss:0.07129#011validation-logloss:0.11194\u001b[0m\n",
      "\u001b[34m[122]#011train-logloss:0.07129#011validation-logloss:0.11194\u001b[0m\n",
      "\u001b[34m[123]#011train-logloss:0.07118#011validation-logloss:0.11189\u001b[0m\n",
      "\u001b[34m[124]#011train-logloss:0.07059#011validation-logloss:0.11148\u001b[0m\n",
      "\u001b[34m[125]#011train-logloss:0.07002#011validation-logloss:0.11146\u001b[0m\n",
      "\u001b[34m[126]#011train-logloss:0.06973#011validation-logloss:0.11110\u001b[0m\n",
      "\u001b[34m[127]#011train-logloss:0.06960#011validation-logloss:0.11110\u001b[0m\n",
      "\u001b[34m[128]#011train-logloss:0.06946#011validation-logloss:0.11103\u001b[0m\n",
      "\u001b[34m[129]#011train-logloss:0.06937#011validation-logloss:0.11105\u001b[0m\n",
      "\u001b[34m[130]#011train-logloss:0.06881#011validation-logloss:0.11098\u001b[0m\n",
      "\u001b[34m[131]#011train-logloss:0.06835#011validation-logloss:0.11064\u001b[0m\n",
      "\u001b[34m[132]#011train-logloss:0.06805#011validation-logloss:0.11073\u001b[0m\n",
      "\u001b[34m[133]#011train-logloss:0.06792#011validation-logloss:0.11068\u001b[0m\n",
      "\u001b[34m[134]#011train-logloss:0.06781#011validation-logloss:0.11066\u001b[0m\n",
      "\u001b[34m[135]#011train-logloss:0.06764#011validation-logloss:0.11043\u001b[0m\n",
      "\u001b[34m[136]#011train-logloss:0.06743#011validation-logloss:0.11023\u001b[0m\n",
      "\u001b[34m[137]#011train-logloss:0.06714#011validation-logloss:0.11056\u001b[0m\n",
      "\u001b[34m[138]#011train-logloss:0.06705#011validation-logloss:0.11052\u001b[0m\n",
      "\u001b[34m[139]#011train-logloss:0.06705#011validation-logloss:0.11053\u001b[0m\n",
      "\u001b[34m[140]#011train-logloss:0.06705#011validation-logloss:0.11054\u001b[0m\n",
      "\u001b[34m[141]#011train-logloss:0.06705#011validation-logloss:0.11053\u001b[0m\n",
      "\u001b[34m[142]#011train-logloss:0.06705#011validation-logloss:0.11054\u001b[0m\n",
      "\u001b[34m[143]#011train-logloss:0.06695#011validation-logloss:0.11039\u001b[0m\n",
      "\u001b[34m[144]#011train-logloss:0.06657#011validation-logloss:0.11012\u001b[0m\n",
      "\u001b[34m[145]#011train-logloss:0.06657#011validation-logloss:0.11015\u001b[0m\n",
      "\u001b[34m[146]#011train-logloss:0.06636#011validation-logloss:0.11021\u001b[0m\n",
      "\u001b[34m[147]#011train-logloss:0.06627#011validation-logloss:0.11029\u001b[0m\n",
      "\u001b[34m[148]#011train-logloss:0.06616#011validation-logloss:0.11034\u001b[0m\n",
      "\u001b[34m[149]#011train-logloss:0.06604#011validation-logloss:0.11028\u001b[0m\n",
      "\u001b[34m[150]#011train-logloss:0.06604#011validation-logloss:0.11029\u001b[0m\n",
      "\u001b[34m[151]#011train-logloss:0.06604#011validation-logloss:0.11030\u001b[0m\n",
      "\u001b[34m[152]#011train-logloss:0.06588#011validation-logloss:0.11044\u001b[0m\n",
      "\u001b[34m[153]#011train-logloss:0.06566#011validation-logloss:0.11025\u001b[0m\n",
      "\u001b[34m[154]#011train-logloss:0.06543#011validation-logloss:0.11019\u001b[0m\n",
      "\u001b[34m[155]#011train-logloss:0.06531#011validation-logloss:0.11011\u001b[0m\n",
      "\u001b[34m[156]#011train-logloss:0.06531#011validation-logloss:0.11010\u001b[0m\n",
      "\u001b[34m[157]#011train-logloss:0.06512#011validation-logloss:0.11000\u001b[0m\n",
      "\u001b[34m[158]#011train-logloss:0.06457#011validation-logloss:0.11029\u001b[0m\n",
      "\u001b[34m[159]#011train-logloss:0.06434#011validation-logloss:0.11030\u001b[0m\n",
      "\u001b[34m[160]#011train-logloss:0.06413#011validation-logloss:0.11006\u001b[0m\n",
      "\u001b[34m[161]#011train-logloss:0.06397#011validation-logloss:0.11017\u001b[0m\n",
      "\u001b[34m[162]#011train-logloss:0.06363#011validation-logloss:0.11018\u001b[0m\n",
      "\u001b[34m[163]#011train-logloss:0.06352#011validation-logloss:0.11003\u001b[0m\n",
      "\u001b[34m[164]#011train-logloss:0.06313#011validation-logloss:0.10958\u001b[0m\n",
      "\u001b[34m[165]#011train-logloss:0.06277#011validation-logloss:0.10971\u001b[0m\n",
      "\u001b[34m[166]#011train-logloss:0.06267#011validation-logloss:0.10975\u001b[0m\n",
      "\u001b[34m[167]#011train-logloss:0.06248#011validation-logloss:0.10982\u001b[0m\n",
      "\u001b[34m[168]#011train-logloss:0.06232#011validation-logloss:0.10970\u001b[0m\n",
      "\u001b[34m[169]#011train-logloss:0.06216#011validation-logloss:0.10976\u001b[0m\n",
      "\u001b[34m[170]#011train-logloss:0.06216#011validation-logloss:0.10977\u001b[0m\n",
      "\u001b[34m[171]#011train-logloss:0.06216#011validation-logloss:0.10977\u001b[0m\n",
      "\u001b[34m[172]#011train-logloss:0.06193#011validation-logloss:0.11007\u001b[0m\n",
      "\u001b[34m[173]#011train-logloss:0.06193#011validation-logloss:0.11007\u001b[0m\n",
      "\u001b[34m[174]#011train-logloss:0.06148#011validation-logloss:0.10965\u001b[0m\n",
      "\u001b[34m[175]#011train-logloss:0.06133#011validation-logloss:0.10971\u001b[0m\n",
      "\u001b[34m[176]#011train-logloss:0.06133#011validation-logloss:0.10972\u001b[0m\n",
      "\u001b[34m[177]#011train-logloss:0.06133#011validation-logloss:0.10971\u001b[0m\n",
      "\u001b[34m[178]#011train-logloss:0.06133#011validation-logloss:0.10973\u001b[0m\n",
      "\u001b[34m[179]#011train-logloss:0.06123#011validation-logloss:0.10960\u001b[0m\n",
      "\u001b[34m[180]#011train-logloss:0.06080#011validation-logloss:0.10969\u001b[0m\n",
      "\u001b[34m[181]#011train-logloss:0.06060#011validation-logloss:0.10949\u001b[0m\n",
      "\u001b[34m[182]#011train-logloss:0.06035#011validation-logloss:0.10968\u001b[0m\n",
      "\u001b[34m[183]#011train-logloss:0.06024#011validation-logloss:0.10971\u001b[0m\n",
      "\u001b[34m[184]#011train-logloss:0.06024#011validation-logloss:0.10970\u001b[0m\n",
      "\u001b[34m[185]#011train-logloss:0.06024#011validation-logloss:0.10968\u001b[0m\n",
      "\u001b[34m[186]#011train-logloss:0.06000#011validation-logloss:0.10986\u001b[0m\n",
      "\u001b[34m[187]#011train-logloss:0.05994#011validation-logloss:0.10988\u001b[0m\n",
      "\u001b[34m[188]#011train-logloss:0.05994#011validation-logloss:0.10988\u001b[0m\n",
      "\u001b[34m[189]#011train-logloss:0.05980#011validation-logloss:0.10979\u001b[0m\n",
      "\u001b[34m[190]#011train-logloss:0.05970#011validation-logloss:0.10951\u001b[0m\n",
      "\u001b[34m[191]#011train-logloss:0.05970#011validation-logloss:0.10948\u001b[0m\n",
      "\u001b[34m[192]#011train-logloss:0.05957#011validation-logloss:0.10960\u001b[0m\n",
      "\u001b[34m[193]#011train-logloss:0.05957#011validation-logloss:0.10959\u001b[0m\n",
      "\u001b[34m[194]#011train-logloss:0.05941#011validation-logloss:0.10940\u001b[0m\n",
      "\u001b[34m[195]#011train-logloss:0.05926#011validation-logloss:0.10926\u001b[0m\n",
      "\u001b[34m[196]#011train-logloss:0.05918#011validation-logloss:0.10914\u001b[0m\n",
      "\u001b[34m[197]#011train-logloss:0.05917#011validation-logloss:0.10915\u001b[0m\n",
      "\u001b[34m[198]#011train-logloss:0.05908#011validation-logloss:0.10910\u001b[0m\n",
      "\u001b[34m[199]#011train-logloss:0.05894#011validation-logloss:0.10918\u001b[0m\n",
      "\u001b[34m[200]#011train-logloss:0.05894#011validation-logloss:0.10920\u001b[0m\n",
      "\u001b[34m[201]#011train-logloss:0.05890#011validation-logloss:0.10916\u001b[0m\n",
      "\u001b[34m[202]#011train-logloss:0.05875#011validation-logloss:0.10911\u001b[0m\n",
      "\u001b[34m[203]#011train-logloss:0.05863#011validation-logloss:0.10938\u001b[0m\n",
      "\u001b[34m[204]#011train-logloss:0.05853#011validation-logloss:0.10930\u001b[0m\n",
      "\u001b[34m[205]#011train-logloss:0.05853#011validation-logloss:0.10931\u001b[0m\n",
      "\u001b[34m[206]#011train-logloss:0.05839#011validation-logloss:0.10949\u001b[0m\n",
      "\u001b[34m[207]#011train-logloss:0.05839#011validation-logloss:0.10948\u001b[0m\n",
      "\u001b[34m[208]#011train-logloss:0.05839#011validation-logloss:0.10947\u001b[0m\n",
      "\u001b[34m[209]#011train-logloss:0.05808#011validation-logloss:0.10874\u001b[0m\n",
      "\u001b[34m[210]#011train-logloss:0.05808#011validation-logloss:0.10874\u001b[0m\n",
      "\u001b[34m[211]#011train-logloss:0.05808#011validation-logloss:0.10874\u001b[0m\n",
      "\u001b[34m[212]#011train-logloss:0.05798#011validation-logloss:0.10882\u001b[0m\n",
      "\u001b[34m[213]#011train-logloss:0.05798#011validation-logloss:0.10884\u001b[0m\n",
      "\u001b[34m[214]#011train-logloss:0.05762#011validation-logloss:0.10894\u001b[0m\n",
      "\u001b[34m[215]#011train-logloss:0.05761#011validation-logloss:0.10892\u001b[0m\n",
      "\u001b[34m[216]#011train-logloss:0.05758#011validation-logloss:0.10891\u001b[0m\n",
      "\u001b[34m[217]#011train-logloss:0.05758#011validation-logloss:0.10891\u001b[0m\n",
      "\u001b[34m[218]#011train-logloss:0.05745#011validation-logloss:0.10895\u001b[0m\n",
      "\u001b[34m[219]#011train-logloss:0.05740#011validation-logloss:0.10894\u001b[0m\n",
      "\u001b[34m[220]#011train-logloss:0.05728#011validation-logloss:0.10884\u001b[0m\n",
      "\u001b[34m[221]#011train-logloss:0.05680#011validation-logloss:0.10851\u001b[0m\n",
      "\u001b[34m[222]#011train-logloss:0.05672#011validation-logloss:0.10864\u001b[0m\n",
      "\u001b[34m[223]#011train-logloss:0.05661#011validation-logloss:0.10868\u001b[0m\n",
      "\u001b[34m[224]#011train-logloss:0.05641#011validation-logloss:0.10871\u001b[0m\n",
      "\u001b[34m[225]#011train-logloss:0.05641#011validation-logloss:0.10869\u001b[0m\n",
      "\u001b[34m[226]#011train-logloss:0.05630#011validation-logloss:0.10850\u001b[0m\n",
      "\u001b[34m[227]#011train-logloss:0.05614#011validation-logloss:0.10867\u001b[0m\n",
      "\u001b[34m[228]#011train-logloss:0.05614#011validation-logloss:0.10869\u001b[0m\n",
      "\u001b[34m[229]#011train-logloss:0.05614#011validation-logloss:0.10867\u001b[0m\n",
      "\u001b[34m[230]#011train-logloss:0.05602#011validation-logloss:0.10861\u001b[0m\n",
      "\u001b[34m[231]#011train-logloss:0.05602#011validation-logloss:0.10862\u001b[0m\n",
      "\u001b[34m[232]#011train-logloss:0.05596#011validation-logloss:0.10881\u001b[0m\n",
      "\u001b[34m[233]#011train-logloss:0.05585#011validation-logloss:0.10870\u001b[0m\n",
      "\u001b[34m[234]#011train-logloss:0.05585#011validation-logloss:0.10872\u001b[0m\n",
      "\u001b[34m[235]#011train-logloss:0.05585#011validation-logloss:0.10872\u001b[0m\n",
      "\u001b[34m[236]#011train-logloss:0.05586#011validation-logloss:0.10873\u001b[0m\n",
      "\u001b[34m[237]#011train-logloss:0.05580#011validation-logloss:0.10857\u001b[0m\n",
      "\u001b[34m[238]#011train-logloss:0.05580#011validation-logloss:0.10856\u001b[0m\n",
      "\u001b[34m[239]#011train-logloss:0.05581#011validation-logloss:0.10858\u001b[0m\n",
      "\u001b[34m[240]#011train-logloss:0.05554#011validation-logloss:0.10834\u001b[0m\n",
      "\u001b[34m[241]#011train-logloss:0.05543#011validation-logloss:0.10817\u001b[0m\n",
      "\u001b[34m[242]#011train-logloss:0.05542#011validation-logloss:0.10816\u001b[0m\n",
      "\u001b[34m[243]#011train-logloss:0.05511#011validation-logloss:0.10779\u001b[0m\n",
      "\u001b[34m[244]#011train-logloss:0.05479#011validation-logloss:0.10790\u001b[0m\n",
      "\u001b[34m[245]#011train-logloss:0.05479#011validation-logloss:0.10789\u001b[0m\n",
      "\u001b[34m[246]#011train-logloss:0.05464#011validation-logloss:0.10784\u001b[0m\n",
      "\u001b[34m[247]#011train-logloss:0.05445#011validation-logloss:0.10810\u001b[0m\n",
      "\u001b[34m[248]#011train-logloss:0.05445#011validation-logloss:0.10812\u001b[0m\n",
      "\u001b[34m[249]#011train-logloss:0.05439#011validation-logloss:0.10790\u001b[0m\n",
      "\u001b[34m[250]#011train-logloss:0.05439#011validation-logloss:0.10789\u001b[0m\n",
      "\u001b[34m[251]#011train-logloss:0.05411#011validation-logloss:0.10799\u001b[0m\n",
      "\u001b[34m[252]#011train-logloss:0.05376#011validation-logloss:0.10788\u001b[0m\n",
      "\u001b[34m[253]#011train-logloss:0.05376#011validation-logloss:0.10793\u001b[0m\n",
      "\u001b[34m[254]#011train-logloss:0.05377#011validation-logloss:0.10795\u001b[0m\n",
      "\u001b[34m[255]#011train-logloss:0.05366#011validation-logloss:0.10814\u001b[0m\n",
      "\u001b[34m[256]#011train-logloss:0.05325#011validation-logloss:0.10813\u001b[0m\n",
      "\u001b[34m[257]#011train-logloss:0.05315#011validation-logloss:0.10784\u001b[0m\n",
      "\u001b[34m[258]#011train-logloss:0.05315#011validation-logloss:0.10784\u001b[0m\n",
      "\u001b[34m[259]#011train-logloss:0.05298#011validation-logloss:0.10801\u001b[0m\n",
      "\u001b[34m[260]#011train-logloss:0.05298#011validation-logloss:0.10801\u001b[0m\n",
      "\u001b[34m[261]#011train-logloss:0.05298#011validation-logloss:0.10800\u001b[0m\n",
      "\u001b[34m[262]#011train-logloss:0.05287#011validation-logloss:0.10792\u001b[0m\n",
      "\u001b[34m[263]#011train-logloss:0.05239#011validation-logloss:0.10742\u001b[0m\n",
      "\u001b[34m[264]#011train-logloss:0.05239#011validation-logloss:0.10743\u001b[0m\n",
      "\u001b[34m[265]#011train-logloss:0.05229#011validation-logloss:0.10739\u001b[0m\n",
      "\u001b[34m[266]#011train-logloss:0.05204#011validation-logloss:0.10711\u001b[0m\n",
      "\u001b[34m[267]#011train-logloss:0.05204#011validation-logloss:0.10711\u001b[0m\n",
      "\u001b[34m[268]#011train-logloss:0.05189#011validation-logloss:0.10708\u001b[0m\n",
      "\u001b[34m[269]#011train-logloss:0.05189#011validation-logloss:0.10712\u001b[0m\n",
      "\u001b[34m[270]#011train-logloss:0.05180#011validation-logloss:0.10703\u001b[0m\n",
      "\u001b[34m[271]#011train-logloss:0.05180#011validation-logloss:0.10704\u001b[0m\n",
      "\u001b[34m[272]#011train-logloss:0.05150#011validation-logloss:0.10660\u001b[0m\n",
      "\u001b[34m[273]#011train-logloss:0.05139#011validation-logloss:0.10679\u001b[0m\n",
      "\u001b[34m[274]#011train-logloss:0.05123#011validation-logloss:0.10673\u001b[0m\n",
      "\u001b[34m[275]#011train-logloss:0.05123#011validation-logloss:0.10670\u001b[0m\n",
      "\u001b[34m[276]#011train-logloss:0.05123#011validation-logloss:0.10671\u001b[0m\n",
      "\u001b[34m[277]#011train-logloss:0.05101#011validation-logloss:0.10695\u001b[0m\n",
      "\u001b[34m[278]#011train-logloss:0.05101#011validation-logloss:0.10695\u001b[0m\n",
      "\u001b[34m[279]#011train-logloss:0.05101#011validation-logloss:0.10696\u001b[0m\n",
      "\u001b[34m[280]#011train-logloss:0.05087#011validation-logloss:0.10696\u001b[0m\n",
      "\u001b[34m[281]#011train-logloss:0.05087#011validation-logloss:0.10695\u001b[0m\n",
      "\u001b[34m[282]#011train-logloss:0.05082#011validation-logloss:0.10685\u001b[0m\n",
      "\u001b[34m[283]#011train-logloss:0.05082#011validation-logloss:0.10685\u001b[0m\n",
      "\u001b[34m[284]#011train-logloss:0.05082#011validation-logloss:0.10688\u001b[0m\n",
      "\u001b[34m[285]#011train-logloss:0.05074#011validation-logloss:0.10678\u001b[0m\n",
      "\u001b[34m[286]#011train-logloss:0.05074#011validation-logloss:0.10677\u001b[0m\n",
      "\u001b[34m[287]#011train-logloss:0.05060#011validation-logloss:0.10679\u001b[0m\n",
      "\u001b[34m[288]#011train-logloss:0.05060#011validation-logloss:0.10676\u001b[0m\n",
      "\u001b[34m[289]#011train-logloss:0.05060#011validation-logloss:0.10679\u001b[0m\n",
      "\u001b[34m[290]#011train-logloss:0.05045#011validation-logloss:0.10677\u001b[0m\n",
      "\u001b[34m[291]#011train-logloss:0.05036#011validation-logloss:0.10669\u001b[0m\n",
      "\u001b[34m[292]#011train-logloss:0.05036#011validation-logloss:0.10669\u001b[0m\n",
      "\u001b[34m[293]#011train-logloss:0.05036#011validation-logloss:0.10671\u001b[0m\n",
      "\u001b[34m[294]#011train-logloss:0.05036#011validation-logloss:0.10672\u001b[0m\n",
      "\u001b[34m[295]#011train-logloss:0.05017#011validation-logloss:0.10687\u001b[0m\n",
      "\u001b[34m[296]#011train-logloss:0.05017#011validation-logloss:0.10684\u001b[0m\n",
      "\u001b[34m[297]#011train-logloss:0.05017#011validation-logloss:0.10686\u001b[0m\n",
      "\u001b[34m[298]#011train-logloss:0.05005#011validation-logloss:0.10671\u001b[0m\n",
      "\u001b[34m[299]#011train-logloss:0.05005#011validation-logloss:0.10672\u001b[0m\n",
      "\u001b[34m[300]#011train-logloss:0.05005#011validation-logloss:0.10675\u001b[0m\n",
      "\u001b[34m[301]#011train-logloss:0.05005#011validation-logloss:0.10677\u001b[0m\n",
      "\u001b[34m[302]#011train-logloss:0.05005#011validation-logloss:0.10679\u001b[0m\n",
      "\u001b[34m[303]#011train-logloss:0.05005#011validation-logloss:0.10675\u001b[0m\n",
      "\u001b[34m[304]#011train-logloss:0.05005#011validation-logloss:0.10675\u001b[0m\n",
      "\u001b[34m[305]#011train-logloss:0.04990#011validation-logloss:0.10661\u001b[0m\n",
      "\u001b[34m[306]#011train-logloss:0.04990#011validation-logloss:0.10660\u001b[0m\n",
      "\u001b[34m[307]#011train-logloss:0.04990#011validation-logloss:0.10662\u001b[0m\n",
      "\u001b[34m[308]#011train-logloss:0.04990#011validation-logloss:0.10661\u001b[0m\n",
      "\u001b[34m[309]#011train-logloss:0.04990#011validation-logloss:0.10662\u001b[0m\n",
      "\u001b[34m[310]#011train-logloss:0.04990#011validation-logloss:0.10663\u001b[0m\n",
      "\u001b[34m[311]#011train-logloss:0.04978#011validation-logloss:0.10671\u001b[0m\n",
      "\u001b[34m[312]#011train-logloss:0.04965#011validation-logloss:0.10666\u001b[0m\n",
      "\u001b[34m[313]#011train-logloss:0.04952#011validation-logloss:0.10663\u001b[0m\n",
      "\u001b[34m[314]#011train-logloss:0.04952#011validation-logloss:0.10661\u001b[0m\n",
      "\u001b[34m[315]#011train-logloss:0.04929#011validation-logloss:0.10653\u001b[0m\n",
      "\u001b[34m[316]#011train-logloss:0.04929#011validation-logloss:0.10652\u001b[0m\n",
      "\u001b[34m[317]#011train-logloss:0.04929#011validation-logloss:0.10651\u001b[0m\n",
      "\u001b[34m[318]#011train-logloss:0.04929#011validation-logloss:0.10652\u001b[0m\n",
      "\u001b[34m[319]#011train-logloss:0.04929#011validation-logloss:0.10652\u001b[0m\n",
      "\u001b[34m[320]#011train-logloss:0.04919#011validation-logloss:0.10659\u001b[0m\n",
      "\u001b[34m[321]#011train-logloss:0.04919#011validation-logloss:0.10659\u001b[0m\n",
      "\u001b[34m[322]#011train-logloss:0.04919#011validation-logloss:0.10658\u001b[0m\n",
      "\u001b[34m[323]#011train-logloss:0.04919#011validation-logloss:0.10658\u001b[0m\n",
      "\u001b[34m[324]#011train-logloss:0.04916#011validation-logloss:0.10663\u001b[0m\n",
      "\u001b[34m[325]#011train-logloss:0.04910#011validation-logloss:0.10669\u001b[0m\n",
      "\u001b[34m[326]#011train-logloss:0.04910#011validation-logloss:0.10669\u001b[0m\n",
      "\u001b[34m[327]#011train-logloss:0.04901#011validation-logloss:0.10649\u001b[0m\n",
      "\u001b[34m[328]#011train-logloss:0.04901#011validation-logloss:0.10652\u001b[0m\n",
      "\u001b[34m[329]#011train-logloss:0.04901#011validation-logloss:0.10653\u001b[0m\n",
      "\u001b[34m[330]#011train-logloss:0.04890#011validation-logloss:0.10636\u001b[0m\n",
      "\u001b[34m[331]#011train-logloss:0.04890#011validation-logloss:0.10639\u001b[0m\n",
      "\u001b[34m[332]#011train-logloss:0.04890#011validation-logloss:0.10639\u001b[0m\n",
      "\u001b[34m[333]#011train-logloss:0.04890#011validation-logloss:0.10639\u001b[0m\n",
      "\u001b[34m[334]#011train-logloss:0.04862#011validation-logloss:0.10630\u001b[0m\n",
      "\u001b[34m[335]#011train-logloss:0.04862#011validation-logloss:0.10627\u001b[0m\n",
      "\u001b[34m[336]#011train-logloss:0.04850#011validation-logloss:0.10639\u001b[0m\n",
      "\u001b[34m[337]#011train-logloss:0.04850#011validation-logloss:0.10639\u001b[0m\n",
      "\u001b[34m[338]#011train-logloss:0.04850#011validation-logloss:0.10637\u001b[0m\n",
      "\u001b[34m[339]#011train-logloss:0.04850#011validation-logloss:0.10638\u001b[0m\n",
      "\u001b[34m[340]#011train-logloss:0.04850#011validation-logloss:0.10637\u001b[0m\n",
      "\u001b[34m[341]#011train-logloss:0.04850#011validation-logloss:0.10635\u001b[0m\n",
      "\u001b[34m[342]#011train-logloss:0.04850#011validation-logloss:0.10638\u001b[0m\n",
      "\u001b[34m[343]#011train-logloss:0.04841#011validation-logloss:0.10631\u001b[0m\n",
      "\u001b[34m[344]#011train-logloss:0.04841#011validation-logloss:0.10631\u001b[0m\n",
      "\u001b[34m[345]#011train-logloss:0.04841#011validation-logloss:0.10634\u001b[0m\n",
      "\u001b[34m[346]#011train-logloss:0.04841#011validation-logloss:0.10631\u001b[0m\n",
      "\u001b[34m[347]#011train-logloss:0.04830#011validation-logloss:0.10628\u001b[0m\n",
      "\u001b[34m[348]#011train-logloss:0.04830#011validation-logloss:0.10624\u001b[0m\n",
      "\u001b[34m[349]#011train-logloss:0.04830#011validation-logloss:0.10625\u001b[0m\n",
      "\u001b[34m[350]#011train-logloss:0.04830#011validation-logloss:0.10623\u001b[0m\n",
      "\u001b[34m[351]#011train-logloss:0.04831#011validation-logloss:0.10620\u001b[0m\n",
      "\u001b[34m[352]#011train-logloss:0.04831#011validation-logloss:0.10620\u001b[0m\n",
      "\u001b[34m[353]#011train-logloss:0.04821#011validation-logloss:0.10630\u001b[0m\n",
      "\u001b[34m[354]#011train-logloss:0.04821#011validation-logloss:0.10631\u001b[0m\n",
      "\u001b[34m[355]#011train-logloss:0.04821#011validation-logloss:0.10630\u001b[0m\n",
      "\u001b[34m[356]#011train-logloss:0.04814#011validation-logloss:0.10651\u001b[0m\n",
      "\u001b[34m[357]#011train-logloss:0.04814#011validation-logloss:0.10650\u001b[0m\n",
      "\u001b[34m[358]#011train-logloss:0.04814#011validation-logloss:0.10649\u001b[0m\n",
      "\u001b[34m[359]#011train-logloss:0.04814#011validation-logloss:0.10647\u001b[0m\n",
      "\u001b[34m[360]#011train-logloss:0.04807#011validation-logloss:0.10637\u001b[0m\n",
      "\u001b[34m[361]#011train-logloss:0.04807#011validation-logloss:0.10642\u001b[0m\n",
      "\u001b[34m[362]#011train-logloss:0.04777#011validation-logloss:0.10616\u001b[0m\n",
      "\u001b[34m[363]#011train-logloss:0.04777#011validation-logloss:0.10616\u001b[0m\n",
      "\u001b[34m[364]#011train-logloss:0.04758#011validation-logloss:0.10625\u001b[0m\n",
      "\u001b[34m[365]#011train-logloss:0.04758#011validation-logloss:0.10622\u001b[0m\n",
      "\u001b[34m[366]#011train-logloss:0.04747#011validation-logloss:0.10610\u001b[0m\n",
      "\u001b[34m[367]#011train-logloss:0.04747#011validation-logloss:0.10612\u001b[0m\n",
      "\u001b[34m[368]#011train-logloss:0.04747#011validation-logloss:0.10613\u001b[0m\n",
      "\u001b[34m[369]#011train-logloss:0.04747#011validation-logloss:0.10612\u001b[0m\n",
      "\u001b[34m[370]#011train-logloss:0.04747#011validation-logloss:0.10612\u001b[0m\n",
      "\u001b[34m[371]#011train-logloss:0.04747#011validation-logloss:0.10614\u001b[0m\n",
      "\u001b[34m[372]#011train-logloss:0.04747#011validation-logloss:0.10611\u001b[0m\n",
      "\u001b[34m[373]#011train-logloss:0.04747#011validation-logloss:0.10607\u001b[0m\n",
      "\u001b[34m[374]#011train-logloss:0.04747#011validation-logloss:0.10607\u001b[0m\n",
      "\u001b[34m[375]#011train-logloss:0.04747#011validation-logloss:0.10607\u001b[0m\n",
      "\u001b[34m[376]#011train-logloss:0.04737#011validation-logloss:0.10594\u001b[0m\n",
      "\u001b[34m[377]#011train-logloss:0.04737#011validation-logloss:0.10591\u001b[0m\n",
      "\u001b[34m[378]#011train-logloss:0.04737#011validation-logloss:0.10590\u001b[0m\n",
      "\u001b[34m[379]#011train-logloss:0.04737#011validation-logloss:0.10594\u001b[0m\n",
      "\u001b[34m[380]#011train-logloss:0.04702#011validation-logloss:0.10579\u001b[0m\n",
      "\u001b[34m[381]#011train-logloss:0.04702#011validation-logloss:0.10579\u001b[0m\n",
      "\u001b[34m[382]#011train-logloss:0.04691#011validation-logloss:0.10574\u001b[0m\n",
      "\u001b[34m[383]#011train-logloss:0.04691#011validation-logloss:0.10573\u001b[0m\n",
      "\u001b[34m[384]#011train-logloss:0.04691#011validation-logloss:0.10572\u001b[0m\n",
      "\u001b[34m[385]#011train-logloss:0.04691#011validation-logloss:0.10572\u001b[0m\n",
      "\u001b[34m[386]#011train-logloss:0.04691#011validation-logloss:0.10571\u001b[0m\n",
      "\u001b[34m[387]#011train-logloss:0.04691#011validation-logloss:0.10567\u001b[0m\n",
      "\u001b[34m[388]#011train-logloss:0.04691#011validation-logloss:0.10566\u001b[0m\n",
      "\u001b[34m[389]#011train-logloss:0.04691#011validation-logloss:0.10566\u001b[0m\n",
      "\u001b[34m[390]#011train-logloss:0.04691#011validation-logloss:0.10570\u001b[0m\n",
      "\u001b[34m[391]#011train-logloss:0.04691#011validation-logloss:0.10570\u001b[0m\n",
      "\u001b[34m[392]#011train-logloss:0.04691#011validation-logloss:0.10569\u001b[0m\n",
      "\u001b[34m[393]#011train-logloss:0.04691#011validation-logloss:0.10566\u001b[0m\n",
      "\u001b[34m[394]#011train-logloss:0.04691#011validation-logloss:0.10569\u001b[0m\n",
      "\u001b[34m[395]#011train-logloss:0.04691#011validation-logloss:0.10568\u001b[0m\n",
      "\u001b[34m[396]#011train-logloss:0.04691#011validation-logloss:0.10569\u001b[0m\n",
      "\u001b[34m[397]#011train-logloss:0.04691#011validation-logloss:0.10569\u001b[0m\n",
      "\u001b[34m[398]#011train-logloss:0.04691#011validation-logloss:0.10570\u001b[0m\n",
      "\u001b[34m[399]#011train-logloss:0.04691#011validation-logloss:0.10572\u001b[0m\n",
      "\u001b[34m[400]#011train-logloss:0.04691#011validation-logloss:0.10574\u001b[0m\n",
      "\u001b[34m[401]#011train-logloss:0.04691#011validation-logloss:0.10570\u001b[0m\n",
      "\u001b[34m[402]#011train-logloss:0.04691#011validation-logloss:0.10570\u001b[0m\n",
      "\u001b[34m[403]#011train-logloss:0.04658#011validation-logloss:0.10521\u001b[0m\n",
      "\u001b[34m[404]#011train-logloss:0.04658#011validation-logloss:0.10520\u001b[0m\n",
      "\u001b[34m[405]#011train-logloss:0.04658#011validation-logloss:0.10517\u001b[0m\n",
      "\u001b[34m[406]#011train-logloss:0.04658#011validation-logloss:0.10516\u001b[0m\n",
      "\u001b[34m[407]#011train-logloss:0.04658#011validation-logloss:0.10516\u001b[0m\n",
      "\u001b[34m[408]#011train-logloss:0.04658#011validation-logloss:0.10516\u001b[0m\n",
      "\u001b[34m[409]#011train-logloss:0.04658#011validation-logloss:0.10519\u001b[0m\n",
      "\u001b[34m[410]#011train-logloss:0.04650#011validation-logloss:0.10532\u001b[0m\n",
      "\u001b[34m[411]#011train-logloss:0.04650#011validation-logloss:0.10530\u001b[0m\n",
      "\u001b[34m[412]#011train-logloss:0.04650#011validation-logloss:0.10534\u001b[0m\n",
      "\u001b[34m[413]#011train-logloss:0.04650#011validation-logloss:0.10535\u001b[0m\n",
      "\u001b[34m[414]#011train-logloss:0.04650#011validation-logloss:0.10532\u001b[0m\n",
      "\u001b[34m[415]#011train-logloss:0.04650#011validation-logloss:0.10533\u001b[0m\n",
      "\u001b[34m[416]#011train-logloss:0.04650#011validation-logloss:0.10531\u001b[0m\n",
      "\u001b[34m[417]#011train-logloss:0.04650#011validation-logloss:0.10532\u001b[0m\n",
      "\u001b[34m[418]#011train-logloss:0.04650#011validation-logloss:0.10530\u001b[0m\n",
      "\u001b[34m[419]#011train-logloss:0.04650#011validation-logloss:0.10529\u001b[0m\n",
      "\u001b[34m[420]#011train-logloss:0.04635#011validation-logloss:0.10518\u001b[0m\n",
      "\u001b[34m[421]#011train-logloss:0.04635#011validation-logloss:0.10515\u001b[0m\n",
      "\u001b[34m[422]#011train-logloss:0.04635#011validation-logloss:0.10518\u001b[0m\n",
      "\u001b[34m[423]#011train-logloss:0.04635#011validation-logloss:0.10519\u001b[0m\n",
      "\u001b[34m[424]#011train-logloss:0.04635#011validation-logloss:0.10520\u001b[0m\n",
      "\u001b[34m[425]#011train-logloss:0.04635#011validation-logloss:0.10516\u001b[0m\n",
      "\u001b[34m[426]#011train-logloss:0.04635#011validation-logloss:0.10515\u001b[0m\n",
      "\u001b[34m[427]#011train-logloss:0.04630#011validation-logloss:0.10513\u001b[0m\n",
      "\u001b[34m[428]#011train-logloss:0.04630#011validation-logloss:0.10512\u001b[0m\n",
      "\u001b[34m[429]#011train-logloss:0.04630#011validation-logloss:0.10512\u001b[0m\n",
      "\u001b[34m[430]#011train-logloss:0.04618#011validation-logloss:0.10512\u001b[0m\n",
      "\u001b[34m[431]#011train-logloss:0.04618#011validation-logloss:0.10515\u001b[0m\n",
      "\u001b[34m[432]#011train-logloss:0.04618#011validation-logloss:0.10516\u001b[0m\n",
      "\u001b[34m[433]#011train-logloss:0.04618#011validation-logloss:0.10516\u001b[0m\n",
      "\u001b[34m[434]#011train-logloss:0.04618#011validation-logloss:0.10516\u001b[0m\n",
      "\u001b[34m[435]#011train-logloss:0.04619#011validation-logloss:0.10518\u001b[0m\n",
      "\u001b[34m[436]#011train-logloss:0.04619#011validation-logloss:0.10520\u001b[0m\n",
      "\u001b[34m[437]#011train-logloss:0.04618#011validation-logloss:0.10516\u001b[0m\n",
      "\u001b[34m[438]#011train-logloss:0.04618#011validation-logloss:0.10514\u001b[0m\n",
      "\u001b[34m[439]#011train-logloss:0.04618#011validation-logloss:0.10512\u001b[0m\n",
      "\u001b[34m[440]#011train-logloss:0.04619#011validation-logloss:0.10510\u001b[0m\n",
      "\u001b[34m[441]#011train-logloss:0.04618#011validation-logloss:0.10512\u001b[0m\n",
      "\u001b[34m[442]#011train-logloss:0.04618#011validation-logloss:0.10511\u001b[0m\n",
      "\u001b[34m[443]#011train-logloss:0.04610#011validation-logloss:0.10509\u001b[0m\n",
      "\u001b[34m[444]#011train-logloss:0.04610#011validation-logloss:0.10510\u001b[0m\n",
      "\u001b[34m[445]#011train-logloss:0.04610#011validation-logloss:0.10510\u001b[0m\n",
      "\u001b[34m[446]#011train-logloss:0.04610#011validation-logloss:0.10511\u001b[0m\n",
      "\u001b[34m[447]#011train-logloss:0.04610#011validation-logloss:0.10513\u001b[0m\n",
      "\u001b[34m[448]#011train-logloss:0.04610#011validation-logloss:0.10513\u001b[0m\n",
      "\u001b[34m[449]#011train-logloss:0.04610#011validation-logloss:0.10514\u001b[0m\n",
      "\u001b[34m[450]#011train-logloss:0.04610#011validation-logloss:0.10514\u001b[0m\n",
      "\u001b[34m[451]#011train-logloss:0.04610#011validation-logloss:0.10515\u001b[0m\n",
      "\u001b[34m[452]#011train-logloss:0.04610#011validation-logloss:0.10514\u001b[0m\n",
      "\u001b[34m[453]#011train-logloss:0.04610#011validation-logloss:0.10515\u001b[0m\n",
      "\u001b[34m[454]#011train-logloss:0.04594#011validation-logloss:0.10494\u001b[0m\n",
      "\u001b[34m[455]#011train-logloss:0.04594#011validation-logloss:0.10490\u001b[0m\n",
      "\u001b[34m[456]#011train-logloss:0.04579#011validation-logloss:0.10492\u001b[0m\n",
      "\u001b[34m[457]#011train-logloss:0.04579#011validation-logloss:0.10491\u001b[0m\n",
      "\u001b[34m[458]#011train-logloss:0.04579#011validation-logloss:0.10492\u001b[0m\n",
      "\u001b[34m[459]#011train-logloss:0.04579#011validation-logloss:0.10493\u001b[0m\n",
      "\u001b[34m[460]#011train-logloss:0.04579#011validation-logloss:0.10492\u001b[0m\n",
      "\u001b[34m[461]#011train-logloss:0.04579#011validation-logloss:0.10490\u001b[0m\n",
      "\u001b[34m[462]#011train-logloss:0.04579#011validation-logloss:0.10491\u001b[0m\n",
      "\u001b[34m[463]#011train-logloss:0.04579#011validation-logloss:0.10495\u001b[0m\n",
      "\u001b[34m[464]#011train-logloss:0.04579#011validation-logloss:0.10498\u001b[0m\n",
      "\u001b[34m[465]#011train-logloss:0.04579#011validation-logloss:0.10499\u001b[0m\n",
      "\u001b[34m[466]#011train-logloss:0.04579#011validation-logloss:0.10497\u001b[0m\n",
      "\u001b[34m[467]#011train-logloss:0.04566#011validation-logloss:0.10504\u001b[0m\n",
      "\u001b[34m[468]#011train-logloss:0.04560#011validation-logloss:0.10484\u001b[0m\n",
      "\u001b[34m[469]#011train-logloss:0.04560#011validation-logloss:0.10485\u001b[0m\n",
      "\u001b[34m[470]#011train-logloss:0.04560#011validation-logloss:0.10483\u001b[0m\n",
      "\u001b[34m[471]#011train-logloss:0.04560#011validation-logloss:0.10484\u001b[0m\n",
      "\u001b[34m[472]#011train-logloss:0.04560#011validation-logloss:0.10485\u001b[0m\n",
      "\u001b[34m[473]#011train-logloss:0.04560#011validation-logloss:0.10484\u001b[0m\n",
      "\u001b[34m[474]#011train-logloss:0.04560#011validation-logloss:0.10489\u001b[0m\n",
      "\u001b[34m[475]#011train-logloss:0.04560#011validation-logloss:0.10490\u001b[0m\n",
      "\u001b[34m[476]#011train-logloss:0.04560#011validation-logloss:0.10489\u001b[0m\n",
      "\u001b[34m[477]#011train-logloss:0.04560#011validation-logloss:0.10488\u001b[0m\n",
      "\u001b[34m[478]#011train-logloss:0.04560#011validation-logloss:0.10489\u001b[0m\n",
      "\u001b[34m[479]#011train-logloss:0.04560#011validation-logloss:0.10486\u001b[0m\n",
      "\u001b[34m[480]#011train-logloss:0.04560#011validation-logloss:0.10485\u001b[0m\n",
      "\u001b[34m[481]#011train-logloss:0.04560#011validation-logloss:0.10488\u001b[0m\n",
      "\u001b[34m[482]#011train-logloss:0.04560#011validation-logloss:0.10488\u001b[0m\n",
      "\u001b[34m[483]#011train-logloss:0.04560#011validation-logloss:0.10490\u001b[0m\n",
      "\u001b[34m[484]#011train-logloss:0.04560#011validation-logloss:0.10490\u001b[0m\n",
      "\u001b[34m[485]#011train-logloss:0.04560#011validation-logloss:0.10487\u001b[0m\n",
      "\u001b[34m[486]#011train-logloss:0.04554#011validation-logloss:0.10478\u001b[0m\n",
      "\u001b[34m[487]#011train-logloss:0.04554#011validation-logloss:0.10476\u001b[0m\n",
      "\u001b[34m[488]#011train-logloss:0.04553#011validation-logloss:0.10472\u001b[0m\n",
      "\u001b[34m[489]#011train-logloss:0.04553#011validation-logloss:0.10469\u001b[0m\n",
      "\u001b[34m[490]#011train-logloss:0.04553#011validation-logloss:0.10467\u001b[0m\n",
      "\u001b[34m[491]#011train-logloss:0.04553#011validation-logloss:0.10470\u001b[0m\n",
      "\u001b[34m[492]#011train-logloss:0.04553#011validation-logloss:0.10467\u001b[0m\n",
      "\u001b[34m[493]#011train-logloss:0.04553#011validation-logloss:0.10469\u001b[0m\n",
      "\u001b[34m[494]#011train-logloss:0.04553#011validation-logloss:0.10469\u001b[0m\n",
      "\u001b[34m[495]#011train-logloss:0.04553#011validation-logloss:0.10467\u001b[0m\n",
      "\u001b[34m[496]#011train-logloss:0.04553#011validation-logloss:0.10469\u001b[0m\n",
      "\u001b[34m[497]#011train-logloss:0.04553#011validation-logloss:0.10467\u001b[0m\n",
      "\u001b[34m[498]#011train-logloss:0.04553#011validation-logloss:0.10471\u001b[0m\n",
      "\u001b[34m[499]#011train-logloss:0.04553#011validation-logloss:0.10469\u001b[0m\n",
      "\u001b[34m[500]#011train-logloss:0.04553#011validation-logloss:0.10467\u001b[0m\n",
      "\u001b[34m[501]#011train-logloss:0.04554#011validation-logloss:0.10465\u001b[0m\n",
      "\u001b[34m[502]#011train-logloss:0.04554#011validation-logloss:0.10462\u001b[0m\n",
      "\u001b[34m[503]#011train-logloss:0.04554#011validation-logloss:0.10464\u001b[0m\n",
      "\u001b[34m[504]#011train-logloss:0.04547#011validation-logloss:0.10459\u001b[0m\n",
      "\u001b[34m[505]#011train-logloss:0.04547#011validation-logloss:0.10462\u001b[0m\n",
      "\u001b[34m[506]#011train-logloss:0.04547#011validation-logloss:0.10462\u001b[0m\n",
      "\u001b[34m[507]#011train-logloss:0.04547#011validation-logloss:0.10464\u001b[0m\n",
      "\u001b[34m[508]#011train-logloss:0.04547#011validation-logloss:0.10460\u001b[0m\n",
      "\u001b[34m[509]#011train-logloss:0.04547#011validation-logloss:0.10462\u001b[0m\n",
      "\u001b[34m[510]#011train-logloss:0.04547#011validation-logloss:0.10464\u001b[0m\n",
      "\u001b[34m[511]#011train-logloss:0.04547#011validation-logloss:0.10465\u001b[0m\n",
      "\u001b[34m[512]#011train-logloss:0.04539#011validation-logloss:0.10463\u001b[0m\n",
      "\u001b[34m[513]#011train-logloss:0.04539#011validation-logloss:0.10468\u001b[0m\n",
      "\u001b[34m[514]#011train-logloss:0.04539#011validation-logloss:0.10465\u001b[0m\n",
      "\u001b[34m[515]#011train-logloss:0.04539#011validation-logloss:0.10464\u001b[0m\n",
      "\u001b[34m[516]#011train-logloss:0.04527#011validation-logloss:0.10450\u001b[0m\n",
      "\u001b[34m[517]#011train-logloss:0.04527#011validation-logloss:0.10445\u001b[0m\n",
      "\u001b[34m[518]#011train-logloss:0.04519#011validation-logloss:0.10464\u001b[0m\n",
      "\u001b[34m[519]#011train-logloss:0.04519#011validation-logloss:0.10459\u001b[0m\n",
      "\u001b[34m[520]#011train-logloss:0.04519#011validation-logloss:0.10463\u001b[0m\n",
      "\u001b[34m[521]#011train-logloss:0.04519#011validation-logloss:0.10461\u001b[0m\n",
      "\u001b[34m[522]#011train-logloss:0.04519#011validation-logloss:0.10461\u001b[0m\n",
      "\u001b[34m[523]#011train-logloss:0.04519#011validation-logloss:0.10461\u001b[0m\n",
      "\u001b[34m[524]#011train-logloss:0.04519#011validation-logloss:0.10463\u001b[0m\n",
      "\u001b[34m[525]#011train-logloss:0.04519#011validation-logloss:0.10464\u001b[0m\n",
      "\u001b[34m[526]#011train-logloss:0.04519#011validation-logloss:0.10461\u001b[0m\n",
      "\u001b[34m[527]#011train-logloss:0.04519#011validation-logloss:0.10463\u001b[0m\n",
      "\u001b[34m[528]#011train-logloss:0.04519#011validation-logloss:0.10465\u001b[0m\n",
      "\u001b[34m[529]#011train-logloss:0.04519#011validation-logloss:0.10466\u001b[0m\n",
      "\u001b[34m[530]#011train-logloss:0.04519#011validation-logloss:0.10465\u001b[0m\n",
      "\u001b[34m[531]#011train-logloss:0.04519#011validation-logloss:0.10460\u001b[0m\n",
      "\u001b[34m[532]#011train-logloss:0.04519#011validation-logloss:0.10461\u001b[0m\n",
      "\u001b[34m[533]#011train-logloss:0.04519#011validation-logloss:0.10462\u001b[0m\n",
      "\u001b[34m[534]#011train-logloss:0.04519#011validation-logloss:0.10465\u001b[0m\n",
      "\u001b[34m[535]#011train-logloss:0.04519#011validation-logloss:0.10467\u001b[0m\n",
      "\u001b[34m[536]#011train-logloss:0.04519#011validation-logloss:0.10466\u001b[0m\n",
      "\u001b[34m[537]#011train-logloss:0.04519#011validation-logloss:0.10463\u001b[0m\n",
      "\u001b[34m[538]#011train-logloss:0.04508#011validation-logloss:0.10494\u001b[0m\n",
      "\u001b[34m[539]#011train-logloss:0.04508#011validation-logloss:0.10494\u001b[0m\n",
      "\u001b[34m[540]#011train-logloss:0.04508#011validation-logloss:0.10491\u001b[0m\n",
      "\u001b[34m[541]#011train-logloss:0.04508#011validation-logloss:0.10491\u001b[0m\n",
      "\u001b[34m[542]#011train-logloss:0.04508#011validation-logloss:0.10492\u001b[0m\n",
      "\u001b[34m[543]#011train-logloss:0.04508#011validation-logloss:0.10491\u001b[0m\n",
      "\u001b[34m[544]#011train-logloss:0.04494#011validation-logloss:0.10499\u001b[0m\n",
      "\u001b[34m[545]#011train-logloss:0.04494#011validation-logloss:0.10500\u001b[0m\n",
      "\u001b[34m[546]#011train-logloss:0.04494#011validation-logloss:0.10501\u001b[0m\n",
      "\u001b[34m[547]#011train-logloss:0.04494#011validation-logloss:0.10498\u001b[0m\n",
      "\u001b[34m[548]#011train-logloss:0.04494#011validation-logloss:0.10498\u001b[0m\n",
      "\u001b[34m[549]#011train-logloss:0.04494#011validation-logloss:0.10500\u001b[0m\n",
      "\u001b[34m[550]#011train-logloss:0.04484#011validation-logloss:0.10478\u001b[0m\n",
      "\u001b[34m[551]#011train-logloss:0.04484#011validation-logloss:0.10479\u001b[0m\n",
      "\u001b[34m[552]#011train-logloss:0.04484#011validation-logloss:0.10479\u001b[0m\n",
      "\u001b[34m[553]#011train-logloss:0.04484#011validation-logloss:0.10478\u001b[0m\n",
      "\u001b[34m[554]#011train-logloss:0.04484#011validation-logloss:0.10481\u001b[0m\n",
      "\u001b[34m[555]#011train-logloss:0.04484#011validation-logloss:0.10481\u001b[0m\n",
      "\u001b[34m[556]#011train-logloss:0.04484#011validation-logloss:0.10478\u001b[0m\n",
      "\u001b[34m[557]#011train-logloss:0.04484#011validation-logloss:0.10477\u001b[0m\n",
      "\u001b[34m[558]#011train-logloss:0.04484#011validation-logloss:0.10477\u001b[0m\n",
      "\u001b[34m[559]#011train-logloss:0.04484#011validation-logloss:0.10479\u001b[0m\n",
      "\u001b[34m[560]#011train-logloss:0.04474#011validation-logloss:0.10475\u001b[0m\n",
      "\u001b[34m[561]#011train-logloss:0.04474#011validation-logloss:0.10476\u001b[0m\n",
      "\u001b[34m[562]#011train-logloss:0.04474#011validation-logloss:0.10480\u001b[0m\n",
      "\u001b[34m[563]#011train-logloss:0.04474#011validation-logloss:0.10483\u001b[0m\n",
      "\u001b[34m[564]#011train-logloss:0.04474#011validation-logloss:0.10484\u001b[0m\n",
      "\u001b[34m[565]#011train-logloss:0.04474#011validation-logloss:0.10481\u001b[0m\n",
      "\u001b[34m[566]#011train-logloss:0.04466#011validation-logloss:0.10463\u001b[0m\n",
      "\u001b[34m[567]#011train-logloss:0.04466#011validation-logloss:0.10462\u001b[0m\n",
      "\u001b[34m[568]#011train-logloss:0.04466#011validation-logloss:0.10464\u001b[0m\n",
      "\u001b[34m[569]#011train-logloss:0.04466#011validation-logloss:0.10462\u001b[0m\n",
      "\u001b[34m[570]#011train-logloss:0.04466#011validation-logloss:0.10461\u001b[0m\n",
      "\u001b[34m[571]#011train-logloss:0.04466#011validation-logloss:0.10460\u001b[0m\n",
      "\u001b[34m[572]#011train-logloss:0.04466#011validation-logloss:0.10459\u001b[0m\n",
      "\u001b[34m[573]#011train-logloss:0.04466#011validation-logloss:0.10455\u001b[0m\n",
      "\u001b[34m[574]#011train-logloss:0.04458#011validation-logloss:0.10473\u001b[0m\n",
      "\u001b[34m[575]#011train-logloss:0.04458#011validation-logloss:0.10477\u001b[0m\n",
      "\u001b[34m[576]#011train-logloss:0.04458#011validation-logloss:0.10476\u001b[0m\n",
      "\u001b[34m[577]#011train-logloss:0.04458#011validation-logloss:0.10475\u001b[0m\n",
      "\u001b[34m[578]#011train-logloss:0.04458#011validation-logloss:0.10479\u001b[0m\n",
      "\u001b[34m[579]#011train-logloss:0.04448#011validation-logloss:0.10474\u001b[0m\n",
      "\u001b[34m[580]#011train-logloss:0.04448#011validation-logloss:0.10475\u001b[0m\n",
      "\u001b[34m[581]#011train-logloss:0.04448#011validation-logloss:0.10476\u001b[0m\n",
      "\u001b[34m[582]#011train-logloss:0.04448#011validation-logloss:0.10476\u001b[0m\n",
      "\u001b[34m[583]#011train-logloss:0.04448#011validation-logloss:0.10476\u001b[0m\n",
      "\u001b[34m[584]#011train-logloss:0.04427#011validation-logloss:0.10488\u001b[0m\n",
      "\u001b[34m[585]#011train-logloss:0.04427#011validation-logloss:0.10489\u001b[0m\n",
      "\u001b[34m[586]#011train-logloss:0.04427#011validation-logloss:0.10486\u001b[0m\n",
      "\u001b[34m[587]#011train-logloss:0.04427#011validation-logloss:0.10487\u001b[0m\n",
      "\u001b[34m[588]#011train-logloss:0.04427#011validation-logloss:0.10487\u001b[0m\n",
      "\u001b[34m[589]#011train-logloss:0.04427#011validation-logloss:0.10484\u001b[0m\n",
      "\u001b[34m[590]#011train-logloss:0.04427#011validation-logloss:0.10484\u001b[0m\n",
      "\u001b[34m[591]#011train-logloss:0.04427#011validation-logloss:0.10486\u001b[0m\n",
      "\u001b[34m[592]#011train-logloss:0.04427#011validation-logloss:0.10486\u001b[0m\n",
      "\u001b[34m[593]#011train-logloss:0.04427#011validation-logloss:0.10490\u001b[0m\n",
      "\u001b[34m[594]#011train-logloss:0.04427#011validation-logloss:0.10490\u001b[0m\n",
      "\u001b[34m[595]#011train-logloss:0.04421#011validation-logloss:0.10500\u001b[0m\n",
      "\u001b[34m[596]#011train-logloss:0.04421#011validation-logloss:0.10498\u001b[0m\n",
      "\u001b[34m[597]#011train-logloss:0.04421#011validation-logloss:0.10496\u001b[0m\n",
      "\u001b[34m[598]#011train-logloss:0.04411#011validation-logloss:0.10481\u001b[0m\n",
      "\u001b[34m[599]#011train-logloss:0.04411#011validation-logloss:0.10482\u001b[0m\n",
      "\u001b[34m[600]#011train-logloss:0.04401#011validation-logloss:0.10487\u001b[0m\n",
      "\u001b[34m[601]#011train-logloss:0.04401#011validation-logloss:0.10487\u001b[0m\n",
      "\u001b[34m[602]#011train-logloss:0.04401#011validation-logloss:0.10485\u001b[0m\n",
      "\u001b[34m[603]#011train-logloss:0.04401#011validation-logloss:0.10485\u001b[0m\n",
      "\u001b[34m[604]#011train-logloss:0.04401#011validation-logloss:0.10484\u001b[0m\n",
      "\u001b[34m[605]#011train-logloss:0.04401#011validation-logloss:0.10487\u001b[0m\n",
      "\u001b[34m[606]#011train-logloss:0.04401#011validation-logloss:0.10485\u001b[0m\n",
      "\u001b[34m[607]#011train-logloss:0.04394#011validation-logloss:0.10475\u001b[0m\n",
      "\u001b[34m[608]#011train-logloss:0.04382#011validation-logloss:0.10477\u001b[0m\n",
      "\u001b[34m[609]#011train-logloss:0.04382#011validation-logloss:0.10475\u001b[0m\n",
      "\u001b[34m[610]#011train-logloss:0.04382#011validation-logloss:0.10478\u001b[0m\n",
      "\u001b[34m[611]#011train-logloss:0.04382#011validation-logloss:0.10476\u001b[0m\n",
      "\u001b[34m[612]#011train-logloss:0.04371#011validation-logloss:0.10487\u001b[0m\n",
      "\u001b[34m[613]#011train-logloss:0.04371#011validation-logloss:0.10483\u001b[0m\n",
      "\u001b[34m[614]#011train-logloss:0.04371#011validation-logloss:0.10484\u001b[0m\n",
      "\u001b[34m[615]#011train-logloss:0.04371#011validation-logloss:0.10482\u001b[0m\n",
      "\u001b[34m[616]#011train-logloss:0.04371#011validation-logloss:0.10480\u001b[0m\n",
      "\u001b[34m[617]#011train-logloss:0.04371#011validation-logloss:0.10482\u001b[0m\n",
      "\u001b[34m[618]#011train-logloss:0.04371#011validation-logloss:0.10480\u001b[0m\n",
      "\u001b[34m[619]#011train-logloss:0.04371#011validation-logloss:0.10481\u001b[0m\n",
      "\u001b[34m[620]#011train-logloss:0.04371#011validation-logloss:0.10483\u001b[0m\n",
      "\u001b[34m[621]#011train-logloss:0.04371#011validation-logloss:0.10484\u001b[0m\n",
      "\u001b[34m[622]#011train-logloss:0.04371#011validation-logloss:0.10487\u001b[0m\n",
      "\u001b[34m[623]#011train-logloss:0.04371#011validation-logloss:0.10487\u001b[0m\n",
      "\u001b[34m[624]#011train-logloss:0.04371#011validation-logloss:0.10487\u001b[0m\n",
      "\u001b[34m[625]#011train-logloss:0.04371#011validation-logloss:0.10489\u001b[0m\n",
      "\u001b[34m[626]#011train-logloss:0.04371#011validation-logloss:0.10488\u001b[0m\n",
      "\u001b[34m[627]#011train-logloss:0.04371#011validation-logloss:0.10483\u001b[0m\n",
      "\u001b[34m[628]#011train-logloss:0.04371#011validation-logloss:0.10485\u001b[0m\n",
      "\u001b[34m[629]#011train-logloss:0.04371#011validation-logloss:0.10484\u001b[0m\n",
      "\u001b[34m[630]#011train-logloss:0.04371#011validation-logloss:0.10484\u001b[0m\n",
      "\u001b[34m[631]#011train-logloss:0.04371#011validation-logloss:0.10483\u001b[0m\n",
      "\u001b[34m[632]#011train-logloss:0.04364#011validation-logloss:0.10504\u001b[0m\n",
      "\u001b[34m[633]#011train-logloss:0.04364#011validation-logloss:0.10505\u001b[0m\n",
      "\u001b[34m[634]#011train-logloss:0.04364#011validation-logloss:0.10504\u001b[0m\n",
      "\u001b[34m[635]#011train-logloss:0.04364#011validation-logloss:0.10501\u001b[0m\n",
      "\u001b[34m[636]#011train-logloss:0.04364#011validation-logloss:0.10502\u001b[0m\n",
      "\u001b[34m[637]#011train-logloss:0.04357#011validation-logloss:0.10508\u001b[0m\n",
      "\u001b[34m[638]#011train-logloss:0.04357#011validation-logloss:0.10509\u001b[0m\n",
      "\u001b[34m[639]#011train-logloss:0.04347#011validation-logloss:0.10493\u001b[0m\n",
      "\u001b[34m[640]#011train-logloss:0.04347#011validation-logloss:0.10492\u001b[0m\n",
      "\u001b[34m[641]#011train-logloss:0.04347#011validation-logloss:0.10493\u001b[0m\n",
      "\u001b[34m[642]#011train-logloss:0.04347#011validation-logloss:0.10493\u001b[0m\n",
      "\u001b[34m[643]#011train-logloss:0.04347#011validation-logloss:0.10489\u001b[0m\n",
      "\u001b[34m[644]#011train-logloss:0.04335#011validation-logloss:0.10470\u001b[0m\n",
      "\u001b[34m[645]#011train-logloss:0.04335#011validation-logloss:0.10472\u001b[0m\n",
      "\u001b[34m[646]#011train-logloss:0.04335#011validation-logloss:0.10472\u001b[0m\n",
      "\u001b[34m[647]#011train-logloss:0.04325#011validation-logloss:0.10468\u001b[0m\n",
      "\u001b[34m[648]#011train-logloss:0.04325#011validation-logloss:0.10472\u001b[0m\n",
      "\u001b[34m[649]#011train-logloss:0.04325#011validation-logloss:0.10474\u001b[0m\n",
      "\u001b[34m[650]#011train-logloss:0.04325#011validation-logloss:0.10473\u001b[0m\n",
      "\u001b[34m[651]#011train-logloss:0.04325#011validation-logloss:0.10472\u001b[0m\n",
      "\u001b[34m[652]#011train-logloss:0.04325#011validation-logloss:0.10469\u001b[0m\n",
      "\u001b[34m[653]#011train-logloss:0.04325#011validation-logloss:0.10468\u001b[0m\n",
      "\u001b[34m[654]#011train-logloss:0.04325#011validation-logloss:0.10464\u001b[0m\n",
      "\u001b[34m[655]#011train-logloss:0.04325#011validation-logloss:0.10465\u001b[0m\n",
      "\u001b[34m[656]#011train-logloss:0.04325#011validation-logloss:0.10465\u001b[0m\n",
      "\u001b[34m[657]#011train-logloss:0.04325#011validation-logloss:0.10468\u001b[0m\n",
      "\u001b[34m[658]#011train-logloss:0.04309#011validation-logloss:0.10450\u001b[0m\n",
      "\u001b[34m[659]#011train-logloss:0.04309#011validation-logloss:0.10447\u001b[0m\n",
      "\u001b[34m[660]#011train-logloss:0.04309#011validation-logloss:0.10449\u001b[0m\n",
      "\u001b[34m[661]#011train-logloss:0.04300#011validation-logloss:0.10454\u001b[0m\n",
      "\u001b[34m[662]#011train-logloss:0.04300#011validation-logloss:0.10452\u001b[0m\n",
      "\u001b[34m[663]#011train-logloss:0.04300#011validation-logloss:0.10451\u001b[0m\n",
      "\u001b[34m[664]#011train-logloss:0.04290#011validation-logloss:0.10451\u001b[0m\n",
      "\u001b[34m[665]#011train-logloss:0.04290#011validation-logloss:0.10449\u001b[0m\n",
      "\u001b[34m[666]#011train-logloss:0.04290#011validation-logloss:0.10446\u001b[0m\n",
      "\u001b[34m[667]#011train-logloss:0.04290#011validation-logloss:0.10443\u001b[0m\n",
      "\u001b[34m[668]#011train-logloss:0.04290#011validation-logloss:0.10447\u001b[0m\n",
      "\u001b[34m[669]#011train-logloss:0.04290#011validation-logloss:0.10448\u001b[0m\n",
      "\u001b[34m[670]#011train-logloss:0.04290#011validation-logloss:0.10450\u001b[0m\n",
      "\u001b[34m[671]#011train-logloss:0.04290#011validation-logloss:0.10449\u001b[0m\n",
      "\u001b[34m[672]#011train-logloss:0.04290#011validation-logloss:0.10450\u001b[0m\n",
      "\u001b[34m[673]#011train-logloss:0.04290#011validation-logloss:0.10453\u001b[0m\n",
      "\u001b[34m[674]#011train-logloss:0.04290#011validation-logloss:0.10450\u001b[0m\n",
      "\u001b[34m[675]#011train-logloss:0.04290#011validation-logloss:0.10450\u001b[0m\n",
      "\u001b[34m[676]#011train-logloss:0.04290#011validation-logloss:0.10452\u001b[0m\n",
      "\u001b[34m[677]#011train-logloss:0.04290#011validation-logloss:0.10455\u001b[0m\n",
      "\u001b[34m[678]#011train-logloss:0.04290#011validation-logloss:0.10449\u001b[0m\n",
      "\u001b[34m[679]#011train-logloss:0.04279#011validation-logloss:0.10461\u001b[0m\n",
      "\u001b[34m[680]#011train-logloss:0.04279#011validation-logloss:0.10462\u001b[0m\n",
      "\u001b[34m[681]#011train-logloss:0.04279#011validation-logloss:0.10463\u001b[0m\n",
      "\u001b[34m[682]#011train-logloss:0.04279#011validation-logloss:0.10460\u001b[0m\n",
      "\u001b[34m[683]#011train-logloss:0.04279#011validation-logloss:0.10461\u001b[0m\n",
      "\u001b[34m[684]#011train-logloss:0.04279#011validation-logloss:0.10461\u001b[0m\n",
      "\u001b[34m[685]#011train-logloss:0.04279#011validation-logloss:0.10458\u001b[0m\n",
      "\u001b[34m[686]#011train-logloss:0.04279#011validation-logloss:0.10455\u001b[0m\n",
      "\u001b[34m[687]#011train-logloss:0.04279#011validation-logloss:0.10458\u001b[0m\n",
      "\u001b[34m[688]#011train-logloss:0.04279#011validation-logloss:0.10458\u001b[0m\n",
      "\u001b[34m[689]#011train-logloss:0.04279#011validation-logloss:0.10459\u001b[0m\n",
      "\u001b[34m[690]#011train-logloss:0.04279#011validation-logloss:0.10456\u001b[0m\n",
      "\u001b[34m[691]#011train-logloss:0.04272#011validation-logloss:0.10463\u001b[0m\n",
      "\u001b[34m[692]#011train-logloss:0.04272#011validation-logloss:0.10466\u001b[0m\n",
      "\u001b[34m[693]#011train-logloss:0.04272#011validation-logloss:0.10462\u001b[0m\n",
      "\u001b[34m[694]#011train-logloss:0.04272#011validation-logloss:0.10460\u001b[0m\n",
      "\u001b[34m[695]#011train-logloss:0.04272#011validation-logloss:0.10461\u001b[0m\n",
      "\u001b[34m[696]#011train-logloss:0.04272#011validation-logloss:0.10464\u001b[0m\n",
      "\u001b[34m[697]#011train-logloss:0.04261#011validation-logloss:0.10456\u001b[0m\n",
      "\u001b[34m[698]#011train-logloss:0.04261#011validation-logloss:0.10453\u001b[0m\n",
      "\u001b[34m[699]#011train-logloss:0.04261#011validation-logloss:0.10453\u001b[0m\n",
      "\u001b[34m[700]#011train-logloss:0.04245#011validation-logloss:0.10445\u001b[0m\n",
      "\u001b[34m[701]#011train-logloss:0.04245#011validation-logloss:0.10446\u001b[0m\n",
      "\u001b[34m[702]#011train-logloss:0.04245#011validation-logloss:0.10443\u001b[0m\n",
      "\u001b[34m[703]#011train-logloss:0.04245#011validation-logloss:0.10443\u001b[0m\n",
      "\u001b[34m[704]#011train-logloss:0.04245#011validation-logloss:0.10445\u001b[0m\n",
      "\u001b[34m[705]#011train-logloss:0.04245#011validation-logloss:0.10441\u001b[0m\n",
      "\u001b[34m[706]#011train-logloss:0.04245#011validation-logloss:0.10446\u001b[0m\n",
      "\u001b[34m[707]#011train-logloss:0.04245#011validation-logloss:0.10448\u001b[0m\n",
      "\u001b[34m[708]#011train-logloss:0.04245#011validation-logloss:0.10447\u001b[0m\n",
      "\n",
      "2024-09-29 20:33:56 Uploading - Uploading generated training model\u001b[34m[709]#011train-logloss:0.04245#011validation-logloss:0.10445\u001b[0m\n",
      "\u001b[34m[710]#011train-logloss:0.04245#011validation-logloss:0.10446\u001b[0m\n",
      "\u001b[34m[711]#011train-logloss:0.04239#011validation-logloss:0.10438\u001b[0m\n",
      "\u001b[34m[712]#011train-logloss:0.04239#011validation-logloss:0.10441\u001b[0m\n",
      "\u001b[34m[713]#011train-logloss:0.04239#011validation-logloss:0.10438\u001b[0m\n",
      "\u001b[34m[714]#011train-logloss:0.04239#011validation-logloss:0.10437\u001b[0m\n",
      "\u001b[34m[715]#011train-logloss:0.04239#011validation-logloss:0.10438\u001b[0m\n",
      "\u001b[34m[716]#011train-logloss:0.04239#011validation-logloss:0.10436\u001b[0m\n",
      "\u001b[34m[717]#011train-logloss:0.04239#011validation-logloss:0.10435\u001b[0m\n",
      "\u001b[34m[718]#011train-logloss:0.04239#011validation-logloss:0.10433\u001b[0m\n",
      "\u001b[34m[719]#011train-logloss:0.04231#011validation-logloss:0.10444\u001b[0m\n",
      "\u001b[34m[720]#011train-logloss:0.04231#011validation-logloss:0.10444\u001b[0m\n",
      "\u001b[34m[721]#011train-logloss:0.04231#011validation-logloss:0.10447\u001b[0m\n",
      "\u001b[34m[722]#011train-logloss:0.04231#011validation-logloss:0.10447\u001b[0m\n",
      "\u001b[34m[723]#011train-logloss:0.04231#011validation-logloss:0.10441\u001b[0m\n",
      "\u001b[34m[724]#011train-logloss:0.04231#011validation-logloss:0.10443\u001b[0m\n",
      "\u001b[34m[725]#011train-logloss:0.04231#011validation-logloss:0.10439\u001b[0m\n",
      "\u001b[34m[726]#011train-logloss:0.04231#011validation-logloss:0.10440\u001b[0m\n",
      "\u001b[34m[727]#011train-logloss:0.04231#011validation-logloss:0.10440\u001b[0m\n",
      "\u001b[34m[728]#011train-logloss:0.04231#011validation-logloss:0.10441\u001b[0m\n",
      "\u001b[34m[729]#011train-logloss:0.04231#011validation-logloss:0.10442\u001b[0m\n",
      "\u001b[34m[730]#011train-logloss:0.04231#011validation-logloss:0.10443\u001b[0m\n",
      "\u001b[34m[731]#011train-logloss:0.04231#011validation-logloss:0.10440\u001b[0m\n",
      "\u001b[34m[732]#011train-logloss:0.04231#011validation-logloss:0.10443\u001b[0m\n",
      "\u001b[34m[733]#011train-logloss:0.04231#011validation-logloss:0.10445\u001b[0m\n",
      "\u001b[34m[734]#011train-logloss:0.04231#011validation-logloss:0.10448\u001b[0m\n",
      "\u001b[34m[735]#011train-logloss:0.04231#011validation-logloss:0.10439\u001b[0m\n",
      "\u001b[34m[736]#011train-logloss:0.04231#011validation-logloss:0.10442\u001b[0m\n",
      "\u001b[34m[737]#011train-logloss:0.04231#011validation-logloss:0.10442\u001b[0m\n",
      "\u001b[34m[738]#011train-logloss:0.04231#011validation-logloss:0.10445\u001b[0m\n",
      "\u001b[34m[739]#011train-logloss:0.04231#011validation-logloss:0.10447\u001b[0m\n",
      "\u001b[34m[740]#011train-logloss:0.04231#011validation-logloss:0.10446\u001b[0m\n",
      "\u001b[34m[741]#011train-logloss:0.04221#011validation-logloss:0.10440\u001b[0m\n",
      "\u001b[34m[742]#011train-logloss:0.04221#011validation-logloss:0.10443\u001b[0m\n",
      "\u001b[34m[743]#011train-logloss:0.04211#011validation-logloss:0.10435\u001b[0m\n",
      "\u001b[34m[744]#011train-logloss:0.04211#011validation-logloss:0.10437\u001b[0m\n",
      "\u001b[34m[745]#011train-logloss:0.04201#011validation-logloss:0.10457\u001b[0m\n",
      "\u001b[34m[746]#011train-logloss:0.04201#011validation-logloss:0.10452\u001b[0m\n",
      "\u001b[34m[747]#011train-logloss:0.04201#011validation-logloss:0.10449\u001b[0m\n",
      "\u001b[34m[748]#011train-logloss:0.04192#011validation-logloss:0.10447\u001b[0m\n",
      "\u001b[34m[749]#011train-logloss:0.04188#011validation-logloss:0.10437\u001b[0m\n",
      "\u001b[34m[750]#011train-logloss:0.04188#011validation-logloss:0.10439\u001b[0m\n",
      "\u001b[34m[751]#011train-logloss:0.04188#011validation-logloss:0.10434\u001b[0m\n",
      "\u001b[34m[752]#011train-logloss:0.04188#011validation-logloss:0.10438\u001b[0m\n",
      "\u001b[34m[753]#011train-logloss:0.04188#011validation-logloss:0.10438\u001b[0m\n",
      "\u001b[34m[754]#011train-logloss:0.04188#011validation-logloss:0.10438\u001b[0m\n",
      "\u001b[34m[755]#011train-logloss:0.04188#011validation-logloss:0.10438\u001b[0m\n",
      "\u001b[34m[756]#011train-logloss:0.04188#011validation-logloss:0.10434\u001b[0m\n",
      "\u001b[34m[757]#011train-logloss:0.04188#011validation-logloss:0.10435\u001b[0m\n",
      "\u001b[34m[758]#011train-logloss:0.04188#011validation-logloss:0.10436\u001b[0m\n",
      "\u001b[34m[759]#011train-logloss:0.04188#011validation-logloss:0.10436\u001b[0m\n",
      "\u001b[34m[760]#011train-logloss:0.04184#011validation-logloss:0.10423\u001b[0m\n",
      "\u001b[34m[761]#011train-logloss:0.04184#011validation-logloss:0.10424\u001b[0m\n",
      "\u001b[34m[762]#011train-logloss:0.04184#011validation-logloss:0.10421\u001b[0m\n",
      "\u001b[34m[763]#011train-logloss:0.04184#011validation-logloss:0.10420\u001b[0m\n",
      "\u001b[34m[764]#011train-logloss:0.04175#011validation-logloss:0.10412\u001b[0m\n",
      "\u001b[34m[765]#011train-logloss:0.04175#011validation-logloss:0.10416\u001b[0m\n",
      "\u001b[34m[766]#011train-logloss:0.04175#011validation-logloss:0.10416\u001b[0m\n",
      "\u001b[34m[767]#011train-logloss:0.04175#011validation-logloss:0.10414\u001b[0m\n",
      "\u001b[34m[768]#011train-logloss:0.04175#011validation-logloss:0.10411\u001b[0m\n",
      "\u001b[34m[769]#011train-logloss:0.04175#011validation-logloss:0.10411\u001b[0m\n",
      "\u001b[34m[770]#011train-logloss:0.04175#011validation-logloss:0.10411\u001b[0m\n",
      "\u001b[34m[771]#011train-logloss:0.04175#011validation-logloss:0.10411\u001b[0m\n",
      "\u001b[34m[772]#011train-logloss:0.04175#011validation-logloss:0.10415\u001b[0m\n",
      "\u001b[34m[773]#011train-logloss:0.04175#011validation-logloss:0.10415\u001b[0m\n",
      "\u001b[34m[774]#011train-logloss:0.04175#011validation-logloss:0.10415\u001b[0m\n",
      "\u001b[34m[775]#011train-logloss:0.04175#011validation-logloss:0.10416\u001b[0m\n",
      "\u001b[34m[776]#011train-logloss:0.04175#011validation-logloss:0.10417\u001b[0m\n",
      "\u001b[34m[777]#011train-logloss:0.04175#011validation-logloss:0.10416\u001b[0m\n",
      "\u001b[34m[778]#011train-logloss:0.04175#011validation-logloss:0.10415\u001b[0m\n",
      "\u001b[34m[779]#011train-logloss:0.04165#011validation-logloss:0.10416\u001b[0m\n",
      "\u001b[34m[780]#011train-logloss:0.04165#011validation-logloss:0.10417\u001b[0m\n",
      "\u001b[34m[781]#011train-logloss:0.04165#011validation-logloss:0.10416\u001b[0m\n",
      "\u001b[34m[782]#011train-logloss:0.04165#011validation-logloss:0.10417\u001b[0m\n",
      "\u001b[34m[783]#011train-logloss:0.04156#011validation-logloss:0.10401\u001b[0m\n",
      "\u001b[34m[784]#011train-logloss:0.04156#011validation-logloss:0.10399\u001b[0m\n",
      "\u001b[34m[785]#011train-logloss:0.04156#011validation-logloss:0.10398\u001b[0m\n",
      "\u001b[34m[786]#011train-logloss:0.04156#011validation-logloss:0.10396\u001b[0m\n",
      "\u001b[34m[787]#011train-logloss:0.04156#011validation-logloss:0.10393\u001b[0m\n",
      "\u001b[34m[788]#011train-logloss:0.04156#011validation-logloss:0.10393\u001b[0m\n",
      "\u001b[34m[789]#011train-logloss:0.04156#011validation-logloss:0.10394\u001b[0m\n",
      "\u001b[34m[790]#011train-logloss:0.04143#011validation-logloss:0.10378\u001b[0m\n",
      "\u001b[34m[791]#011train-logloss:0.04133#011validation-logloss:0.10368\u001b[0m\n",
      "\u001b[34m[792]#011train-logloss:0.04133#011validation-logloss:0.10369\u001b[0m\n",
      "\u001b[34m[793]#011train-logloss:0.04133#011validation-logloss:0.10371\u001b[0m\n",
      "\u001b[34m[794]#011train-logloss:0.04133#011validation-logloss:0.10372\u001b[0m\n",
      "\u001b[34m[795]#011train-logloss:0.04123#011validation-logloss:0.10371\u001b[0m\n",
      "\u001b[34m[796]#011train-logloss:0.04123#011validation-logloss:0.10368\u001b[0m\n",
      "\u001b[34m[797]#011train-logloss:0.04123#011validation-logloss:0.10369\u001b[0m\n",
      "\u001b[34m[798]#011train-logloss:0.04123#011validation-logloss:0.10368\u001b[0m\n",
      "\u001b[34m[799]#011train-logloss:0.04123#011validation-logloss:0.10367\u001b[0m\n",
      "\n",
      "2024-09-29 20:34:09 Completed - Training job completed\n",
      "Training seconds: 109\n",
      "Billable seconds: 109\n"
     ]
    }
   ],
   "source": [
    "#add_your_task_3_3_code_here\n",
    "\n",
    "\n",
    "from sagemaker import image_uris\n",
    "#train-model\n",
    "# Retrieve the container image\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    region=boto3.Session().region_name, \n",
    "    framework=\"xgboost\", \n",
    "    version=\"1.5-1\"\n",
    ")\n",
    "\n",
    "# Set the hyperparameters\n",
    "eta=0.2\n",
    "gamma=4\n",
    "max_depth=5\n",
    "min_child_weight=6\n",
    "num_round=800\n",
    "objective='binary:logistic'\n",
    "subsample=0.8\n",
    "\n",
    "hyperparameters = {\n",
    "    \"eta\":eta,\n",
    "    \"gamma\":gamma,\n",
    "    \"max_depth\":max_depth,\n",
    "    \"min_child_weight\":min_child_weight,\n",
    "    \"num_round\":num_round,\n",
    "    \"objective\":objective,\n",
    "    \"subsample\":subsample\n",
    "}\n",
    "\n",
    "# Set up the estimator\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,    \n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    max_run=1800,\n",
    "    hyperparameters=hyperparameters,\n",
    "    tags = run_tags\n",
    ")\n",
    "\n",
    "with Run(\n",
    "    experiment_name=capstone_experiment_name,\n",
    "    run_name=capstone_run_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ") as run:\n",
    "    run.log_parameter(\"eta\", eta)\n",
    "    run.log_parameter(\"gamma\", gamma)\n",
    "    run.log_parameter(\"max_depth\", max_depth)\n",
    "    run.log_parameter(\"min_child_weight\", min_child_weight)\n",
    "    run.log_parameter(\"objective\", objective)\n",
    "    run.log_parameter(\"subsample\", subsample)\n",
    "    run.log_parameter(\"num_round\", num_round)\n",
    "\n",
    "# Train the model associating the training run with the current \"experiment\"\n",
    "    xgb.fit(\n",
    "        inputs = data_inputs\n",
    "    )        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4: Hyperparameter tuning\n",
    "\n",
    "<a id=\"task3-4-continue\"></a>\n",
    "\n",
    "Now that you have finished a training job and analyzed it, tune hyperparameter ranges based on your findings and run more training jobs to improve the model.\n",
    "\n",
    "**Hint 1**: To get started, set the hyperparameter ranges for **alpha**, **eta**, **max_depth**, **min_child_weight**, and **num_round**.\n",
    "\n",
    "**Hint 2**: When you run **HyperparameterTuner**, make sure to set the **objective_metric_name** and **objective_type** based on your findings.\n",
    "\n",
    "**Hint 3**: To check if you have improved over the SageMaker Autopilot results, open the **Experiments and runs** menu in **SageMaker resources**. In your run, view the **Metrics**. The **ObjectiveMetric** should be higher than the **F1** score of **0.616** and the **validation:auc** should have a **Final value** higher than the SageMaker Autopilot score of **0.918**.\n",
    "\n",
    "When you tuned your hyperparameters, which one caused the greatest improvement in model performance?\n",
    "\n",
    "For detailed steps how to configure training hyperparameter ranges, refer to <a href=\"#task3-4\" target=\"_self\">**Configure training hyperparameter ranges (Task 3.4)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have configured the training hyperparameter ranges and started more training jobs, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating hyperparameter tuning job with name: sagemaker-xgboost-240929-2111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................................!\n"
     ]
    }
   ],
   "source": [
    "#add_your_task_3_4_code_here\n",
    "\n",
    "#tune-model\n",
    "# Setup the hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    \"alpha\": ContinuousParameter(0, 2),\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "    \"min_child_weight\": ContinuousParameter(1, 10),\n",
    "    \"num_round\": IntegerParameter(100, 1000)\n",
    "}\n",
    "\n",
    "# Define the target metric and the objective type (max/min)\n",
    "objective_metric_name = \"validation:auc\"\n",
    "objective_type=\"Maximize\"\n",
    "\n",
    "# Define the HyperparameterTuner\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator = xgb,\n",
    "    objective_metric_name = objective_metric_name,\n",
    "    hyperparameter_ranges = hyperparameter_ranges,\n",
    "    objective_type = objective_type,\n",
    "    max_jobs=12,\n",
    "    max_parallel_jobs=4,\n",
    "    early_stopping_type=\"Auto\"\n",
    ")\n",
    "\n",
    "# Tune the model\n",
    "tuner.fit(\n",
    "    inputs = data_inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Evaluate the model for bias\n",
    "\n",
    "Now that your model is trained, evaluate your model using Amazon SageMaker Clarify. If you find any issues, you can remove the imbalance detected and retrain the model.\n",
    "\n",
    "To complete this task, you complete the following subtasks:\n",
    "\n",
    "- Create a model from the training job.\n",
    "- Create a SageMaker Clarify model configuration.\n",
    "- Create a SageMaker Clarify bias configuration.\n",
    "- Use a SageMaker Clarify processor job to run the bias, data, and model reports.\n",
    "- Remove the imbalance detected with SageMaker Clarify (optional).\n",
    "- Retrain the model (optional).\n",
    "\n",
    "This challenge takes approximately *80* minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Create a model from the training job\n",
    "\n",
    "<a id=\"task4-1-continue\"></a>\n",
    "\n",
    "Create an XGBoost model, calling **create_model** with the **model_name**, **role**, and **container_def** that you define.\n",
    "\n",
    "**Hint 1**: Call **xgb.create_model()** and pick a name for your model.\n",
    "\n",
    "**Hint 2**: Use your session and call **create_model**, passing in the **model_name**, **role**, and **container_def**.\n",
    "\n",
    "For detailed steps how to create a model, refer to <a href=\"#task4-1\" target=\"_self\">**Create a model (Task 4.1)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have created a model, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_4_1_code_here\n",
    "#model-configurations\n",
    "model_name = \"capstone-clarify-model\"\n",
    "model = xgb.create_model(name=model_name)\n",
    "container_def = model.prepare_container_def()\n",
    "sagemaker_session.create_model(model_name, role, container_def)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Create a SageMaker Clarify model configuration\n",
    "\n",
    "<a id=\"task4-2-continue\"></a>\n",
    "\n",
    "Create a SageMaker Clarify model configuration using **SageMakerClarifyProcessor**.\n",
    "\n",
    "**Hint 1**: Set the **instance_count** and the **instance_type**.\n",
    "\n",
    "**Hint 2**: Use the **role** and **session** created at the beginning of the Capstone lab.\n",
    "\n",
    "For detailed steps how to create a SageMaker Clarify model configuration, refer to <a href=\"#task4-2\" target=\"_self\">**Create a SageMaker Clarify model configuration (Task 4.2)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have created a SageMaker Clarify model configuration, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_4_2_code_here\n",
    "\n",
    "#define-clarify-processor\n",
    "clarify_processor = clarify.SageMakerClarifyProcessor(\n",
    "    role=role, \n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.m5.xlarge\", \n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.3: Create a SageMaker Clarify bias configuration\n",
    "\n",
    "<a id=\"task4-3-continue\"></a>\n",
    "\n",
    "Create a data configuration, a model configuration, a label configuration, and a bias configuration.\n",
    "\n",
    "**Hint 1**: Start with a **DataConfig**, setting the input path, output path, headers, and dataset type.\n",
    "\n",
    "**Hint 2**: Then, create a **ModelConfig**, choosing the content and accept type, the model name, the instance type, and the instance count.\n",
    "\n",
    "**Hint 3**: Next, create a **ModelPredictedLabelConfig**, setting the probability threshold.\n",
    "\n",
    "**Hint 4**: Finally, create a **BiasConfig**, setting the label values or threshold, the facet name, and the facet values or threshold.\n",
    "\n",
    "Which facets do you want to explore first in your bias report? Are there any features that are particularly susceptible to bias?\n",
    "\n",
    "For detailed steps how to create a SageMaker Clarify bias configuration, refer to <a href=\"#task4-3\" target=\"_self\">**Create a SageMaker Clarify bias configuration (Task 4.3)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have created a SageMaker Clarify bias configuration, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_4_3_code_here\n",
    "\n",
    "#define-data-config\n",
    "bias_report_output_path = \"s3://{}/{}/clarify-bias\".format(bucket, prefix)\n",
    "bias_data_config = clarify.DataConfig(\n",
    "    s3_data_input_path=train_path,\n",
    "    s3_output_path=bias_report_output_path,\n",
    "    label=\"fraud\",\n",
    "    headers=train_data.columns.to_list(),\n",
    "    dataset_type=\"text/csv\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-model-config\n",
    "model_config = clarify.ModelConfig(\n",
    "    model_name=model_name,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    accept_type=\"text/csv\",\n",
    "    content_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-label-config\n",
    "predictions_config = clarify.ModelPredictedLabelConfig(probability_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-bias-config\n",
    "bias_config = clarify.BiasConfig(\n",
    "    label_values_or_threshold=[1], facet_name=\"customer_gender_female\", facet_values_or_threshold=[0], group_name=\"customer_age\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.4: Use a SageMaker Clarify processor job to run the bias, data, and model reports\n",
    "\n",
    "<a id=\"task4-4-continue\"></a>\n",
    "\n",
    "You chose all your configurations already for the bias, data, and model reports. Now, run the reports.\n",
    "\n",
    "**Hint 1**: Pass the **data_config**, **bias_config**, **model_predicted_label_config**, and **model_config** values to **run_bias**.\n",
    "\n",
    "**Hint 2**: You need to set the **pre_training_methods** and **post_training_methods**.\n",
    "\n",
    "For detailed steps how to run bias, data, and model reports using SageMaker Clarify, refer to <a href=\"#task4-4\" target=\"_self\">**Run bias, data, and model reports using SageMaker Clarify (Task 4.4)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have used a SageMaker Clarify processor job to run the bias, data, and model reports, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_4_4_code_here\n",
    "#run-bias-report\n",
    "clarify_processor.run_bias(\n",
    "    data_config=bias_data_config,\n",
    "    bias_config=bias_config,\n",
    "    model_config=model_config,\n",
    "    model_predicted_label_config=predictions_config,\n",
    "    pre_training_methods=\"all\",\n",
    "    post_training_methods=\"all\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.5: Remove the imbalance detected with SageMaker Clarify (optional)\n",
    "\n",
    "<a id=\"task4-5-continue\"></a>\n",
    "\n",
    "There are many ways to remove the imbalance detected with SageMaker Clarify. Use any method that you are familiar with. In this lab, a Synthetic Minority Over-sampling Technique (SMOTE) example is provided that removes bias from one of the columns.\n",
    "\n",
    "**Hint 1**: After you are finished removing the imbalance and want to retest, create a new resampled dataframe. You create and upload a new CSV file in the next task.\n",
    "\n",
    "For detailed steps how to remove imbalance, refer to <a href=\"#task4-5\" target=\"_self\">**Remove imbalance (Task 4.5)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have removed any imbalance detected with SageMaker Clarify, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_4_5_code_here\n",
    "#display-summary\n",
    "gender = train_data[\"customer_gender_female\"]\n",
    "gender.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove-imbalance\n",
    "sm = SMOTE(random_state=42)\n",
    "train_data_upsampled, gender_res = sm.fit_resample(train_data, gender)\n",
    "train_data_upsampled[\"customer_gender_female\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.6: Retrain the model (optional)\n",
    "\n",
    "<a id=\"task4-6-continue\"></a>\n",
    "\n",
    "Upload the new file to Amazon S3. Then, create a new estimator and retrain with the new data.\n",
    "\n",
    "**Hint 1**: Use **s3_client.upload_file** to upload the new file to your bucket.\n",
    "\n",
    "**Hint 2**: Use **xgboost_starter_script.py** and call **XGBoost**. Then, retrain with the new data.\n",
    "\n",
    "Did the retrained model achieve a higher F1 score? If you used SageMaker Debugger, were you able to resolve all of the discovered issues?\n",
    "\n",
    "For detailed steps how to retrain the model, refer to <a href=\"#task4-6\" target=\"_self\">**Retrain the model (Task 4.6)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have retrained the model, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_4_6_code_here\n",
    "#upload-upsampled-csv\n",
    "train_data_upsampled.to_csv(\"data/upsampled_train.csv\", index=False, header=False)\n",
    "retrain_path = S3Uploader.upload(\"data/upsampled_train.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "retrain_input = TrainingInput(retrain_path, content_type=\"text/csv\")\n",
    "\n",
    "retrain_data_inputs = {\n",
    "    \"train\": retrain_input,\n",
    "    \"validation\": validation_input\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create-estimator\n",
    "hyperparameters= {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"300\",\n",
    "}\n",
    "\n",
    "xgb_retrained = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role, \n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain-upsampled-data\n",
    "xgb_retrained.fit(\n",
    "    inputs = retrain_data_inputs\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 5: Batch transform\n",
    "\n",
    "Your model is ready for deployment. Use a batch transform job with batch records and view the prediction and accuracy data in Amazon S3. Then, clean up some of your SageMaker instances.\n",
    "\n",
    "To complete this task, you complete the following subtasks:\n",
    "\n",
    "- Create a batch transform job for your model.\n",
    "- View the prediction data in Amazon S3.\n",
    "- Clean up SageMaker instances (optional).\n",
    "\n",
    "This challenge takes approximately *40* minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Create a batch transform job for your model\n",
    "\n",
    "<a id=\"task5-1-continue\"></a>\n",
    "\n",
    "Create a batch transform job using **transformer** on the model estimator. Then, run the batch job.\n",
    "\n",
    "**Hint 1**: Use transformer and configure the **instance_count**, **instance_type**, **strategy**, **assemble_with**, and **output_path**.\n",
    "\n",
    "**Hint 2**: Send your test data, which is listed in **test_path** to the endpoint and wait for the results.\n",
    "\n",
    "For detailed steps how to create a batch transform job, refer to <a href=\"#task5-1\" target=\"_self\">**Create a batch transform job (Task 5.1)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have created a batch transform job and run it with a set of records, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_5_1_code_here\n",
    "#create-batch-transformer\n",
    "# Use the retrained model if it exists, otherwise, use the original model\n",
    "try:\n",
    "    model = xgb_retrained\n",
    "except NameError:\n",
    "    model = xgb\n",
    "\n",
    "# Create the transformer\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    strategy=\"MultiRecord\",\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",\n",
    "    output_path=\"s3://{}/{}/batch-transform/\".format(bucket, prefix)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run-batch-transform-job\n",
    "test_data_batch = test_data.drop(\"fraud\", axis=1)\n",
    "test_data_batch.to_csv(\"test_data_batch.csv\", index=False, header=False)\n",
    "test_path_batch = S3Uploader.upload(\"test_data_batch.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "\n",
    "transformer.transform(test_path_batch, content_type=\"text/csv\", split_type=\"Line\", join_source=\"Input\")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: View the prediction data in Amazon S3\n",
    "\n",
    "<a id=\"task5-2-continue\"></a>\n",
    "\n",
    "When your batch transform job is complete, read the data from Amazon S3. \n",
    "\n",
    "**Hint 1**: You can copy data from the transformer output using **%aws s3 cp --recursive $transformer.output_path ./**\n",
    "\n",
    "**Hint 2**: When you have the data, you can view it with **%head test_data_batch.csv.out**\n",
    "\n",
    "Take a look at the predictions. Are there any predictions that are surprising?\n",
    "\n",
    "For detailed steps how to view prediction data from a batch transform job, refer to <a href=\"#task5-2\" target=\"_self\">**View prediction data from a batch transform job (Task 5.2)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have viewed the prediction data from the batch transform job, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_5_2_code_here\n",
    "\n",
    "!aws s3 cp --recursive $transformer.output_path ./\n",
    "test_data = pd.read_csv(\"test_data_batch.csv.out\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.3: Clean up SageMaker instances (optional)\n",
    "\n",
    "To keep costs low, best practice is to delete instances that you are not using anymore. You can quickly delete instances using SageMaker Studio. Take a moment now to open your current resources list in SageMaker Studio and close out any remaining instances.\n",
    "\n",
    "If you plan to complete the next pipeline task, **leave the notebook instance running**.\n",
    "\n",
    "**Hint 1**: You can view a list of running instances by choosing the **Running Terminals and Kernels** icon in SageMaker Studio.\n",
    "\n",
    "**Hint 2**: You can use the **Shut down** icon to stop an instance.\n",
    "\n",
    "<a id=\"task5-3-continue\"></a>\n",
    "\n",
    "For detailed steps how to clean up SageMaker instances in SageMaker Studio, refer to <a href=\"#task5-3\" target=\"_self\">**Clean up SageMaker instances in SageMaker Studio (Task 5.3)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have stopped all SageMaker instances in SageMaker Studio, you have completed this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 6: Build an automated pipeline (optional)\n",
    "\n",
    "Now that you have used the Amazon SageMaker Python SDK and the Amazon SageMaker Studio for a machine learning (ML) workflow, use SageMaker Pipelines to scale your workflow. Proceed through the provided pipeline script in the lab environment to complete this challenge. \n",
    "\n",
    "- Create the pipeline steps.\n",
    "    - Query processed data from SageMaker Feature Store.\n",
    "    - Train and tune the model.\n",
    "    - Evaluate the trained model.\n",
    "    - Conduct a batch transform job.\n",
    "    - Register a model.\n",
    "    - Evaluate model training with SageMaker Clarify.\n",
    "- Define and start the pipeline.\n",
    "- View ML Lineage Tracking.\n",
    "\n",
    "This challenge takes approximately *120* minutes to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.1: Configure a pipeline\n",
    "\n",
    "<a id=\"task6-1-continue\"></a>\n",
    "\n",
    "Use a pipeline template and configure your inputs and outputs. When your configurations are ready, run the pipeline. Your pipeline can include a wide range of steps. Here is a suggested list of steps to configure:\n",
    "- **AutoModelProcess**: A **Processing** step that pulls in the .csv file and splits it into train, test, and validation datasets.\n",
    "- **AutoHyperParameterTuning**: A **Tuning** step that takes a range of hyperparameters and tunes the model.\n",
    "- **AutoEvalBestModel**: A **Processing** step that creates an evaluation report to describe the best model.\n",
    "- **CheckAUCScoreAutoEvaluation**: A **Condition** step that evaluates the models based on an evaluation metric. \n",
    "- **AutoCreateModel**: A **Model** step that creates a model.\n",
    "- **RegisterAutoModel-RegisterModel**: A **RegisterModel** step that registers a model.\n",
    "- **AutoModelConfigFile**: A **Processing** step that creates a bias report.\n",
    "- **AutoTransform**: A **Transform** step that runs a batch transform job.\n",
    "- **ClarifyProcessingStep**: A **Processing** step that runs a SageMaker Clarify job.\n",
    "\n",
    "**Hint 1**: There are many pipeline steps to pick from. To learn more about pipeline steps and view sample code for each step, refer to the *Build and manage pipeline steps* document in the *Additional resources* section for more information.\n",
    "\n",
    "**Hint 2**: Start with a **Processing** step to pull your data in. Then, create a **Tuning** step to tune your model. Next, create a **Model** step to create your model.\n",
    "\n",
    "**Hint 3**: The detailed steps contain a sample solution, and includes steps to create evaluation reports, a bias report, run a batch transform job, and run a SageMaker Clarify job. \n",
    "\n",
    "For detailed steps how to configure a pipeline, refer to <a href=\"#task6-1\" target=\"_self\">**Configure a pipeline (Task 6.1)**</a> in the *Appendix* section.\n",
    "\n",
    "After you have set up the pipeline and started the pipeline job, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_6_1_code_here\n",
    "#run-pipeline\n",
    "# Set the variables\n",
    "model_name = \"Auto-model\"\n",
    "sklearn_processor_version=\"0.23-1\"\n",
    "model_package_group_name=\"AutoModelPackageGroup\"\n",
    "pipeline_name= \"AutoModelSMPipeline\"\n",
    "clarify_image = sagemaker.image_uris.retrieve(framework='sklearn',version=sklearn_processor_version,region=region)\n",
    "\n",
    "# Upload files to the default S3 bucket\n",
    "s3_client.put_object(Bucket=bucket,Key='data/')\n",
    "s3_client.put_object(Bucket=bucket,Key='input/code/')\n",
    "s3_client.upload_file(Filename=\"data/batch_data.csv\", Bucket=bucket, Key=\"data/batch_data.csv\")  #If you edit this, make sure to also edit the headers listed in generate_config to match your column names.\n",
    "s3_client.upload_file(Filename=\"data/claims_customer.csv\", Bucket=bucket, Key=\"data/claims_customer.csv\")  #If you edit this, make sure to also edit the headers listed in generate_config to match your column names.\n",
    "s3_client.upload_file(Filename=\"pipelines/evaluate.py\", Bucket=bucket, Key=\"input/code/evaluate.py\")\n",
    "s3_client.upload_file(Filename=\"pipelines/generate_config.py\", Bucket=bucket, Key=\"input/code/generate_config.py\")\n",
    "s3_client.upload_file(Filename=\"pipelines/preprocess.py\", Bucket=bucket, Key=\"input/code/preprocess.py\")\n",
    "\n",
    "# Configure important settings. Change the input_data if you want to\n",
    "# use a file other than the claims_customer.csv and batch_data.csv files.\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "processing_instance_type = ParameterString(\n",
    "        name=\"ProcessingInstanceType\",\n",
    "        default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "training_instance_type = ParameterString(\n",
    "        name=\"TrainingInstanceType\",\n",
    "        default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "input_data = ParameterString(\n",
    "        name=\"InputData\",\n",
    "        default_value=\"s3://{}/data/claims_customer.csv\".format(bucket), \n",
    ")\n",
    "batch_data = ParameterString(\n",
    "        name=\"BatchData\",\n",
    "        default_value=\"s3://{}/data/batch_data.csv\".format(bucket),\n",
    ")\n",
    "\n",
    "# Run a scikit-learn script to do data processing on SageMaker using \n",
    "# using the SKLearnProcessor class\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "        framework_version=sklearn_processor_version,\n",
    "        instance_type=processing_instance_type.default_value, \n",
    "        instance_count=processing_instance_count,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    ")\n",
    "\n",
    "# Configure the processing step to pull in the input_data\n",
    "step_process = ProcessingStep(\n",
    "        name=\"AutoModelProcess\",\n",
    "        processor=sklearn_processor,\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\",\\\n",
    "                             destination=f\"s3://{bucket}/output/train\" ),\n",
    "            ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\",\\\n",
    "                            destination=f\"s3://{bucket}/output/validation\"),\n",
    "            ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\",\\\n",
    "                            destination=f\"s3://{bucket}/output/test\"),\n",
    "            ProcessingOutput(output_name=\"batch\", source=\"/opt/ml/processing/batch\",\\\n",
    "                            destination=f\"s3://{bucket}/data/batch\"),\n",
    "            ProcessingOutput(output_name=\"baseline\", source=\"/opt/ml/processing/baseline\",\\\n",
    "                            destination=f\"s3://{bucket}/input/baseline\")\n",
    "        ],\n",
    "        code=f\"s3://{bucket}/input/code/preprocess.py\",\n",
    "        job_arguments=[\"--input-data\", input_data],\n",
    ")\n",
    "\n",
    "# Set up the model path, image uri, and hyperparameters for the estimator\n",
    "model_path = f\"s3://{bucket}/output\"\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.5-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=training_instance_type.default_value,\n",
    ")\n",
    "\n",
    "fixed_hyperparameters = {\n",
    "    \"eval_metric\":\"auc\",\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"num_round\":\"100\",\n",
    "    \"rate_drop\":\"0.3\",\n",
    "    \"tweedie_variance_power\":\"1.4\"\n",
    "}\n",
    "\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    hyperparameters=fixed_hyperparameters,\n",
    "    output_path=model_path,\n",
    "    base_job_name=f\"auto-train\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "# Set the hyperparameter ranges for the tuning step and configure the tuning step\n",
    "hyperparameter_ranges = {\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "    \"min_child_weight\": ContinuousParameter(1, 10),\n",
    "    \"alpha\": ContinuousParameter(0, 2),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "}\n",
    "objective_metric_name = \"validation:auc\"\n",
    "\n",
    "step_tuning = TuningStep(\n",
    "    name = \"AutoHyperParameterTuning\",\n",
    "    tuner = HyperparameterTuner(xgb_train, objective_metric_name, hyperparameter_ranges, max_jobs=2, max_parallel_jobs=2),\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Configure the processing step for evaluation\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-auto-eval\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"AutoEvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"AutoEvalBestModel\",\n",
    "    processor=script_eval,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_tuning.get_top_model_s3_uri(top_k=0,s3_bucket=bucket,prefix=\"output\"),\n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\",\\\n",
    "                            destination=f\"s3://{bucket}/output/evaluation\"),\n",
    "    ],\n",
    "    code=f\"s3://{bucket}/input/code/evaluate.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")\n",
    "\n",
    "# Configure model creation\n",
    "model = Model(\n",
    "    image_uri=image_uri,        \n",
    "    model_data=step_tuning.get_top_model_s3_uri(top_k=0,s3_bucket=bucket,prefix=\"output\"),\n",
    "    name=model_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    accelerator_type=\"ml.inf1.xlarge\",\n",
    ")\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"AutoCreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    ")\n",
    "\n",
    "script_processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=clarify_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=processing_instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "bias_report_output_path = f\"s3://{bucket}/clarify-output/bias\"\n",
    "clarify_instance_type = 'ml.m5.xlarge'\n",
    "step_config_file = ProcessingStep(\n",
    "    name=\"AutoModelConfigFile\",\n",
    "    processor=script_processor,\n",
    "    code=f\"s3://{bucket}/input/code/generate_config.py\",\n",
    "    job_arguments=[\"--modelname\",step_create_model.properties.ModelName,\"--bias-report-output-path\",bias_report_output_path,\"--clarify-instance-type\",clarify_instance_type,\\\n",
    "                  \"--default-bucket\",bucket,\"--num-baseline-samples\",\"50\",\"--instance-count\",\"1\"],\n",
    "    depends_on= [step_create_model.name]\n",
    ")\n",
    "\n",
    "# Configure the step to perform a batch transform job\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",    \n",
    "    output_path=f\"s3://{bucket}/AutoTransform\"\n",
    ")\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"AutoTransform\",\n",
    "    transformer=transformer,\n",
    "    inputs=TransformInput(data=batch_data,content_type=\"text/csv\",join_source=\"Input\",split_type=\"Line\")\n",
    ")\n",
    "\n",
    "# Configure the SageMaker Clarify processing step\n",
    "analysis_config_path = f\"s3://{bucket}/clarify-output/bias/analysis_config.json\"\n",
    "\n",
    "data_config = sagemaker.clarify.DataConfig(\n",
    "    s3_data_input_path=f's3://{bucket}/output/train/train.csv', \n",
    "    s3_output_path=bias_report_output_path,\n",
    "    label=0,\n",
    "    headers=list(pd.read_csv(\"./data/claims_customer.csv\", index_col=None).columns), #If you edit this, make sure to also edit the headers listed in generate_config to match your column names.\n",
    "    dataset_type=\"text/csv\",\n",
    ")\n",
    "\n",
    "clarify_processor = sagemaker.clarify.SageMakerClarifyProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=clarify_instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "config_input = ProcessingInput(\n",
    "    input_name=\"analysis_config\",\n",
    "    source=analysis_config_path,\n",
    "    destination=\"/opt/ml/processing/input/analysis_config\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_compression_type=\"None\",\n",
    ")\n",
    "\n",
    "data_input = ProcessingInput(\n",
    "    input_name=\"dataset\",\n",
    "    source=data_config.s3_data_input_path,\n",
    "    destination=\"/opt/ml/processing/input/data\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=data_config.s3_data_distribution_type,\n",
    "    s3_compression_type=data_config.s3_compression_type,\n",
    ")\n",
    "\n",
    "result_output = ProcessingOutput(\n",
    "    source=\"/opt/ml/processing/output\",\n",
    "    destination=data_config.s3_output_path,\n",
    "    output_name=\"analysis_result\",\n",
    "    s3_upload_mode=\"EndOfJob\",\n",
    ")\n",
    "\n",
    "step_clarify = ProcessingStep(\n",
    "    name=\"ClarifyProcessingStep\",\n",
    "    processor=clarify_processor,\n",
    "    inputs= [data_input, config_input],\n",
    "    outputs=[result_output],\n",
    "    depends_on = [step_config_file.name]\n",
    ")\n",
    "\n",
    "# Configure the model registration step\n",
    "model_statistics = MetricsSource(\n",
    "    s3_uri=\"s3://{}/output/evaluation/evaluation.json\".format(bucket),\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "explainability = MetricsSource(\n",
    "    s3_uri=\"s3://{}/clarify-output/bias/analysis.json\".format(bucket),\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "\n",
    "bias = MetricsSource(\n",
    "    s3_uri=\"s3://{}/clarify-output/bias/analysis.json\".format(bucket),\n",
    "    content_type=\"application/json\"\n",
    ") \n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=model_statistics,\n",
    "    explainability=explainability,\n",
    "    bias=bias\n",
    ")\n",
    "\n",
    "step_register = RegisterModel(\n",
    "    name=\"RegisterAutoModel\",\n",
    "    estimator=xgb_train,\n",
    "    model_data=step_tuning.get_top_model_s3_uri(top_k=0,s3_bucket=bucket,prefix=\"output\"),\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "\n",
    "# Create the model evaluation step\n",
    "cond_lte = ConditionGreaterThan(\n",
    "    left=JsonGet(\n",
    "        step=step_eval,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"binary_classification_metrics.auc.value\"\n",
    "    ),\n",
    "    right=0.75,\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"CheckAUCScoreAutoEvaluation\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_create_model,step_config_file,step_transform,step_clarify,step_register],\n",
    "    else_steps=[],\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "def get_pipeline(\n",
    "    region,\n",
    "    role=None,\n",
    "    default_bucket=None,\n",
    "    model_package_group_name=\"AutoModelPackageGroup\",\n",
    "    pipeline_name=\"AutoModelPipeline\",\n",
    "    base_prefix = None,\n",
    "    custom_image_uri = None,\n",
    "    sklearn_processor_version=None\n",
    "    ):\n",
    "    \"\"\"Gets a SageMaker ML Pipeline instance working with auto data.\n",
    "    Args:\n",
    "        region: AWS region to create and run the pipeline.\n",
    "        role: IAM role to create and run steps and pipeline.\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        an instance of a pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    # pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[\n",
    "            processing_instance_type,\n",
    "            processing_instance_count,\n",
    "            training_instance_type,\n",
    "            input_data,\n",
    "            batch_data,\n",
    "        ],\n",
    "        steps=[step_process,step_tuning,step_eval,step_cond],\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = get_pipeline(\n",
    "    region = region,\n",
    "    role=role,\n",
    "    default_bucket=bucket,\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    pipeline_name=pipeline_name,\n",
    "    custom_image_uri=clarify_image,\n",
    "    sklearn_processor_version=sklearn_processor_version\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n",
    "\n",
    "# Run the pipeline\n",
    "RunPipeline = pipeline.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.2: Monitor the pipeline\n",
    "\n",
    "<a id=\"task6-2-continue\"></a>\n",
    "\n",
    "Monitor the pipeline while its running, viewing the inputs and outputs. \n",
    "\n",
    "**Hint 1**: Use `RunPipeline.describe()` to describe the pipeline that you just created.\n",
    "\n",
    "**Hint 2**: You can view the pipeline steps running in the SageMaker Studio UI. Open the **SageMaker resources** menu, choose **Pipelines**, and choose the pipeline that you created. \n",
    "\n",
    "For detailed steps how to monitor a pipeline, refer to <a href=\"#task6-2\" target=\"_self\">**Monitor a pipeline**</a> in the *Appendix* section.\n",
    "\n",
    "After you have finished monitoring the pipeline, you have completed this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_your_task_6_2_code_here\n",
    "#describe-pipeline\n",
    "RunPipeline.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list-pipeline-steps\n",
    "RunPipeline.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove-pipeline\n",
    "response = sagemaker_session.boto_session.client(\"sagemaker\", region_name=region).delete_pipeline(PipelineName='AutoModelSMPipeline')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You used an auto insurance dataset to detect claims that are possibly fraudulent. You explored a technical solution to predict the likelihood that a given auto insurance claim is fraudulent using SageMaker Studio and the Amazon SageMaker Python SDK.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with the **Conclusion**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "- [Autopilot metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html)\n",
    "- [Processing step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1-1\" id=\"task1-1\"></a>\n",
    "\n",
    "### Appendix: Review your data (Task 1.1)\n",
    "\n",
    "To review your data, specify the path and load the data using Pandas. Take a moment to review a sample of both tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read-csv-files\n",
    "claims_data = pd.read_csv(\"./data/claims_preprocessed.csv\", index_col=0)\n",
    "customers_data = pd.read_csv(\"./data/customers_preprocessed.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#claims-data-sample\n",
    "claims_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers-data-sample\n",
    "customers_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task1-1-continue\" target=\"_self\">Task 1.1</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1-2-1\" id=\"task1-2-1\"></a>\n",
    "\n",
    "### Appendix: Explore a dataset in SageMaker Studio (Task 1.2)\n",
    "\n",
    "Start your data exploration in SageMaker Data Wrangler. Import your files from your S3 bucket and analyze the data.\n",
    "\n",
    "The next step will bring you to a new tab in SageMaker Studio. To follow these directions, use one of the following options:\n",
    "- **Option 1:** View the tabs side by side. To create a split screen view from the main SageMaker Studio window, either drag the **capstone.ipynb** tab to the side or choose the **capstone.ipynb** tab, and then from the toolbar, select **File** and **New View for Notebook**. You can now have the directions visible as you explore the feature group.\n",
    "- **Option 2:** Switch between the SageMaker Studio tabs to follow these instructions.\n",
    "\n",
    "1. On the left side of SageMaker Studio, choose the **Home** icon.\n",
    "1. Expand the **Data** section, and then choose **Data Wrangler**.\n",
    "\n",
    "SageMaker Studio opens the **Data Wrangler** tab.\n",
    "\n",
    "1. Choose **+** **Create Data Wrangler flow**.\n",
    "\n",
    "SageMaker Studio opens the **untitled.flow** tab.\n",
    "\n",
    "1. Wait until the **untitled.flow** tab is finished loading, indicated by a progress bar. It might take 23 minutes.\n",
    "\n",
    "SageMaker Studio opens the **Create connection** page inside of the *Data Wrangler* tab.\n",
    "\n",
    "1. Open the context (right-click) menu on the **untitled.flow** file tab, and then, to change the file name, choose **Rename Data Wrangler Flow...**.\n",
    "\n",
    "SageMaker Studio opens the **Rename File** message window.\n",
    "\n",
    "1. For **New Name**, enter `CapstoneDataWrangler.flow`.\n",
    "1. Choose <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">Rename</span>.\n",
    "\n",
    "The **Rename File** message window is closed.\n",
    "\n",
    "1. In the *CapstoneDataWrangler.flow* tab, in the **Data sources** section, choose **Amazon S3**.\n",
    "\n",
    "SageMaker Studio opens the **Import a dataset from S3** page inside of the *DataWrangler.flow* tab.\n",
    "\n",
    "1. In the list of buckets, open the bucket that contains **databucket** in its name.\n",
    "1. Choose the first dataset, a file named **claims.csv**.\n",
    "\n",
    "1. Choose <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">Import</span>.\n",
    "\n",
    "1. Return to the **Data flow** view, choose the **< Data flow** located at the top left of the *CapstoneDataWrangler.flow* tab.\n",
    "1. Select the **Import** tab, located at the top left of the *CapstoneDataWrangler.flow* tab.\n",
    "\n",
    "SageMaker Studio opens the **Create connection** page.\n",
    "\n",
    "1. In the *CapstoneDataWrangler.flow* tab, in the **Data sources** section, choose **Amazon S3**.\n",
    "\n",
    "SageMaker Studio opens the **Import a dataset from S3** page inside of the *DataWrangler.flow* tab.\n",
    "\n",
    "1. In the list of buckets, open the bucket that contains **databucket** in its name.\n",
    "1. Choose the second dataset, a file named **customers.csv**.\n",
    "1. Choose <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">Import</span>.\n",
    "\n",
    "1. Return to the **Data flow** view, choose the **< Data flow** located at the top left of the *CapstoneDataWrangler.flow* tab.\n",
    "\n",
    "1. In the Data flow tab, choose the **+** sign next to the **Data types** icon and choose **Get data insights**.\n",
    "\n",
    "1. Create the report and explore the insights.\n",
    "\n",
    "1. Return to the **Data flow** tab, choose the **+** sign next to the **Data types** icon, and choose **Add analysis**.\n",
    "\n",
    "1. Create the analysis and explore the results.\n",
    "\n",
    "Use any report types that help you explore the datasets thoroughly. After you finish, you can continue with the next task.\n",
    "\n",
    "To continue this lab, return to <a href=\"#task1-2-continue\" target=\"_self\">Task 1.2</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1-2-2\" id=\"task1-2-2\"></a>\n",
    "\n",
    "### Appendix: Explore a dataset in the notebook (Task 1.2)\n",
    "\n",
    "There are many ways in which you can explore datasets. Here are several examples of some data exploration steps that you can take. Use these as a reference to start exploring aspects of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gender-graph\n",
    "import matplotlib.pyplot as plt\n",
    "customers_data.customer_gender_female.value_counts(normalize=True).plot.bar()\n",
    "plt.xticks([0, 1], [\"Male\", \"Female\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fraud-graph\n",
    "claims_data.fraud.value_counts(normalize=True).plot.bar()\n",
    "plt.xticks([0, 1], [\"Not Fraud\", \"Fraud\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#education-category-graphs\n",
    "educ = customers_data.customer_education.value_counts(normalize=True, sort=False)\n",
    "plt.bar(educ.index, educ.values)\n",
    "plt.xlabel(\"Customer Education Level\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#claim-amount-graph\n",
    "plt.hist(claims_data.total_claim_amount, bins=30)\n",
    "plt.xlabel(\"Total Claim Amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#claims-filed-graph\n",
    "customers_data.num_claims_past_year.hist(density=True)\n",
    "plt.suptitle(\"Number of Claims in the Past Year\")\n",
    "plt.xlabel(\"Number of claims per year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paid-plot-graphs\n",
    "sns.pairplot(\n",
    "    data=customers_data, vars=[\"num_insurers_past_5_years\", \"months_as_customer\", \"customer_age\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fraud-insurers-graph\n",
    "combined_data = customers_data.join(claims_data)\n",
    "sns.lineplot(x=\"num_insurers_past_5_years\", y=\"fraud\", data=combined_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#months-as-customer-graph\n",
    "sns.boxplot(x=customers_data[\"months_as_customer\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customer-age-graph\n",
    "sns.boxplot(x=customers_data[\"customer_age\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fraud-gender-graph\n",
    "combined_data.groupby(\"customer_gender_female\").mean()[\"fraud\"].plot.bar()\n",
    "plt.xticks([0, 1], [\"Male\", \"Female\"])\n",
    "plt.suptitle(\"Fraud by Gender\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation-matrix-graph\n",
    "cols = [\n",
    "    \"fraud\",\n",
    "    \"customer_gender_male\",\n",
    "    \"customer_gender_female\",\n",
    "    \"months_as_customer\",\n",
    "    \"num_insurers_past_5_years\",\n",
    "]\n",
    "corr = combined_data[cols].corr()\n",
    "\n",
    "# plot the correlation matrix\n",
    "sns.heatmap(corr, annot=True, cmap=\"Reds\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load-combined-data\n",
    "combined_data = pd.read_csv(\"./data/claims_customer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove-unnecessary-columns\n",
    "combined_data = combined_data.loc[:, ~combined_data.columns.str.contains(\"^Unnamed: 0\")]\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe-combined-data\n",
    "combined_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate-statistics\n",
    "combined_stats = []\n",
    "\n",
    "for col in combined_data.columns:\n",
    "    combined_stats.append(\n",
    "        (\n",
    "            col,\n",
    "            combined_data[col].nunique(),\n",
    "            combined_data[col].isnull().sum() * 100 / combined_data.shape[0],\n",
    "            combined_data[col].value_counts(normalize=True, dropna=False).values[0] * 100,\n",
    "            combined_data[col].dtype,\n",
    "        )\n",
    "    )\n",
    "\n",
    "stats_df = pd.DataFrame(\n",
    "    combined_stats,\n",
    "    columns=[\"feature\", \"unique_values\", \"percent_missing\", \"percent_largest_category\", \"datatype\"],\n",
    ")\n",
    "stats_df.sort_values(\"percent_largest_category\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap-graph\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "corr_list = [\n",
    "    \"customer_age\",\n",
    "    \"months_as_customer\",\n",
    "    \"total_claim_amount\",\n",
    "    \"injury_claim\",\n",
    "    \"vehicle_claim\",\n",
    "    \"incident_severity\",\n",
    "    \"fraud\",\n",
    "]\n",
    "\n",
    "corr_df = combined_data[corr_list]\n",
    "corr = round(corr_df.corr(), 2)\n",
    "\n",
    "fix, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "ax = sns.heatmap(corr, mask=mask, ax=ax, annot=True, cmap=\"OrRd\")\n",
    "\n",
    "ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=10, ha=\"right\", rotation=45)\n",
    "ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=10, va=\"center\", rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task1-2-continue\" target=\"_self\">Task 1.2</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1-3-1\" id=\"task1-3-1\"></a>\n",
    "\n",
    "### Appendix: Joining tables in SageMaker Studio (Task 1.3)\n",
    "\n",
    "1. Return to the **Data flow** view, choose the **< Data flow** located at the top left of the *CapstoneDataWrangler.flow* tab.\n",
    "1. Choose the **+** sign next to the **claims.CSV Data types** icon, and from the context menu, choose **Join**.\n",
    "\n",
    "SageMaker Data Wrangler displays the **Join** page.\n",
    "\n",
    "1. Choose the  **customers.csv Data types** icon.\n",
    "1. Choose <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">Configure</span>.\n",
    "1. For **Join Type**, select **Inner**.\n",
    "1. In the **Columns** section:\n",
    "\n",
    "    - For **Left**, select <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">policy_id</span>.\n",
    "    \n",
    "    - For **Right**, select <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">policy_id</span>.\n",
    "\n",
    "1. Choose <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">Preview</span>.\n",
    "\n",
    "1. Choose <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">Add</span>.\n",
    "\n",
    "To continue this lab, return to <a href=\"#task1-3-continue\" target=\"_self\">Task 1.3</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1-3-2\" id=\"task1-3-2\"></a>\n",
    "\n",
    "### Appendix: Run a pre-training bias report (Task 1.3)\n",
    "\n",
    "Create a SageMaker Clarify bias report using a SageMaker Data Wrangler flow.\n",
    "\n",
    "1. Choose the **CapstoneDataWrangler.flow** tab.\n",
    "1. Go to the **Data flow** view. If necessary, choose the **< Data flow** located at the top left of the *DataWranglerLab.flow* tab. \n",
    "1. Choose **+** sign next to the **Join** icon, and from the context menu, choose **Add analysis**.\n",
    "1. In the **Create analysis** section:\n",
    "\n",
    "- For **Analysis type**, select **Bias Report**.\n",
    "- For **Analysis name**, enter `fraud bias by age`.\n",
    "- For **Select the column your model predicts (target)**, select **fraud**.\n",
    "- For **Is your predicted column a value or threshold?**, choose the **value** option.\n",
    "- For **Predicted value(s)**, enter **1**.\n",
    "- For **Select the column to analyze for bias**, select **customer_age**.\n",
    "\n",
    "1. Choose <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">Check for bias</span>.\n",
    "\n",
    "After the job is finished, view the returned metrics. Take note if there is bias and plan any processing steps that you want to take on the column that you analyzed. \n",
    "\n",
    "1. For any analysis you want to save, choose <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">Save</span>.\n",
    "\n",
    "You can repeat these steps for any columns that you want to analyze for bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task1-3-continue\" target=\"_self\">Task 1.3</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1-4-1\" id=\"task1-4-1\"></a>\n",
    "\n",
    "### Appendix: Prepare data using SageMaker Data Wrangler (Task 1.4)\n",
    "\n",
    "Combine the datasets, join them in SageMaker Data Wrangler using the **policy_id**.\n",
    "\n",
    "With SageMaker Data Wrangler, you can join data at any point in the flow. You can complete data preparation on the individual files before joining them, or you can transform the features after the join. A SageMaker Data Wrangler flow is flexible.\n",
    "\n",
    "If you have not joined the tables in task 1.3, the following steps guide you through the process.\n",
    "\n",
    "1. Return to the **Data flow** view, choose the **< Data flow** located at the top left of the *CapstoneDataWrangler.flow* tab.\n",
    "1. Choose the **+** sign next to the **Data types** icon, and from the context menu, choose **Join**.\n",
    "\n",
    "SageMaker Data Wrangler displays the **Join** page.\n",
    "\n",
    "1. Choose the second  **Data types** icon.\n",
    "1. Choose <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">Configure</span>.\n",
    "1. For **Join Type**, select **Inner**.\n",
    "1. In the **Columns** section:\n",
    "\n",
    "    - For **Left**, select <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">policy_id</span>.\n",
    "    \n",
    "    - For **Right**, select <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">policy_id</span>.\n",
    "\n",
    "1. Choose <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">Preview</span>.\n",
    "\n",
    "1. Choose <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">Add</span>.\n",
    "\n",
    "With the data tables joined, transform the combined data.\n",
    "\n",
    "1. Return to the **Data flow** view, choose the **< Data flow** located at the top left of the *CapstoneDataWrangler.flow* tab.\n",
    "1. Choose the **+** sign next to the **Join** icon, and from the context menu, choose **Add transform**.\n",
    "\n",
    "Multiple transformations can be added to data sets using this menu. A preview of the dataset is shown on the left side of the transformation menu.\n",
    "\n",
    "Add the following transform steps to the SageMaker Data Wrangler flow:\n",
    "- Encode categorical (One-hot encoding): **authorities_contacted**, **collision_type**, **customer_gender**, **driver_relationship**, **incident_type**, and **policy_state** using a **Skip** invalid handling strategy, select output style of **Columns**.\n",
    "- Encode categorical (Ordinal encode): **customer_education**, **incident_severity**, **police_report_available**, and **policy_liability** using a **Skip** invalid handling strategy.\n",
    "- Parse column as type: **vehicle_claim** and **total_claim_amount** from **Float** to **Long**.\n",
    "- Manage columns (Drop column): **customer_zip** and **policy_id_1**.\n",
    "- Manage columns (Move column): **fraud** (using **Move to start**).\n",
    "- Manage columns (Rename column): Replace the **/** symbol from **collision_type_N/A**, and **driver_relationship_N/A** with a **_**.\n",
    "- Manage columns (Rename column): Rename **policy_id_0** to **policy_id**.\n",
    "\n",
    "If any column name has a **/** character in it, rename the column to replace **/** with **_**. If any column name has a blank space character in it, rename the column to replace the blank space with **_**. For example, any column created with one-hot encoding that has **N/A** as a value need to be renamed. SageMaker Feature Store does not accept columns with a **/** or blank space characters in it.\n",
    "\n",
    "When you have transformed your data and are ready to start training your model, you can continue to the next task. You can always return to this flow and make changes based on your findings during training and tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task1-4-continue\" target=\"_self\">Task 1.4</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task1-4-2\" id=\"task1-4-2\"></a>\n",
    "\n",
    "### Appendix: Import an example set of processed data (Task 1.4)\n",
    "\n",
    "If you get stuck during preprocessing or want to load a set of data that has been processed for you, access the processed data stored in the data folder in your S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed-data-import\n",
    "s3_client.upload_file(Filename=\"data/claims_customer.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/claims_customer.csv\")\n",
    "df_processed = pd.read_csv(\"./data/claims_customer.csv\", index_col=None)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task1-4-continue\" target=\"_self\">Task 1.4</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task2-1\" id=\"task2-1\"></a>\n",
    "\n",
    "### Appendix: Create a feature group using the Export to option (Task 2.1)\n",
    "\n",
    "SageMaker Data Wrangler can export data to SageMaker Feature Store. It creates a notebook with all the code required to configure a feature group and ingest your transformed data into the feature group.\n",
    "\n",
    "1. Choose **Add step**.\n",
    "\n",
    "1. Choose **Custom transform**.\n",
    "\n",
    "1. For **Name**, enter `event_time`. \n",
    "\n",
    "1. Select **Python (PySpark)** if it is not already selected.\n",
    "\n",
    "1. For **Your custom transform**, enter the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "date_time = datetime.date.today()\n",
    "\n",
    "df = df.withColumn(\"event_time\", lit(time.mktime(date_time.timetuple())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Choose <span style=\"background-color:#1a1b22; font-size:90%; color:#57c4f8; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#57c4f8; border-width:thin; border-style:solid; border-radius:2px; margin-right:5px; white-space:nowrap\">Preview</span>.\n",
    "\n",
    "1. Choose <span style=\"background-color:#57c4f8; font-size:90%;  color:black; position:relative; top:-1px; padding-top:3px; padding-bottom:3px; padding-left:10px; padding-right:10px; border-color:#00a0d2; border-radius:2px; margin-right:5px; white-space:nowrap\">Add</span>.\n",
    "\n",
    "This adds **event_time** as a column to your dataset. SageMaker Feature Store requires an **event_time** and a unique **record** ID. Use **policy_id** as your **record** ID.\n",
    "\n",
    "1. To return to your data flow, choose the **< Data flow** icon.\n",
    "\n",
    "1. Choose the **+** icon next to your transformations in SageMaker Data Wrangler.\n",
    "\n",
    "1. Choose **Export to**.\n",
    "\n",
    "1. Choose **SageMaker Feature Store (via Jupyter Notebook)**.\n",
    "\n",
    "A new notebook opens.\n",
    "\n",
    "1. In the first cell, change the following variables:\n",
    "- For **record_identifier_feature_name**, replace **None** with `\"policy_id\"`. If you joined the customers and claims tables together and did not drop the second **policy_id** column, you might need to replace **None** with `\"policy_id_0\"`. Do not change the **None** value after the **if** statement.\n",
    "- For **event_time_feature_name**, replace **None** with `\"event_time\"`. Do not change the **None** value after the **if** statement.\n",
    "\n",
    "**Expected output:** When you are done editing the cell, it should resemble this example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_identifier_feature_name = \"policy_id\"\n",
    "if record_identifier_feature_name is None:\n",
    "   raise SystemExit(\"Select a column name as the feature group record identifier.\")\n",
    "\n",
    "event_time_feature_name = \"event_time\"\n",
    "if event_time_feature_name is None:\n",
    "   raise SystemExit(\"Select a column name as the event time feature name.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Run through all the cells to create a feature definition, a feature group, and ingest the transformed data into the feature group using a processing job. \n",
    "\n",
    "When the cells are complete, your feature store is ready to use.\n",
    "\n",
    "To continue this lab, return to <a href=\"#task2-1-continue\" target=\"_self\">Task 2.1</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task2-2\" id=\"task2-2\"></a>\n",
    "\n",
    "### Appendix: Extract records from an offline store with Athena (Task 2.2)\n",
    "\n",
    "Set up an Athena query with **athena_query**. Then, set your **query_string**. Finally, run the query and view a sample of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure-and-run-athena-query\n",
    "try:\n",
    "    # If there is a feature group, get the name\n",
    "    feature_group_name = sagemaker_session.boto_session.client(\"sagemaker\", region_name=region).list_feature_groups()['FeatureGroupSummaries'][0]['FeatureGroupName']\n",
    "    feature_group = FeatureGroup(name=feature_group_name, sagemaker_session=sagemaker_session)\n",
    "\n",
    "    # Confirm the Athena settings are configured\n",
    "    try:\n",
    "        boto3.client('athena').update_work_group(\n",
    "            WorkGroup='primary',\n",
    "            ConfigurationUpdates={\n",
    "                'EnforceWorkGroupConfiguration':False\n",
    "            }\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Configure the query\n",
    "    query = feature_group.athena_query()\n",
    "    table = query.table_name\n",
    "    query_string = f'SELECT * FROM \"{table}\" '\n",
    "    output_location = f\"s3://{sagemaker_session.default_bucket()}/query_results/\"\n",
    "    print(f\"Athena query output location: \\n{output_location}\")\n",
    "\n",
    "    # Run the query\n",
    "    query.run(query_string=query_string, output_location=output_location)\n",
    "    query.wait()\n",
    "    df_feature_store = query.as_dataframe()\n",
    "    \n",
    "    # Wait for data to appear in the feature group\n",
    "    attempts = 0\n",
    "    while len(df_feature_store.index) == 0 and attempts < 30:\n",
    "        print(\"Waiting for feature group to populate...\")\n",
    "        time.sleep(60)\n",
    "        # Rerun the query\n",
    "        query.run(query_string=query_string, output_location=output_location)\n",
    "        query.wait()\n",
    "        df_feature_store = query.as_dataframe()\n",
    "        # Increment the attempts\n",
    "        attempts += 1\n",
    "    if len(df_feature_store.index) != 0:\n",
    "        print(\"The feature group is populated.\")\n",
    "except IndexError as e:\n",
    "    # If there is no feature group, thrown an error\n",
    "    print(\"No feature groups were found. Please create a feature group.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the previous code block, the Amazon Athena query records are saved in the Amazon S3 bucket which has a name starting with *sagemaker*. The saved query object is in a directory named **query_results**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task2-2-continue\" target=\"_self\">Task 2.2</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task3-1\" id=\"task3-1\"></a>\n",
    "\n",
    "### Appendix: Name an experiment and a run (Task 3.1)\n",
    "\n",
    "To create an experiment, use the library **sagemaker.experiments.run**. Set the **experiment_name**, a **run_name**, and the **description**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "#create experiment and run-names\n",
    "create_date = strftime(\"%m%d%H%M\")\n",
    "capstone_experiment_name=\"capstone-experiment-{}\".format(create_date)\n",
    "capstone_run_name = \"lab-capstone-run-{}\".format(create_date)\n",
    "\n",
    "# define a run_tag\n",
    "run_tags = [{'Key': 'lab-capstone', 'Value': 'lab-capstone-run'}]\n",
    "\n",
    "# provide a description\n",
    "description=\"Using SM Experiments with the Auto dataset.\"\n",
    "\n",
    "print(f\"Experiment name - {capstone_experiment_name},  run name - {capstone_run_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task3-1-continue\" target=\"_self\">Task 3.1</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task3-2\" id=\"task3-2\"></a>\n",
    "\n",
    "### Appendix: Split data into train, test, and validation datasets (Task 3.2)\n",
    "\n",
    "To split your data, use **np.split** and specify how you want to split your data. Then, create CSV files and upload them to your S3 bucket. Next, set up your training inputs. Finally, create your **data_inputs** variable. Use the data_inputs throughout the challenge to specify the train and validation datasets when training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-validation-test-split\n",
    "try:\n",
    "    # If there is a feature group, use it\n",
    "    df_feature_store = df_feature_store.iloc[: , :-4]\n",
    "    df_processed_pre_split = df_feature_store\n",
    "    print(\"Using the records from the feature group\")\n",
    "except NameError:\n",
    "    # If there is no feature group, use the processed dataset\n",
    "    df_processed = pd.read_csv(\"./data/claims_customer.csv\", index_col=None)\n",
    "    df_processed_pre_split = df_processed\n",
    "    print(\"Using the processed records from Amazon S3\")\n",
    "\n",
    "# Split the data into train, validation, and test datasets\n",
    "train_data, validation_data, test_data = np.split(\n",
    "    df_processed_pre_split.sample(frac=1, random_state=1729),\n",
    "    [int(0.7 * len(df_processed_pre_split)), int(0.9 * len(df_processed_pre_split))],\n",
    ")\n",
    "\n",
    "# Create the CSV files and upload them to your default bucket\n",
    "train_data.to_csv(\"train_data.csv\", index=False, header=False)\n",
    "validation_data.to_csv(\"validation_data.csv\", index=False, header=False)\n",
    "test_data.to_csv(\"test_data.csv\", index=False, header=False)\n",
    "\n",
    "train_path = S3Uploader.upload(\"train_data.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "validation_path = S3Uploader.upload(\"validation_data.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "test_path = S3Uploader.upload(\"test_data.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "\n",
    "# Set the training inputs\n",
    "train_input = TrainingInput(train_path, content_type=\"text/csv\")\n",
    "validation_input = TrainingInput(validation_path, content_type=\"text/csv\")\n",
    "test_input = TrainingInput(test_path, content_type=\"text/csv\")\n",
    "\n",
    "data_inputs = {\n",
    "    \"train\": train_input,\n",
    "    \"validation\": validation_input\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task3-2-continue\" target=\"_self\">Task 3.2</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task3-3-1\" id=\"task3-3-1\"></a>\n",
    "\n",
    "### Appendix: Configure and run a basic training job (Task 3.3)\n",
    "\n",
    "If you want to start with a basic training job, use the basic **XGBoost** container. Then, configure your estimator, noting the container and role that you want to use. When those configurations are set, you can choose your hyperparameters. You can use the default values that are provided in the code that follows, or you can edit them based on your findings during data preparation.\n",
    "\n",
    "To run the training job, call **fit()**, setting the inputs as your **data_inputs** variable and setting your configurations for a **run_name**, and a **experiment_name**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker import image_uris\n",
    "#train-model\n",
    "# Retrieve the container image\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    region=boto3.Session().region_name, \n",
    "    framework=\"xgboost\", \n",
    "    version=\"1.5-1\"\n",
    ")\n",
    "\n",
    "# Set the hyperparameters\n",
    "eta=0.2\n",
    "gamma=4\n",
    "max_depth=5\n",
    "min_child_weight=6\n",
    "num_round=800\n",
    "objective='binary:logistic'\n",
    "subsample=0.8\n",
    "\n",
    "hyperparameters = {\n",
    "    \"eta\":eta,\n",
    "    \"gamma\":gamma,\n",
    "    \"max_depth\":max_depth,\n",
    "    \"min_child_weight\":min_child_weight,\n",
    "    \"num_round\":num_round,\n",
    "    \"objective\":objective,\n",
    "    \"subsample\":subsample\n",
    "}\n",
    "\n",
    "# Set up the estimator\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,    \n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    max_run=1800,\n",
    "    hyperparameters=hyperparameters,\n",
    "    tags = run_tags\n",
    ")\n",
    "\n",
    "with Run(\n",
    "    experiment_name=capstone_experiment_name,\n",
    "    run_name=capstone_run_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ") as run:\n",
    "    run.log_parameter(\"eta\", eta)\n",
    "    run.log_parameter(\"gamma\", gamma)\n",
    "    run.log_parameter(\"max_depth\", max_depth)\n",
    "    run.log_parameter(\"min_child_weight\", min_child_weight)\n",
    "    run.log_parameter(\"objective\", objective)\n",
    "    run.log_parameter(\"subsample\", subsample)\n",
    "    run.log_parameter(\"num_round\", num_round)\n",
    "\n",
    "# Train the model associating the training run with the current \"experiment\"\n",
    "    xgb.fit(\n",
    "        inputs = data_inputs\n",
    "    )        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task3-3-continue\" target=\"_self\">Task 3.3</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task3-3-2\" id=\"task3-3-2\"></a>\n",
    "\n",
    "### Appendix: Configure and run a training job with SageMaker Debugger enabled and analyze reports (Task 3.3)\n",
    "\n",
    "SageMaker Debugger helps you find additional reports that can inform your hyperparameter tuning quickly, saving time when you start to run more training jobs with hyperparameter ranges. To enable the debugger, configure your **DebuggerHookConfig** and **rules**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable-debugger\n",
    "# Retrieve the container image\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    region=boto3.Session().region_name, \n",
    "    framework=\"xgboost\", \n",
    "    version=\"1.5-1\"\n",
    ")\n",
    "\n",
    "# Set the hyperparameters\n",
    "eta=0.2\n",
    "gamma=4\n",
    "max_depth=5\n",
    "min_child_weight=6\n",
    "num_round=300\n",
    "objective='binary:logistic'\n",
    "subsample=0.7\n",
    "        \n",
    "hyperparameters = {\n",
    "        \"eta\":eta,\n",
    "        \"gamma\":gamma,\n",
    "        \"max_depth\":max_depth,\n",
    "        \"min_child_weight\":min_child_weight,\n",
    "        \"num_round\":num_round,\n",
    "        \"objective\":objective,\n",
    "        \"subsample\":subsample\n",
    "}\n",
    "\n",
    "# Set up the estimator\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role, \n",
    "    base_job_name=base_job_name,\n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    max_run=1800,\n",
    "    hyperparameters=hyperparameters,\n",
    "    tags = run_tags,\n",
    "\n",
    "    #Set the Debugger Hook Config\n",
    "    debugger_hook_config=DebuggerHookConfig(\n",
    "        s3_output_path=bucket_path,  # Required\n",
    "        collection_configs=[\n",
    "            CollectionConfig(name=\"metrics\", parameters={\"save_interval\": str(save_interval)}),\n",
    "            CollectionConfig(name=\"feature_importance\", parameters={\"save_interval\": str(save_interval)},),\n",
    "            CollectionConfig(name=\"full_shap\", parameters={\"save_interval\": str(save_interval)}),\n",
    "            CollectionConfig(name=\"average_shap\", parameters={\"save_interval\": str(save_interval)}),\n",
    "        ],\n",
    "        ),\n",
    "        #Set the Debugger Profiler Configuration\n",
    "        profiler_config = ProfilerConfig(\n",
    "            system_monitor_interval_millis=500,\n",
    "            framework_profile_params=FrameworkProfile()\n",
    "    ),\n",
    "        #Configure the Debugger Rule Object\n",
    "        rules = [\n",
    "            ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "            Rule.sagemaker(rule_configs.create_xgboost_report()),  \n",
    "            Rule.sagemaker(rule_configs.overfit()),\n",
    "            Rule.sagemaker(rule_configs.overtraining()),\n",
    "            Rule.sagemaker(rule_configs.loss_not_decreasing(),\n",
    "                rule_parameters={\n",
    "                    \"collection_names\": \"metrics\",\n",
    "                    \"num_steps\": str(save_interval * 2),\n",
    "                }\n",
    "            )\n",
    "    ]\n",
    ")\n",
    "with Run(\n",
    "    experiment_name=capstone_experiment_name,\n",
    "    run_name=capstone_run_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ") as run:\n",
    "    run.log_parameter(\"eta\", eta)\n",
    "    run.log_parameter(\"gamma\", gamma)\n",
    "    run.log_parameter(\"max_depth\", max_depth)\n",
    "    run.log_parameter(\"min_child_weight\", min_child_weight)\n",
    "    run.log_parameter(\"objective\", objective)\n",
    "    run.log_parameter(\"subsample\", subsample)\n",
    "    run.log_parameter(\"num_round\", num_round)\n",
    "# Train the model\n",
    "xgb.fit(\n",
    "    inputs = data_inputs\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task3-3-continue\" target=\"_self\">Task 3.3</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task3-4\" id=\"task3-4\"></a>\n",
    "\n",
    "### Appendix: Configure training hyperparameter ranges (Task 3.4)\n",
    "\n",
    "Now that you have trained at least one model, you can use what you learned from data processing and SageMaker Debugger to inform what ranges you select for your hyperparameters. Edit the following hyperparameter ranges and run the tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune-model\n",
    "# Setup the hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    \"alpha\": ContinuousParameter(0, 2),\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "    \"min_child_weight\": ContinuousParameter(1, 10),\n",
    "    \"num_round\": IntegerParameter(100, 1000)\n",
    "}\n",
    "\n",
    "# Define the target metric and the objective type (max/min)\n",
    "objective_metric_name = \"validation:auc\"\n",
    "objective_type=\"Maximize\"\n",
    "\n",
    "# Define the HyperparameterTuner\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator = xgb,\n",
    "    objective_metric_name = objective_metric_name,\n",
    "    hyperparameter_ranges = hyperparameter_ranges,\n",
    "    objective_type = objective_type,\n",
    "    max_jobs=12,\n",
    "    max_parallel_jobs=4,\n",
    "    early_stopping_type=\"Auto\"\n",
    ")\n",
    "\n",
    "# Tune the model\n",
    "tuner.fit(\n",
    "    inputs = data_inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task3-4-continue\" target=\"_self\">Task 3.4</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task4-1\" id=\"task4-1\"></a>\n",
    "\n",
    "### Appendix: Create a model (Task 4.1)\n",
    "\n",
    "Create a XGBoost model, calling **create_model** with the **model_name**, **role**, and **container_def** that you define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model-configurations\n",
    "model_name = \"capstone-clarify-model\"\n",
    "model = xgb.create_model(name=model_name)\n",
    "container_def = model.prepare_container_def()\n",
    "sagemaker_session.create_model(model_name, role, container_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task4-1-continue\" target=\"_self\">Task 4.1</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task4-2\" id=\"task4-2\"></a>\n",
    "\n",
    "### Appendix: Create a SageMaker Clarify model configuration (Task 4.2)\n",
    "\n",
    "Create a SageMaker Clarify model configuration using **SageMakerClarifyProcessor**. Set the **instance_count** and the **instance_type**. Use the **role** and **session** created at the beginning of the Capstone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-clarify-processor\n",
    "clarify_processor = clarify.SageMakerClarifyProcessor(\n",
    "    role=role, \n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.m5.xlarge\", \n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task4-2-continue\" target=\"_self\">Task 4.2</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task4-3\" id=\"task4-3\"></a>\n",
    "\n",
    "### Appendix: Create a SageMaker Clarify bias configuration (Task 4.3)\n",
    "\n",
    "To create a SageMaker Clarify bias configuration, choose an output path for the data, set the input path from the training job, in addition to the **label**, **headers**, and **dataset_type**.\n",
    "\n",
    "Then, you create a **ModelConfig** and **ModelPredictedLabelConfig**.\n",
    "\n",
    "Lastly, you configure a **BiasConfig** with the fields that you want SageMaker Clarify to observe. You can add or remove any fields that you are interested in exploring based on your initial findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-data-config\n",
    "bias_report_output_path = \"s3://{}/{}/clarify-bias\".format(bucket, prefix)\n",
    "bias_data_config = clarify.DataConfig(\n",
    "    s3_data_input_path=train_path,\n",
    "    s3_output_path=bias_report_output_path,\n",
    "    label=\"fraud\",\n",
    "    headers=train_data.columns.to_list(),\n",
    "    dataset_type=\"text/csv\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-model-config\n",
    "model_config = clarify.ModelConfig(\n",
    "    model_name=model_name,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    accept_type=\"text/csv\",\n",
    "    content_type=\"text/csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-label-config\n",
    "predictions_config = clarify.ModelPredictedLabelConfig(probability_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define-bias-config\n",
    "bias_config = clarify.BiasConfig(\n",
    "    label_values_or_threshold=[1], facet_name=\"customer_gender_female\", facet_values_or_threshold=[0], group_name=\"customer_age\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task4-3-continue\" target=\"_self\">Task 4.3</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task4-4\" id=\"task4-4\"></a>\n",
    "\n",
    "### Appendix: Run bias, data, and model reports using SageMaker Clarify (Task 4.4)\n",
    "\n",
    "Now that your SageMaker Clarify job is configured, run the job by calling **run_bias**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run-bias-report\n",
    "clarify_processor.run_bias(\n",
    "    data_config=bias_data_config,\n",
    "    bias_config=bias_config,\n",
    "    model_config=model_config,\n",
    "    model_predicted_label_config=predictions_config,\n",
    "    pre_training_methods=\"all\",\n",
    "    post_training_methods=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task4-4-continue\" target=\"_self\">Task 4.4</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task4-5\" id=\"task4-5\"></a>\n",
    "\n",
    "### Appendix: Remove imbalance (Task 4.5)\n",
    "\n",
    "In this example, you are upsampling **customer_gender_female** to reduce bias in the dataset. If you find other features that contain bias, you can also remove imbalance on those features. The **random_state** has been set to `42`, but you can change this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display-summary\n",
    "gender = train_data[\"customer_gender_female\"]\n",
    "gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove-imbalance\n",
    "sm = SMOTE(random_state=42)\n",
    "train_data_upsampled, gender_res = sm.fit_resample(train_data, gender)\n",
    "train_data_upsampled[\"customer_gender_female\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task4-5-continue\" target=\"_self\">Task 4.5</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task4-6\" id=\"task4-6\"></a>\n",
    "\n",
    "### Appendix: Retrain the model (Task 4.6)\n",
    "\n",
    "You have found imbalance and have a new training dataset. Use this dataset and retrain the file. To do this, upload the new file and create a new estimator. Then, retrain the data with **fit()**. Several hyperparameters are included in the following code as a sample. You can add, remove, or adjust these as needed during retraining to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload-upsampled-csv\n",
    "train_data_upsampled.to_csv(\"data/upsampled_train.csv\", index=False, header=False)\n",
    "retrain_path = S3Uploader.upload(\"data/upsampled_train.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "retrain_input = TrainingInput(retrain_path, content_type=\"text/csv\")\n",
    "\n",
    "retrain_data_inputs = {\n",
    "    \"train\": retrain_input,\n",
    "    \"validation\": validation_input\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create-estimator\n",
    "hyperparameters= {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"300\",\n",
    "}\n",
    "\n",
    "xgb_retrained = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role, \n",
    "    instance_count=1, \n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain-upsampled-data\n",
    "xgb_retrained.fit(\n",
    "    inputs = retrain_data_inputs\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task4-6-continue\" target=\"_self\">Task 4.6</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task5-1\" id=\"task5-1\"></a>\n",
    "\n",
    "### Appendix: Create a batch transform job (Task 5.1)\n",
    "\n",
    "Use your model estimator and create a batch transform job with **transformer**. Set the strategy to **MultiRecord** to increase the processing efficiency. Then, pass in your **test_path** and wait for the inference to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create-batch-transformer\n",
    "# Use the retrained model if it exists, otherwise, use the original model\n",
    "try:\n",
    "    model = xgb_retrained\n",
    "except NameError:\n",
    "    model = xgb\n",
    "\n",
    "# Create the transformer\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    strategy=\"MultiRecord\",\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",\n",
    "    output_path=\"s3://{}/{}/batch-transform/\".format(bucket, prefix)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run-batch-transform-job\n",
    "test_data_batch = test_data.drop(\"fraud\", axis=1)\n",
    "test_data_batch.to_csv(\"test_data_batch.csv\", index=False, header=False)\n",
    "test_path_batch = S3Uploader.upload(\"test_data_batch.csv\", \"s3://{}/{}\".format(bucket, prefix))\n",
    "\n",
    "transformer.transform(test_path_batch, content_type=\"text/csv\", split_type=\"Line\", join_source=\"Input\")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task5-1-continue\" target=\"_self\">Task 5.1</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task5-2\" id=\"task5-2\"></a>\n",
    "\n",
    "### Appendix: View prediction and accuracy data from a batch transform job (Task 5.2)\n",
    "\n",
    "When your batch transform job is finished, view the prediction data stored in Amazon S3. You can reference the output path that you set up in the **transformer** and sample the data.\n",
    "\n",
    "In this output, the **fraud** prediction is appended to the end of each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $transformer.output_path ./\n",
    "test_data = pd.read_csv(\"test_data_batch.csv.out\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task5-2-continue\" target=\"_self\">Task 5.2</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task5-3\" id=\"task5-3\"></a>\n",
    "\n",
    "### Appendix: Clean up SageMaker instances in SageMaker Studio (Task 5.3)\n",
    "\n",
    "While developing models in SageMaker Studio, periodically check if there are any instances that you want to clean up. If there are, you can shut down instances from within SageMaker Studio.\n",
    "\n",
    "1. In the left menu bar, choose the **Running Terminals and Kernels** icon (circle with a square in the middle).\n",
    "\n",
    "1. If there are any instances that are still open, to the right of each instance type, chose the **Shut down** icon. \n",
    "\n",
    "You can view the applications that are running on each instance to confirm which ones you want to close.\n",
    "\n",
    "1. If a popup window appears, choose **Shut down all**.\n",
    "\n",
    "1. Choose the **Refresh List** icon periodically until the instance is no longer on the list. It might take 25 minutes for an instance to shut down.\n",
    "\n",
    "You do not need to shut down the instance that your **capstone.ipynb** notebook is using.\n",
    "\n",
    "To continue this lab, return to <a href=\"#task5-3-continue\" target=\"_self\">Task 5.3</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task6-1\" id=\"task6-1\"></a>\n",
    "\n",
    "### Appendix: Configure a pipeline (Task 6.1)\n",
    "\n",
    "To create a pipeline, define each step of the pipeline process and then run it.\n",
    "\n",
    "In this example, you create the following steps:\n",
    "- **AutoModelProcess**: A **Processing** step that pulls in the .csv file and splits it into train, test, and validation datasets.\n",
    "- **AutoHyperParameterTuning**: A **Tuning** step that takes a range of hyperparameters and tunes the model.\n",
    "- **AutoEvalBestModel**: A **Processing** step that creates an evaluation report to describe the best model.\n",
    "- **CheckAUCScoreAutoEvaluation**: A **Condition** step that evaluates the models based on an evaluation metric. \n",
    "- **AutoCreateModel**: A **Model** step that creates a model.\n",
    "- **RegisterAutoModel-RegisterModel**: A **RegisterModel** step that registers a model.\n",
    "- **AutoModelConfigFile**: A **Processing** step that creates a bias report.\n",
    "- **AutoTransform**: A **Transform** step that runs a batch transform job.\n",
    "- **ClarifyProcessingStep**: A **Processing** step that runs a SageMaker Clarify job.\n",
    "\n",
    "If you get stuck at any point while building the pipeline, you can customize the following code or use it as a guide while you build your own pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run-pipeline\n",
    "# Set the variables\n",
    "model_name = \"Auto-model\"\n",
    "sklearn_processor_version=\"0.23-1\"\n",
    "model_package_group_name=\"AutoModelPackageGroup\"\n",
    "pipeline_name= \"AutoModelSMPipeline\"\n",
    "clarify_image = sagemaker.image_uris.retrieve(framework='sklearn',version=sklearn_processor_version,region=region)\n",
    "\n",
    "# Upload files to the default S3 bucket\n",
    "s3_client.put_object(Bucket=bucket,Key='data/')\n",
    "s3_client.put_object(Bucket=bucket,Key='input/code/')\n",
    "s3_client.upload_file(Filename=\"data/batch_data.csv\", Bucket=bucket, Key=\"data/batch_data.csv\")  #If you edit this, make sure to also edit the headers listed in generate_config to match your column names.\n",
    "s3_client.upload_file(Filename=\"data/claims_customer.csv\", Bucket=bucket, Key=\"data/claims_customer.csv\")  #If you edit this, make sure to also edit the headers listed in generate_config to match your column names.\n",
    "s3_client.upload_file(Filename=\"pipelines/evaluate.py\", Bucket=bucket, Key=\"input/code/evaluate.py\")\n",
    "s3_client.upload_file(Filename=\"pipelines/generate_config.py\", Bucket=bucket, Key=\"input/code/generate_config.py\")\n",
    "s3_client.upload_file(Filename=\"pipelines/preprocess.py\", Bucket=bucket, Key=\"input/code/preprocess.py\")\n",
    "\n",
    "# Configure important settings. Change the input_data if you want to\n",
    "# use a file other than the claims_customer.csv and batch_data.csv files.\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "processing_instance_type = ParameterString(\n",
    "        name=\"ProcessingInstanceType\",\n",
    "        default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "training_instance_type = ParameterString(\n",
    "        name=\"TrainingInstanceType\",\n",
    "        default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "input_data = ParameterString(\n",
    "        name=\"InputData\",\n",
    "        default_value=\"s3://{}/data/claims_customer.csv\".format(bucket), \n",
    ")\n",
    "batch_data = ParameterString(\n",
    "        name=\"BatchData\",\n",
    "        default_value=\"s3://{}/data/batch_data.csv\".format(bucket),\n",
    ")\n",
    "\n",
    "# Run a scikit-learn script to do data processing on SageMaker using \n",
    "# using the SKLearnProcessor class\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "        framework_version=sklearn_processor_version,\n",
    "        instance_type=processing_instance_type.default_value, \n",
    "        instance_count=processing_instance_count,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        role=role,\n",
    ")\n",
    "\n",
    "# Configure the processing step to pull in the input_data\n",
    "step_process = ProcessingStep(\n",
    "        name=\"AutoModelProcess\",\n",
    "        processor=sklearn_processor,\n",
    "        outputs=[\n",
    "            ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\",\\\n",
    "                             destination=f\"s3://{bucket}/output/train\" ),\n",
    "            ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\",\\\n",
    "                            destination=f\"s3://{bucket}/output/validation\"),\n",
    "            ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\",\\\n",
    "                            destination=f\"s3://{bucket}/output/test\"),\n",
    "            ProcessingOutput(output_name=\"batch\", source=\"/opt/ml/processing/batch\",\\\n",
    "                            destination=f\"s3://{bucket}/data/batch\"),\n",
    "            ProcessingOutput(output_name=\"baseline\", source=\"/opt/ml/processing/baseline\",\\\n",
    "                            destination=f\"s3://{bucket}/input/baseline\")\n",
    "        ],\n",
    "        code=f\"s3://{bucket}/input/code/preprocess.py\",\n",
    "        job_arguments=[\"--input-data\", input_data],\n",
    ")\n",
    "\n",
    "# Set up the model path, image uri, and hyperparameters for the estimator\n",
    "model_path = f\"s3://{bucket}/output\"\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.5-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=training_instance_type.default_value,\n",
    ")\n",
    "\n",
    "fixed_hyperparameters = {\n",
    "    \"eval_metric\":\"auc\",\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"num_round\":\"100\",\n",
    "    \"rate_drop\":\"0.3\",\n",
    "    \"tweedie_variance_power\":\"1.4\"\n",
    "}\n",
    "\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    hyperparameters=fixed_hyperparameters,\n",
    "    output_path=model_path,\n",
    "    base_job_name=f\"auto-train\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "# Set the hyperparameter ranges for the tuning step and configure the tuning step\n",
    "hyperparameter_ranges = {\n",
    "    \"eta\": ContinuousParameter(0, 1),\n",
    "    \"min_child_weight\": ContinuousParameter(1, 10),\n",
    "    \"alpha\": ContinuousParameter(0, 2),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "}\n",
    "objective_metric_name = \"validation:auc\"\n",
    "\n",
    "step_tuning = TuningStep(\n",
    "    name = \"AutoHyperParameterTuning\",\n",
    "    tuner = HyperparameterTuner(xgb_train, objective_metric_name, hyperparameter_ranges, max_jobs=2, max_parallel_jobs=2),\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Configure the processing step for evaluation\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-auto-eval\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"AutoEvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"AutoEvalBestModel\",\n",
    "    processor=script_eval,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_tuning.get_top_model_s3_uri(top_k=0,s3_bucket=bucket,prefix=\"output\"),\n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\",\\\n",
    "                            destination=f\"s3://{bucket}/output/evaluation\"),\n",
    "    ],\n",
    "    code=f\"s3://{bucket}/input/code/evaluate.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")\n",
    "\n",
    "# Configure model creation\n",
    "model = Model(\n",
    "    image_uri=image_uri,        \n",
    "    model_data=step_tuning.get_top_model_s3_uri(top_k=0,s3_bucket=bucket,prefix=\"output\"),\n",
    "    name=model_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    accelerator_type=\"ml.inf1.xlarge\",\n",
    ")\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"AutoCreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    ")\n",
    "\n",
    "script_processor = ScriptProcessor(\n",
    "    command=['python3'],\n",
    "    image_uri=clarify_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=processing_instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "bias_report_output_path = f\"s3://{bucket}/clarify-output/bias\"\n",
    "clarify_instance_type = 'ml.m5.xlarge'\n",
    "step_config_file = ProcessingStep(\n",
    "    name=\"AutoModelConfigFile\",\n",
    "    processor=script_processor,\n",
    "    code=f\"s3://{bucket}/input/code/generate_config.py\",\n",
    "    job_arguments=[\"--modelname\",step_create_model.properties.ModelName,\"--bias-report-output-path\",bias_report_output_path,\"--clarify-instance-type\",clarify_instance_type,\\\n",
    "                  \"--default-bucket\",bucket,\"--num-baseline-samples\",\"50\",\"--instance-count\",\"1\"],\n",
    "    depends_on= [step_create_model.name]\n",
    ")\n",
    "\n",
    "# Configure the step to perform a batch transform job\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",    \n",
    "    output_path=f\"s3://{bucket}/AutoTransform\"\n",
    ")\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"AutoTransform\",\n",
    "    transformer=transformer,\n",
    "    inputs=TransformInput(data=batch_data,content_type=\"text/csv\",join_source=\"Input\",split_type=\"Line\")\n",
    ")\n",
    "\n",
    "# Configure the SageMaker Clarify processing step\n",
    "analysis_config_path = f\"s3://{bucket}/clarify-output/bias/analysis_config.json\"\n",
    "\n",
    "data_config = sagemaker.clarify.DataConfig(\n",
    "    s3_data_input_path=f's3://{bucket}/output/train/train.csv', \n",
    "    s3_output_path=bias_report_output_path,\n",
    "    label=0,\n",
    "    headers=list(pd.read_csv(\"./data/claims_customer.csv\", index_col=None).columns), #If you edit this, make sure to also edit the headers listed in generate_config to match your column names.\n",
    "    dataset_type=\"text/csv\",\n",
    ")\n",
    "\n",
    "clarify_processor = sagemaker.clarify.SageMakerClarifyProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=clarify_instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "config_input = ProcessingInput(\n",
    "    input_name=\"analysis_config\",\n",
    "    source=analysis_config_path,\n",
    "    destination=\"/opt/ml/processing/input/analysis_config\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_compression_type=\"None\",\n",
    ")\n",
    "\n",
    "data_input = ProcessingInput(\n",
    "    input_name=\"dataset\",\n",
    "    source=data_config.s3_data_input_path,\n",
    "    destination=\"/opt/ml/processing/input/data\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=data_config.s3_data_distribution_type,\n",
    "    s3_compression_type=data_config.s3_compression_type,\n",
    ")\n",
    "\n",
    "result_output = ProcessingOutput(\n",
    "    source=\"/opt/ml/processing/output\",\n",
    "    destination=data_config.s3_output_path,\n",
    "    output_name=\"analysis_result\",\n",
    "    s3_upload_mode=\"EndOfJob\",\n",
    ")\n",
    "\n",
    "step_clarify = ProcessingStep(\n",
    "    name=\"ClarifyProcessingStep\",\n",
    "    processor=clarify_processor,\n",
    "    inputs= [data_input, config_input],\n",
    "    outputs=[result_output],\n",
    "    depends_on = [step_config_file.name]\n",
    ")\n",
    "\n",
    "# Configure the model registration step\n",
    "model_statistics = MetricsSource(\n",
    "    s3_uri=\"s3://{}/output/evaluation/evaluation.json\".format(bucket),\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "explainability = MetricsSource(\n",
    "    s3_uri=\"s3://{}/clarify-output/bias/analysis.json\".format(bucket),\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "\n",
    "bias = MetricsSource(\n",
    "    s3_uri=\"s3://{}/clarify-output/bias/analysis.json\".format(bucket),\n",
    "    content_type=\"application/json\"\n",
    ") \n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=model_statistics,\n",
    "    explainability=explainability,\n",
    "    bias=bias\n",
    ")\n",
    "\n",
    "step_register = RegisterModel(\n",
    "    name=\"RegisterAutoModel\",\n",
    "    estimator=xgb_train,\n",
    "    model_data=step_tuning.get_top_model_s3_uri(top_k=0,s3_bucket=bucket,prefix=\"output\"),\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "\n",
    "# Create the model evaluation step\n",
    "cond_lte = ConditionGreaterThan(\n",
    "    left=JsonGet(\n",
    "        step=step_eval,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"binary_classification_metrics.auc.value\"\n",
    "    ),\n",
    "    right=0.75,\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"CheckAUCScoreAutoEvaluation\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_create_model,step_config_file,step_transform,step_clarify,step_register],\n",
    "    else_steps=[],\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "def get_pipeline(\n",
    "    region,\n",
    "    role=None,\n",
    "    default_bucket=None,\n",
    "    model_package_group_name=\"AutoModelPackageGroup\",\n",
    "    pipeline_name=\"AutoModelPipeline\",\n",
    "    base_prefix = None,\n",
    "    custom_image_uri = None,\n",
    "    sklearn_processor_version=None\n",
    "    ):\n",
    "    \"\"\"Gets a SageMaker ML Pipeline instance working with auto data.\n",
    "    Args:\n",
    "        region: AWS region to create and run the pipeline.\n",
    "        role: IAM role to create and run steps and pipeline.\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        an instance of a pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    # pipeline instance\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[\n",
    "            processing_instance_type,\n",
    "            processing_instance_count,\n",
    "            training_instance_type,\n",
    "            input_data,\n",
    "            batch_data,\n",
    "        ],\n",
    "        steps=[step_process,step_tuning,step_eval,step_cond],\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = get_pipeline(\n",
    "    region = region,\n",
    "    role=role,\n",
    "    default_bucket=bucket,\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    pipeline_name=pipeline_name,\n",
    "    custom_image_uri=clarify_image,\n",
    "    sklearn_processor_version=sklearn_processor_version\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n",
    "\n",
    "# Run the pipeline\n",
    "RunPipeline = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task6-1-continue\" target=\"_self\">Task 6.1</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"task6-2\" id=\"task6-2\"></a>\n",
    "\n",
    "### Appendix: Monitor a pipeline (Task 6.2)\n",
    "\n",
    "Now that you have created and run the pipeline, monitor the pipeline. You can view the pipeline status in SageMaker Studio.\n",
    "\n",
    "If you want to delete the pipeline, you can remove it using **delete_pipeline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe-pipeline\n",
    "RunPipeline.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list-pipeline-steps\n",
    "RunPipeline.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove-pipeline\n",
    "response = sagemaker_session.boto_session.client(\"sagemaker\", region_name=region).delete_pipeline(PipelineName='AutoModelSMPipeline')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue this lab, return to <a href=\"#task6-2-continue\" target=\"_self\">Task 6.2</a>."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 4.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-311-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
